start,end,text
640,7040," looks like we're live hello everyone and welcome to yet another azuzian session uh let's make a"
7040,14960," little bit of an announcement and officially start the stream as usual as usual uh so let's do the"
14960,23680," usual red circle uh live on twitch and what are we doing today on twitch.at television a website"
23680,28880," so today we continue doing machine learning in c but specifically today we're going to be"
28880,37840," developing a new framework for neural networks easy that's right it is possible in 2023 let's go"
37840,42640," so i'm gonna give the link to where we do know that which dot tv is slashed sodding and i'm gonna ping"
42640,47440," everyone who's interested in being pinged and there we go the stream has officially started holy"
47440,52560," that's how we're supposed to act in 2023 on social media otherwise nobody's gonna watch you"
52560,61440," because you have to be engaging and i won't die anyway hello everyone so uh hello hello welcome"
61440,69920," welcome welcome so on the previous stream we um um essentially we essentially explored"
69920,76400," uh a paradigm called machine learning right so uh in this paradigm you create a some sort of a"
76400,83680," mathematical model of uh some sort of a process right so and essentially you do not create that model"
83680,89040," directly right so the model has a bunch of parameters you do not specify the parameters of the models"
89040,97360," directly what you do you write a description of the behavior of the model that you desire uh so-called cost"
97360,102800," function right so it's basically like a distance between uh the how the model actually behaves and"
102800,109200," how you want it to be here and then you basically tweak and mess with parameters algorithmically purely"
109200,115280," algorithmically until the cost function becomes like small enough that you would say that's a good model"
115280,120320," right so that's a good model so essentially this entire process uh takes your description of the model"
120320,125120," and just you know finds the model that fits the best that specific description so it's it's kind of a"
125120,131360," different paradigm of programming so to speak right so you're not creating a thing directly you create"
131360,136400," a thing that describes how the thing has to behave right and of course because of it's a different"
136400,142160," paradigm it has its own strengths strengths and weaknesses some things are easy in that paradigm"
142160,151360," some things are difficult and vice versa right so yeah and on the previous stream we actually studied very"
151360,158560," very um with various with very simple examples we considered three different models so you can find"
158560,163440," the notes from the previous streaming here so i'm going to copy paste them in the chat so here they are"
163440,170880," uh right and for people who's watching me on um on youtube uh it's going to be in the description"
171440,175840," right so what we started with we started with a very simple model maybe we can even take a look at"
175840,182960," that right so let's take a look at the ml notes so the model is called twice right and essentially what's"
182960,193920," up with that model right so you is you are supposed to uh supply a number x into that model right so and"
193920,200720," the model itself it has only one parameter right which is like you know the weight of the connection if we"
200720,205600," look at that model as a single neuron but if you look at it as a single neuron we're supposed to"
205600,210240," actually put weight on the connection in here right because that's the strength of connection but let's not"
210240,217280," go to wind up about all of that and on the uh in the output you would see this thing being multiplied"
217280,223520," by two so here you have two x right and essentially uh the simple solution for that model is just to say"
223520,228960," w equal to uh to two but we don't know that we pretended that that's a very difficult very"
228960,234240," complicated uh model and we wanted to use machine learning approach to actually find that specific"
234240,239680," solution right and uh we used a very simple approach where we define the cost function right"
239680,247360," and then we used a very simple finite differences approach essentially we wiggled the parameters of the"
247360,253280," the model by a certain epsilon and uh if that drive the cost function down we just increased by that"
253280,257680," specific epsilon that specific direction nothing particularly special so i think in this specific"
257680,263840," case i also introduced the biases right so to to demonstrate uh how how it works it's just addition"
263840,269360," number but doesn't really matter so this was the twice model right it just multiplies the specific"
269360,277600," thing by by two right so this is twice so the other thing we explored we explored a model with yet a"
277600,283840," single neuron right but already with two connections right so there was two connections in here so we have"
283840,295920," uh a value x1 and x2 uh there is two weights in here a w1 and w2 and also um there is like one one bias"
295920,301600," specifically for that neuron and uh essentially what we did with that single neuron we modeled a couple"
301600,311920," of uh you know uh gates right and a couple of like a binary gates like or gate uh or gate and uh and gate"
311920,318800," and so on and so forth and we also demonstrated that it is not not really possible to model zor gate with"
318800,326800," just a single neuron uh right so it's not particularly possible uh to model uh to model zor gate right to"
326800,335920," model zor gate uh we have to uh create a more complicated um architecture of the neural network"
335920,343600," right so we had to come out we had to create three separate neurons right three separate neurons so the input"
344160,352000," goes into these two right so actually uh you have two inputs in here so this is not like an actual neurons"
352000,358800," of the neural network but they are sort of like an input so this is x1 and this is x2 and uh the first"
358800,365120," input connects to here right and the second one connects to here and again these two also connect to"
365120,373040," these uh two things right so and then the final result is combined by this neuron right the final"
373040,379760," result is combined by this neuron so this particular thing has uh at least like six weights because each"
379760,385760," of the connection has its own weight so there's a four connections in here uh and two connections in here"
385760,394720," so there's a six weights and uh each of the neuron have their own bias like b1 b2 and b3 so there was"
394720,401040," in total like nine parameters in that model uh and uh so the way we actually represented that specific"
401040,407200," model right the way we represented that specific model is just like we literally listed listed all of"
407200,414000," these parameters in a structure and we passed around and so on and so forth right so the problem with this"
414000,422560," entire stuff is that this particular approach is not scalable right because we already have like a three"
422560,427360," neurons in here right but it's already kind of difficult to work with we had to do a lot of copy"
427360,432800," paste uh right just to work with all of that we could have actually put all of that into an array"
432800,439840," like a linear array of those things but that doesn't describe the architecture of the neural network what if we"
439840,445600," have uh three neurons but organized in a completely different architecture right so here they are like"
445600,452000," sort of in a triangle but what if we just uh straighten them up linearly so how do we describe that we"
452000,458480," literally have to change the code of the function that forwards information in here so sort of the actual"
458480,465520," um topology so to speak of the uh of the neural network is described by literally this code right it's sort of like"
465520,471360," hard coded how can we make it more flexible so yeah and that's the problem that i wanted to"
471360,476560," want you to tackle today there is a very convenient and kind of like elegant way of representing neural"
476560,484320," networks in the memory of a computer and uh so we're gonna explore that sort of technique and we're gonna"
484320,491360," build a framework around the technique today so that's the plan for today so we've got some subs right"
491360,494880," so we've got some subs that i would like to acknowledge right so let me quickly do that"
494880,502240," uh so we have some people uh silence uh showman thank you so much for tier one subscription they"
502240,507520," subscribed actually off screen uh but i won't acknowledge them anyway uh jack patterns thank"
507520,512320," you so much for twitch prime swinging sorcerer thank you so much for twitch prime lindra braga thank"
512320,516240," you so much for twitch prime azura akimori thank you so much for twitch prime with the message"
516240,520560," 11 months it's so cool that's actually pretty cool almost a year by the way almost a year"
520560,526160," uh not ignacio i hope i pronounce your name correctly thank you so much for twitch prime subscription with"
526160,535440," a message hello a smiley face uh hello hello hello welcome welcome welcome uh so how can we represent all of"
535440,546080," that to understand how we can represent all of that we need to understand how exactly we're passing the"
546080,555120," information through the neural network right so here are the inputs x1 and x2 and here on each of"
555120,561600," the connection we have uh additional numbers so effectively we have uh four numbers in here so let's actually"
561600,568640," denote them with w1 so this is the weight of this connection uh then w2 this is the weight of this"
568640,578640," one uh right so then w3 uh this is the weight of this one and w4 is the weight of this one right uh so"
578640,587840," let's denote maybe the actual values in here with a uh one and uh a2 right so this is the input x1 and x2 and"
587840,593200," and this is activation of this specific uh specific uh specific neuron a1 and a2 that's why it's called"
593200,601920," a so how we compute a1 and a2 right so how do we compute them so a1 is equal to what right so we take"
601920,607200," x and multiply it by the weight uh by the weight of that specific connection so this is going to be"
607200,617600," x1 multiplied by w1 right and then x plus x2 all right and then multiply by this connection by w3"
617600,626160," right so this is w3 so okay uh then we also have to add a bias in here so we also add a bias in here"
626800,634720," so then we need to compute a2 right so we need to compute a2 and uh for a2 we also have x1 so as"
634720,639200," you can see it's like similar thing in here multiplied by already different connection different weight"
639200,652800," w2 right so this is w2 plus um um x2 x2 multiplied by uh w4 right multiplied by w4 plus b but it's actually"
652800,659520," b1 and b2 so this is b1 and this is b2 and this is how we compute these two activations in here"
659520,664560," but here is an interesting thing don't you notice something interesting about this stuff"
664560,670160," don't you notice if you have a background with linear algebra"
670160,676880," who has a background with the linear algebra they can probably notice something interesting in here"
679520,687760," they may notice that specifically at least this part uh excluding uh the biases right"
687760,694640," excluding the biases looks like a matrix multiplication it's sort of like it's a dot product and matrix"
694640,703040," multiplication so can we just take uh these values like x1 and x2 w1 w3 and like express them in terms of"
703040,708320," matrices uh right so essentially we can say okay the input is going to be a matrix x1"
709120,713680," and x2 so a single matrix uh represent uh"
713680,722240," my paint fucking crashed"
722240,728160," anyway so"
735120,746320," oh my god thank you so much recovery we're saved chat we're saved oh my god uh so it's called something"
746320,748480," like an an aura aura"
748480,751520," oh oh my god"
751520,760800," uh okay so uh i hope it's yeah yeah there we go all right that's totally fine so i don't know what exactly"
760800,763600," happened but we're we're totally safe now so everything's fine"
763600,774240," okay so reliable software in 2023 how about that reliable software thank you so much uh right if i"
774240,777760," had money i would probably donate to my paint unfortunately i don't have any money"
777760,786080," so uh okay let's actually define this matrix so it's going to be x1 uh and this is x2 right so this is the"
786080,792560," sort of like an input matrix that represents the input for the neural network that we're passing"
792560,799600," through the neural network right so to create this sort of effect of what we have to multiply it by"
799600,805760," what kind of matrix do we have to multiply it by uh right so it's definitely going to be matrix uh two"
805760,811280," by two all right so it's going to be the matrix two by two and it's going to be a matrix of the"
811280,816400," weights of the corresponding connection right so it's going to be the weights of the corresponding connection"
816400,822560," so we'll need to come up with the indices properly in here right so usually i like to think"
822560,830320," of matrix multiplication as sort of like this cross product of the things right so essentially if"
830320,838400," you have so this is a matrix one by two right so this is a one by two and two by two so the general"
838400,846240," sort of like a rule that you memorize right is that um you can only multiply the matrices where"
846240,853040," their inner size is equal right and it's quite important to remember that this is the amount of"
853040,859280," rows right so this is the amount of rows and this is the amount of columns right so why it is important"
859280,867520," because usually we think about the coordinates as x and y but x is a horizontal one it's a columns x is"
867520,875200," a columns and y is uh rows but here it's the other way around so first come the rows and then the columns"
875200,882160," right so it's quite important and they're only multipliable when their rows are equal so otherwise"
882160,887440," the multiplication between such matrices is not defined as far as i know and the final result"
887440,894000," the result of the final matrix is going to be one by two so it's going to be like basically outer sizes"
894000,899680," right so it's going to be the outer sizes so if this thing was something like five the final result"
899680,904640," would be five by two right but this one is one so it's going to be one by two so that means this entire"
904640,913920," thing is going to be similar to this matrix right it's going to be something like uh a one a two right"
913920,918320," so that's basically this entire multiplication and that that's basically what we uh want to have in"
918320,925280," here so as you can see uh this is the input for the um for the neural network these are all of the"
925280,931200," connections right there is four connections in here these are all of the connections and when we apply"
931200,937360," them together with sort of like passing this information through the neural network right"
937360,941280," but this entire thing doesn't really take into account the biases because one of the things you"
941280,947040," have to do in here right is add the biases but it's actually very easy to uh to take into account i think"
947040,954160," it's just basically you do plus the uh the vector of biases and vector is just basically a special case of"
954160,959600," matrix right so we're going to use these terms sort of interchangeably and this one is going to be a1"
960160,969520," and uh a2 right so a1 and a2 uh right and this is basically like a passing information through the"
969520,974640," neural network you have this signal you pass it through here is the weights and here is the biases"
974640,979680," so the main problem here as i already said is this like what what what what are the indices in here"
979680,987760," right the way i like to think about matrix multiplication is that uh i put the left matrix here usually right so i put"
987760,993680," the matrix in here the one the first one the second matrix uh where i put i put it in here so this is"
993680,1001200," the second matrix so i put them like that and the way i multiply them the way i multiply them is is just"
1001200,1009920," i basically find the intersection of this row right so this is this row and this column right and that"
1009920,1017200," intersection creates the element this like element of the final matrix and how you do that you basically"
1017200,1024560," multiply these elements multiply these elements together right uh right and add them to here then"
1024560,1030480," multiply these elements together and add them to here and this is one of the reasons why their inner"
1030480,1038240," sizes has to be equal right so because here's the columns and here's the rows here's the columns and here's the"
1038240,1044160," the rows it is impossible to combine them together if their internal sizes are not equal right so this"
1044160,1050240," this this is where this requirement is coming from right because you need to sort of like uh intersect"
1050240,1055440," these entire things like together uh right and this in this specific case right so this one is going"
1055440,1065200," going to be like a1 a2 and this is going to be w's right and what do we do in here so a1 multiplied by w1"
1065200,1074960," so that means um all right w1 i said actually it has to be x i'm sorry i already already you know"
1075520,1085200," being type x1 and x2 right so x1 multiplied by w1 and then w3 right so that means uh all right so that"
1085200,1092480," this column is one three and this one is two four and it's actually fits perfectly one two three four"
1092480,1097120," okay that's perfect right so uh the reason why i was actually like demonstrated all that is because i"
1097120,1101200," want you to come up with the like uh with the correct indices in here right so because it's kind of"
1101200,1108000," difficult to not screw this stuff up uh right so but essentially people usually induce those things"
1108000,1112800," in a completely different way where they usually you know in the stem as the uh you know as they usually"
1112800,1119840," in the stem the matrix right so one one uh this one has to be one two right this one is one two"
1119840,1129200," uh all right so this one is one two this one is two one and two two so because of that this one is"
1129200,1144480," one uh one one one one two right one two uh two one two two two one uh two two right didn't i hope i"
1144480,1149760," didn't make any mistakes in here yeah that's basically what it is right and on top of that by the way"
1149760,1155760," uh there is also an activation function right so uh there's also an activation function basically you have"
1155760,1162960," to like apply uh for example sigmoid or relu to each element of the final matrix in here so we can say"
1162960,1170080," that after you did all of that after you did all of that you just like put this activation in here and"
1170080,1177840," that's basically how you uh pass a single through a single uh layer all right"
1177840,1186560," right through a single layer"
1186560,1198000," uh and uh yeah so and then uh when you want to pass this final thing through another layer right you would"
1198000,1204480," have to uh do the same thing there will be a matrix of weights in here so there will be some weights in"
1204480,1211360," here as well but this time this thing is going to be uh how many of these things right if we have this"
1211360,1222080," thing one by two and we want a final thing to be one by one right so the matrix of um the matrix of weights is"
1222080,1232960," going to be um two by one right so two by one so that means you would have to take a one uh a two right"
1232960,1239440," so this is the uh activation from the previous layer then you would have to multiply it by the weights"
1239440,1245360," these specific weights in here this one is going to be something like uh v1 and v2 right"
1247760,1252960," something like this since it's two uh one by two two by one the final thing is going to be one single"
1252960,1258480," number you would have to add a single bias in here so this is going to be matrix of a single element i'm"
1258480,1265600," not sure if that's how it works with matrices but anyway uh right so then you apply the activation of"
1265600,1272080," this entire thing right you apply the activation and you get the final output of the neural network right so"
1272080,1279360," essentially the simple form of passing a single through a single layer looks like this right we can"
1279360,1286320," even sort of like um you know simplify this entire thing so the uh the matrix of activation could be just"
1286320,1295520," a uh multiplied by the matrix of weights plus the matrix of biases right plus the matrix of biases plus"
1295520,1302320," some sort of activation right and you basically keep repeating keep repeating this thing through"
1302320,1308720," each individual layer right through each individual layer until you went through the entire neural network"
1308720,1314480," and that way you can actually create like a fully connected new neural networks of uh different"
1314480,1320480," architectures right of absolutely like arbitrary architectures so maybe you can have like three inputs in"
1320480,1328800," here right you have three inputs in here two inputs in here and maybe four inputs uh four uh you know neurons"
1328800,1333680," in here and maybe one in here right so essentially all of the connections are going to be something like this"
1333680,1338880," uh right something like this then all here it's going to be connected like this"
1338880,1347280," right and this entire thing is finally going to be fully connected to"
1347280,1355360," here uh here uh right so and essentially this is going to be matrix one by three three uh three by two"
1355360,1360160," two by four and four by one right and you have like several matches and you pass these signals"
1360160,1366880," through this entire network by matrix multiplication addition and then uh activation right so you also have"
1366880,1376240," to apply the activation function if you have one uh right so and that basically that simple formula by the way"
1376880,1382400," that simple formula uh creates basically the entire sort of like a mental framework"
1382400,1388880," of uh working with neural networks right so it's like mental framework and it's actually very convenient"
1388880,1393680," to think about this kind of stuff because it can create like arbitrary complex uh fully connected"
1393680,1399120," neural networks right so it only can represent fully connected ones if you don't really want to have a"
1399120,1412720," fully connected one twitch literally disconnected me yeah hello welcome back so twitch literally disconnected me"
1412720,1419520," uh my internet was fine my internet was totally fine it's something on twitch side it literally disconnected me"
1420080,1422080," i think they don't like me anyway"
1422080,1432000," so basically right so this thing creates a fully connected neural networks right it creates fully"
1432000,1437920," connected neural networks and i suppose if you don't need some of the connections the uh training process"
1437920,1444480," right uh the training process will just deactivate some of the connections right so yeah that's basically how i"
1444480,1450560," think about it right i'm not a machine learning person right i don't really know all of the justifications"
1450560,1458240," about this kind of like decisions but i personally find this like application of linear algebra for"
1458240,1466000," representing neural networks kind of elegant isn't it like it's kind of cool right so that you have the"
1466560,1470720," the the the signals right that pass through the neural network and it's just like a sequence of"
1470720,1476000," matrix multiplication it's just like it's kind of cool uh how it just like fits perfectly in here"
1476000,1484400," i don't know right and what's cool is that this is just two matrix operations right these are just two"
1484400,1490720," matrix operations and as far as i know these are two very important um you know sort of like matrix"
1490720,1497600," separations to the point that there's little hardware that is designed specifically to multiply"
1497600,1503840," matrices and add matrices right so if you ever heard right if you ever heard about like a specialized"
1503840,1509760," hardware or people doing stuff on gpu right a specialist hardware for artificial intelligence like"
1509760,1516800," what how what a specialized hardware for artificial intelligence is doing if you read about that you would"
1516800,1522240," find that it's just doing matrix operations and it's just like what like what does it have to do with"
1522240,1526960," artificial intelligence why do you need matrix multiplications and additions what what the here's"
1526960,1534000," the answer that that's why right and if you optimize just these two operations like passing information"
1534000,1543680," through a neural network is going to be like very fast right um yeah so and basically what i wanted"
1543680,1551840," to do today right what i wanted to do today is develop a simple uh framework uh in c that basically"
1551840,1561440," allows us to define train and feed forward neural networks in c right so my idea is going to be the"
1561440,1566880," following we're going to start developing um header only header only neural network library and we're going"
1566880,1574560," to port our small little models small little models to that framework right and see how uh working with"
1574560,1580160," these models in that framework is going to look like right so that's basically the idea for today's"
1580160,1587040," stream right that's basically the idea is that is that it uh right so let's let's say go let's say go"
1587040,1596400," oh the time has a come um yeah so how are we gonna call our small a little uh framework we're gonna call"
1596400,1601840," it something like since i want it to be like a stb style uh stb style header only library i think i'm"
1601840,1607680," gonna call it an n dot h maybe it's already taken maybe this name is already taken but i don't really"
1607680,1615680," care i create this entire thing for educational purposes right so and then dot h there we go um"
1615680,1621600," right so let's create an inclusion guard right and then dot h so this is needed just in case you include"
1621600,1630480," this thing twice uh right so this is where we are going to have all of our declarations right so because"
1630480,1636960," the thing about uh stb header only libraries is that they act simultaneously as headers they act"
1636960,1644400," simultaneously as headers and as the c files right so here we're going to have a header part and then"
1644400,1652480," the c part is going to be behind if defined and then implementation uh right so and if and then"
1652480,1657600," implementation so this is where we're going to have all of the implementations of the declarations up"
1657600,1663120," there so this is the header part and this is the c part the way you work with all of that so let's"
1663120,1670880," actually create nn.c uh right the way you work with all of that is that you include nn.h not bb nn.h"
1670880,1678960," right you create an entry point something like this uh right and by default it acts like a header"
1678960,1684800," right by default it acts like a header as soon as you define and then implementation it starts to act"
1684800,1690560," like a header plus a c file so it will include all of the implementations into your main program the"
1690560,1696160," reason why it acts like that is because it gives you control over where exactly you want to place your"
1696160,1701200," implementations when you compile right so because other parts of your other translation units probably"
1701200,1706320," don't want to include implementations they just want to use this entire thing as uh as a header"
1706320,1711840," because they only care about declarations right so that's basically uh stb libraries 101 right if you ever"
1711840,1717840," wanted to create one so this is how basically they operate there is more information about how stb libraries"
1717840,1725280," work at the nothings slash stb so i think there is even document there that basically describes the"
1725280,1732800," principles of of these libraries that uh sean has come up with uh stb how to right so these are the"
1732800,1739120," principles right so i'm not going to go into them too too much uh right but i'm going to put them uh in here"
1739120,1744240," essentially uh i'm going to put them in here"
1749120,1754640," just a second"
1754640,1764880," okay so uh i'm gonna give that thing in the description right so where is my description do"
1764880,1770800," i have description here it is uh stb library library principles principles"
1773760,1779120," right so because it's it's very like uh not everyone knows about this kind of approach of"
1779120,1786080," developing things in c because the amount of people who develop in c at all it actually becomes"
1786080,1791440," smaller and smaller and smaller is the boat down by the way oh the boat is probably down yeah so"
1791440,1800800," damn probably oh you know what that means that means that there was problem with my internet yeah that"
1800800,1805280," means there was a problem with my internet but it didn't really ruin the ping so i constantly ping"
1805280,1811680," uh the other thing so just a second um so i'm going to put the description disclaimer in here"
1811680,1822640," fixing bot just a second right so let me quickly do that i need to ssh to my"
1822640,1826960," the separate machine because the boat is running in a separate machine uh right"
1829440,1834240," actually it's kind of interesting it's running in a virtual machine on a separate physical machine"
1834240,1839840," so first i ssh to a physical machine then within the physical machine i ssh to a virtual machine"
1839840,1847680," uh right so data center yeah it is it is like data center yeah yeah so it's it's kind of dead"
1847680,1858080," uh right so it's a session yeah so and within the virtual machine uh i actually run the bot inside of"
1858080,1864640," team ux session so the way i actually approach that i ssh to a physical machine then ssh to a virtual"
1864640,1869600," machine within the virtual machine i attach to team ux session and already with the two new session i"
1869600,1874880," have like console open that allows me to restart something and stuff like that uh right so i'm not"
1874880,1878640," going to show that just in case it shows some sensitive information but that's how it works"
1879280,1882560," uh uh you have no auto reconnect for for for the bot"
1882560,1889840," i didn't have time to implement i'm a professional software developer i know what i'm doing"
1889840,1894800," uh okay it seems to be working now it seems to be working"
1894800,1901520," right so i mean i'm gonna implement the outer reconnect at some point it's just like i didn't"
1901520,1908480," have time okay so i spent all my time preparing for epic sodding session for you guys right i spent all"
1908480,1915120," of my time on making sure the content is good the content is high quality right so if i spent all of"
1915120,1921760," my time uh on uh implementing useless features for the bot the content will be shed so that's my justification"
1924960,1934000," anyway so um let's continue let's continue uh so what we need to do right so since the entire sort"
1934000,1942000," of approach of working with neural networks uh in 2023 generally is based around matrices right uh so"
1942000,1947200," we need to have a type that represents the matrix right so since all the matrices are floats uh right"
1947200,1953200," we're going to just have a simple structure right uh called matrix right and here's the interesting"
1953200,1961840," thing unlike with 3d graphics um the matrices can be arbitrary sizes right so we can never know how big"
1961840,1968800," or small the matrix is going to be right it purely depends on how many neurons we have in a layer and"
1968800,1974480," stuff like that so the matrices could be very small very big they could be square they could be rectangular"
1974480,1979920," they could be just vectors row vectors column vectors like anything they could be like any shapes"
1979920,1986880," right so we cannot code uh matrices of all the sizes so because of that it has to be very dynamic thing"
1986880,1993040," uh right so i think we're going to have like a structure right with the size which contains rows of"
1993040,2000240," the matrix columns of the matrix and uh we're going to have basically a pointer to the beginning of the"
2000240,2008640," matrix that contains the floats right so essentially uh we're going to allocate some um you know array of"
2008640,2014480," floats continuous array of floats and the shape of the matrix is going to be described with these two"
2014480,2020240," values right so that's how we're going to work with all of that right so uh and unfortunately that"
2020240,2025920," requires like working with dynamic memory but it is what it is and it isn't what it isn't so uh i"
2025920,2030720," suppose one of the things we can do we can create an operation that allocates a matrix right so let's go"
2030720,2036240," create mat alloc uh it's going to accept rows and columns right so this is going to be rows and"
2036240,2042720," columns and i suppose it's going to just use malloc and dynamically allocate enough floats and just give"
2042720,2047280," the pointer in there right so but it does not necessarily have to work like that right so if you"
2047280,2053360," want to allocate floats in the static memory you can easily do that i suppose you would be able to do"
2053360,2059200," something like uh right float um you know some sort of a matrix right so this is going to be something"
2059200,2065920," like this and you're going to say uh zero zero like uh one zero one right so here is your data for your"
2065920,2072880," metrics and then uh right so let's call it d for for data and then you would do what you would say okay"
2072880,2080080," here we have two rows and two columns right and the data starts at d and there we go we define the"
2080080,2087680," matrix 2x2 but what if you meant not a matrix 2x2 what if you meant matrix 1x2 what if you wanted this"
2087680,2093600," thing to be like a row a single row in that case you can say this and it's going to work anyway right"
2093600,2098480," so basically this is just the data and this thing defines the shape of that data you can say that this"
2098480,2102240," is actually a column thing so you say it's a four rows but one thing in here"
2102880,2110080," uh right so seems to be okay right so this is how it's going to work it's kind of similar to"
2110080,2114960," wait a second it's kind of similar to the canvas in all you see"
2114960,2123040," i mean why not right do you guys know the the framework all right so all you see uh right so there"
2123040,2131520," is a canvas uh yeah so yeah but instead of uh you know values of the matrix right floats you have"
2131520,2138400," you in 32 but you also have width and height but on top of that you have a stride on top of that you"
2138400,2146400," have a stride and stride it is needed to actually take sub canvases right but would we need to take"
2146400,2148480," sub matrices at some points"
2148480,2152320," i mean is it really needed"
2155360,2161040," maybe wait a second for instance imagine that you have a training data right and uh let's say"
2161040,2168320," that the training data is like this the first two elements in here is uh zero and the second one is"
2168320,2174400," zero so let's actually create a training data for the or gate right so here is going to be that uh and"
2174400,2180880," then it's going to be that and this is going to be that so and what if you at some point want to split this"
2180880,2186800," entire thing uh what if we want to split this entire thing into two matrices so this is going"
2186800,2191600," to be the first matrix and this is going to be the second matrix right this is going to be the second"
2191600,2198960," matrix how can we do that right so we can say that this data data input it starts at es right so the"
2198960,2204400," amount of rows is going to be four the amount of columns is going to be two but we also need to define"
2204400,2211120," how far you need to jump to get to the next row so you need to specify this stride and the actual stride"
2211120,2218720," is going to be actually three right so that way you sort of have uh the inputs and then the outputs"
2218720,2224640," right in case of the output the output is going to be this matrix so that means you have four rows one"
2224640,2235040," column stride is still three but you start not at zero you start at here so the element zero one two"
2235040,2240560," you have to start at three right you can even do something like something like this"
2240560,2249120," yeah so you just define the continuous data and you just like split it into mattresses or something like"
2249120,2256240," that why not so but i don't really want to go into that because i'm not really sure if it's going to"
2256240,2262400," be needed that much but it's an interesting idea to think about right what you guys think right i think"
2262400,2272080," it's kind of cool right so um so let's uh continue so we need to do uh matrix allocation right and the"
2272080,2277520," two main operations are going to be matrix uh dot right so we're going to do multiplication between the"
2277520,2285200," matrices uh right and matrix sum so but i need to think how we're going to organize the arguments of"
2285200,2291120," this entire stuff right since the matrix is a very much like dynamic thing uh i don't want to return"
2291120,2297440," a new matrix right so i don't want to do things like like this where you accept these two matrices"
2297440,2303120," these are immutable arguments and it just creates a new one right i don't want this thing to be"
2303120,2307280," allocating memory because i don't really expect that right so if i if something"
2307280,2314240," allocates memory i need to expect it's doing that like for example in case of this specific uh this"
2314240,2321920," specific uh function right maybe because of that uh this operation is going to accept three matrices"
2321920,2330000," right three matrices so essentially you pre-allocate the memory for all of the three"
2330000,2336960," you make sure that their sizes fit and everything uh right and we basically uh iterate through these"
2336960,2342320," matrices and multiply them in here right so that way none of these functions will do any memory"
2342320,2349040," management so you do all the memory management up front uh right so i think that's how i want to go"
2349040,2356400," about that but to be fair the destination into which we're multiplying things i would like to put at the"
2356400,2363360," beginning of the function to make it kind of similar to functions like mem copy right you know what i mean"
2363360,2370080," so there is a mem copy where you copy one thing into another one right and the destination is usually the"
2370080,2375360," first element right so the destination is first and source is the second one and it's kind of consistent"
2375360,2381920," across some of these string functions in c right mem set also accepts the destination as the first"
2381920,2387680," element and stuff like that so because my mental model thinks about these operations like in terms of"
2387680,2392960," c library i think it would be easier for me to have the destination sort of as the first element if that"
2393520,2400000," makes any sense right so right here these two things are immutable and this is the destination"
2400000,2404800," right so and i don't even need to accept this by a pointer because i already have a pointer to the"
2404800,2410160," elements inside of the structure so the structure itself is already a very lightweight pointer"
2410160,2417040," right so here's the thing matrix as a structure is very lightweight because it consists of only three"
2417040,2425520," 64-bit integers 64-bit integers the first 64-bit integer is rows columns and the third 64-bit integer is a"
2425520,2432400," pointer to the beginning of the data uh of the matrix so the structure itself doesn't contain the data"
2432400,2435600," right it doesn't really contain the data"
2435600,2444160," right so and similarly i want to do something uh similar for the sum right so when i add matrices"
2444160,2449200," together right so this one is going to be destination right so maybe i'm going to even call dst so this"
2449200,2457200," is dst and then we go so a and b we multiply this entire thing into dst and then the sum right um"
2457200,2466320," question what if size t is more than 64 bits uh does it grow in bit size size t is not bigger than 64 bits"
2466320,2473840," i'm operating on the architecture x86 64 linux and on this specific platform size t is"
2474320,2484880," 64 bits it is 64 bits so um it could be on some of the platform 32 bits but these are very much like old legacy"
2484880,2491360," platforms and maybe something in embedded but i don't really plan to run this thing on embedded so here's"
2491360,2499200," the interesting thing this uh thing is for educational purposes only so that's why i'm making it work at least on my machine"
2499760,2507120," uh the thing the problems on other machines that may occur are not of my concern as of right now so"
2507120,2514160," first thing i'm trying to do i'm trying to make it work on my machine and my machine"
2514160,2521680," only on my machine it's 64 bits totally fine right so then once we have something working on my machine we"
2521680,2527120," can look at it and think and think okay how can i run it on other machine oh it creates that little"
2527120,2531840," problem let's fix the problem okay it runs on two machines okay let's try to run it on the third"
2531840,2537600," machine oh it doesn't uh have to it doesn't have any problems because uh that's all the problems there"
2537600,2544800," are so it can run on three machines so that's how software development work right so um you don't try to"
2544800,2550160," solve all the possible problems even non-existent ones simultaneously this is not how we do that"
2550160,2553200," otherwise you're never going to go anywhere okay so"
2553200,2563520," now uh that's the only operations that i care about right now at least right so this is what we're going"
2563520,2567840," to have right now and one of the things i probably want to have is something that prints the matrix right"
2567840,2572240," because i want to be able to see the matrix that we just created right so let's create something like"
2572240,2576960," print and let's go ahead and implement all of these separations right so why the why the"
2576960,2583440," fuck knows right so that's the only thing we need in here uh okay so when i'm allocating something this"
2583440,2590960," is going to be just that uh right so i should be pretty straightforward so essentially i can set the"
2590960,2597920," rows to to the rows right so then the columns uh to the columns and then when i need to allocate the"
2597920,2603600," elements i'm just going to do malloc right i need the size of a single float right so the size overflow"
2603600,2611200," but maybe i'm going to make the uh the type that we're using here sort of like a variable maybe it's"
2611200,2617760," going to be type def because maybe you want to use double or maybe floats uh 6 16 or something like that"
2617760,2622800," so because of that i want to do the following trick i'm going to take es which is the pointer to float i'm"
2622800,2628480," going to dereference this entire thing and i'm going to take sides of that specific thing that way if i"
2628480,2635120," ever change this type to double i would never have to go down there and change anything in here right"
2635120,2640160," because if i would just say the size of float if i change it to double i should not forget to go in"
2640160,2645360," here and switch it to double uh if i do something like this i never have to do that right i can just"
2645360,2651280," switch that to a different type and it will work out in it right so this is the size of a single float"
2651280,2656880," then i multiply it by the rows and then multiply by the columns and this should allocate uh enough memory"
2656880,2663360," for all of these floats and we're going to assert that this entire thing uh is not equal to no right"
2663360,2671200," and after that we're going to just return this entire thing right as simple as that so what about dot"
2671200,2675520," operation right so i suppose i'm going to postpone implementing dot operation because it's a little"
2675520,2689120," complicated uh right so and some operation also let's postpone that so void dst a and here print we"
2689120,2696080," don't really do anything so essentially i just want to uh try to compile this entire stuff and see okay"
2696080,2700720," so let's create maybe a build a sage script that is going to build the example for us"
2700720,2707920," i just realized that i probably have a bunch of uh subs that i didn't acknowledge actually two uh"
2707920,2712240," team scar thank you so much for twitch prime and a upon natry thank you so much for your one"
2712240,2719120," subscription thank you thank you thank you all right uh so uh let me let me see so i'm going to create"
2719120,2727920," a build sh uh and then uh what we're going to do we're going to do clank w o w extra o uh nn and then"
2727920,2732640," dot c we also probably want to link with lm and there we go so i'm also going to actually enable the"
2732640,2740160," uh the tracing and stuff like that let's make this shit executable and boom boom uh it doesn't even"
2740160,2745760," freaking work right because we don't have a size t so size t is defined somewhere i think"
2746320,2753040," in std depth right so that's the place where all all of the definitions go malloc all right malloc is"
2753040,2760800," defined somewhere there so here's an interesting thing about um about stb style libraries is that"
2760800,2767360," usually they provide a way to customize the allocation uh right so usually you have some sort of a macro"
2767360,2774000," called nn malloc right which accepts the size well in here we can say that it's equivalent to malloc right"
2774800,2782720," it is equivalent to malloc and what they do is if we only define this thing if it was not defined"
2782720,2791680," already right so by default uh right it's gonna use malloc right and and malloc by default is gonna use"
2791680,2801920," malloc but then when you include this entire library you can very easily say okay and and malloc is my malloc which"
2801920,2805920," actually allocates the memory completely differently maybe it doesn't even call to libc"
2805920,2810400," maybe you have a pre-allocated pool of memory and you just linearly allocate that so essentially"
2810400,2816640," the cool thing about this tb libraries is that they give give you more control over how you allocate your"
2816640,2822080," memory right and i kind of want to mimic that as well right so uh and that also sort of decouples all of"
2822080,2828240," that from libc uh just in case you want to run this library in an environment where you don't have libc"
2828240,2832640," and you may think is that well this is embedded right like do we want to run an embedded do we care"
2832640,2838080," about embedded but this is not necessarily true there is a couple of platforms where there is no libc"
2838080,2846880," well i mean people for example in wasm in wasm by default there is no libc right but there are"
2846880,2853920," implementations for for libc there is also wazi there are huge pain in the ass to work with right so"
2853920,2859040," it's just like and sometimes the pain is so huge that i feel like sometimes it's just not worth it"
2859040,2864560," but for some people it's worth it right but for me usually it's not worth it every time i'm programming in"
2864560,2870560," c and i'm writing like a wasm application i literally don't use c i i just i just cannot"
2870560,2876400," i'm too dumb to use mscripten but every time i open in script and i try to follow the tutorials this"
2876400,2881360," thing is so lousy it's just like it falls apart right in my hands it starts spitting out errors"
2881360,2887280," i have to go google ask chat gpt and stuff like i just don't want to do that let's reduce the complexity"
2887280,2893840," get rid of all this and keep it as simple as possible and it just works it's beautiful right so"
2893840,2898640," i don't know so people who are way smarter than me can deal with all that madness i cannot i'm very"
2898640,2905680," dumb person right so i like simple things i like to keep things simple it's just like yeah sorry uh"
2905680,2911200," sorry sorry sorry sorry okay so we'll get a bunch of more subs a bunch of more subs thank you so much"
2911200,2918320," uh eponetri i think i already acknowledged that uh dr stud thank you so much for twitch prime and tinted"
2918320,2925200," trunk uh thank you so much for twitch prime as well google development yes so i'm a google developer"
2925200,2933200," uh sorry okay so we're not going to have our own malloc uh what i want to do is just like a run build"
2933200,2940000," this h and let's go to the completion so this has to be and if and uh also assert right so"
2940000,2947520," yeah implicit declaring library uh in this thing in this case we can actually also include std leap"
2947520,2954640," all right because we know what we want this thing from and assert assert is also often customizable if not"
2954640,2964960," defined and then assert uh we're gonna just include assert dot h uh define and then assert uh and it's"
2964960,2971520," just like literally the same assert in here right and it's very useful again for example in web assembly"
2971520,2977680," environment where you probably will implement your own assert if you don't use libc or something like that"
2978640,2988640," so and let's just do it like this okay cool so and how we're going to be using all of that right so let's"
2988640,2996160," create the matrix and let's define the matrix of uh a lock uh two by two right so a lock two by two and"
2996160,3002720," then i'm going to print this entire matrix just to see how it looks like because i'm really curious"
3002720,3010800," at how it is it does look like so did i call it something something else so mat print oh yeah yeah"
3010800,3019840," i forgot to accept a matrix that i want to print actually so uh all right so m there we go"
3019840,3029760," that's a poggers right if we try to run this entire stuff now right so if i do build and n uh it will"
3029760,3034800," not print anything because matte print is not implemented okay so how can we implement matte"
3034800,3043360," print uh so we need to start iterating rows and columns right so as usual uh right less than m rows"
3043360,3050960," plus plus i then uh this is going to be j this is going to be columns this is going to be j and let's"
3050960,3055280," literally just print all of these things in here right let's just literally print them and here's an"
3055280,3064000," interesting thing so es is just like a pointer to the linear memory right so but we need to interpret"
3064000,3070640," it as a two-dimensional array so that means right we have to do some very scary we take we have to take"
3070640,3080000," the row right and we need to skip i rows what's the size of the row the size of the row is the amount of"
3080000,3087600," columns does it does it make sense right this is actually mentally difficult thing to imagine but"
3087600,3093840," imagine that you have something like uh this this is a single row you have four rows"
3093840,3106080," right so what's the size of that row the size of this row is four columns right so the length of the"
3106080,3112720," row is the amount of columns right does it make sense right so that's very important so we need to"
3112720,3119360," skip i rows so that's why we have to multiply it by columns because that size of a single row right and"
3119360,3128480," then we have to offset it by j columns right uh and that's very much error prone right so you may actually"
3128480,3133600," confuse some of the indices and it's just like it's very dangerous i don't like to do these things like"
3133600,3138880," that uh right and because of that like usually in c i don't have much choice unfortunately right i don't"
3138880,3144480," have my choice uh i like to define like a macro that does this kind of stuff for me so essentially i want"
3144480,3152480," to make something like mat at right so it will accept the uh the matrix uh row and column like i and j"
3152480,3159520," so and essentially i'm just gonna you know uh turn it into into a macro for me right and it's gonna work"
3159520,3166160," like that but but here's the problem right so m i and j could be an expression right for instance if"
3166160,3174320," you put mat at m i plus one right the i is going to be literally replaced with i plus one and this is"
3174320,3182800," not a correct an expression so in uh in macros which you usually do you actually wrap all of the arguments in"
3182800,3187760," parentheses just in case there are complicated expressions that will ruin this entire thing"
3187760,3192880," uh right so you have to be careful with this kind of stuff but i mean it's it's a macro usually you"
3192880,3198400," don't look inside of it so it looks a little bit ugly but that's fine uh right and instead of like doing"
3198400,3206160," this kind of stuff right instead of doing this kind of stuff i can now do mat at m ij and i didn't have to"
3206160,3211200," think about any of that stuff right and i have to think it's a row size column what what okay the"
3211200,3217440," element at ij i is the rows and j is the columns that's it right all that madness is actually tucked"
3217440,3223040," away nicely under this macro i don't care about it right so i don't really care about it unfortunately"
3223040,3227520," like in c you have to do these kind of tricks right so in better languages you would probably use"
3227520,3233360," something better maybe like operator overloading or something like that but see in c it is what it is and"
3233360,3238960," it isn't what it isn't so we drew a single row and then at the end of the row we want to put some sort"
3238960,3247360," of like a new line in here so and that's how we're going to be printing the uh this thing so uh and of"
3247360,3255040," course we have this kind of interesting thing print f uh so maybe we want to include stdio"
3257440,3262720," so uh we just allocated a bunch of memory and this is basically two by two and yeah that that works"
3262720,3270080," nicely uh that actually works nicely so what if i define like one by two right so this is a single"
3270080,3278320," row uh two by one this is a single colon what about uh 10 by 10 uh 10 by 10 yeah that's that's the matrix"
3278320,3283040," 10 by 10 so everything seems to be working nicely uh all of that is zero right one of the things we"
3283040,3288240," probably want to be able to do we want to be able to randomize the matrix right because that's how we"
3288240,3295440," usually initialize uh the uh the neural networks we just randomize everything so maybe because of that"
3295440,3302960," maybe because of that i want to create a function uh right so here we created this thing mat rand"
3302960,3308720," is going to just accept a single matrix and just randomize this entire stuff so let's actually go ahead and do"
3308720,3321520," that uh right so we've got some subs i can see i can see subs money money money money uh cheer tremo i"
3321520,3324800," hope i pronounced your nickname correctly thank you so much for tier one subscription with the message"
3324800,3331520," ml and c all here for it uh well yeah ml and c it's actually very simple because like neural networks"
3331520,3338320," specifically is just magic multiplication and believe it or not all of the specialized libraries uh for"
3338320,3342000," metrics multiplication and addition and stuff like that are written in c anyway"
3342000,3351680," whatever numpy or tensorflow or whatever people use there all of that is written in c so maybe you're"
3351680,3357680," using that in python but it is just like a like a thin wrapper over over c code anyway all of that is"
3357680,3362720," written in c anyway it's actually it's kind of funny how people are surprised you're doing like a machine"
3362720,3370240," learning and see all of the machine learning is done in c all of it 99.99999 percent of it is done in c"
3370240,3376720," sometimes in rust people do sometimes right but all of that is like python is just like a wrapper just a glue code"
3378880,3385360," it's just a glue code um so yeah anyway"
3385360,3394560," it's all bytes well maybe anyways so let's actually randomize uh randomize this entire thing"
3394560,3399360," um so and by the way so once we implement this"
3401040,3410160," framework we may want to actually slap some functions that speed up the matrix multiplications"
3410160,3414240," and additions and stuff like that i think we could try to use lab pack uh right"
3414240,3420400," uh if i remember correctly that specific thing had"
3422560,3424800," but if i didn't remember i think they banned my ip"
3424800,3435920," yeah okay yeah i can i cannot visit this thing i remember yeah i remember i cannot visit this thing"
3435920,3442960," they probably banned my ip so but but i can visit it over uh vpn or tor right so they didn't like me"
3444960,3453520," so anyway why i don't know i don't know that's a very interesting question why why half of the internet"
3453520,3460560," is banned for me i don't know uh so um all right so let's do the for the following thing so we need to"
3460560,3467920," randomize all of this stuff right so it's going to be m i and j and we're going to just do rand float"
3467920,3473840," right so this is going to be the function that i use all the time uh right so i think we can make"
3473840,3489760," it part of the um we can make it part of the framework here as well right so float rand uh float"
3489760,3502720," the phone the fun part of my reality is browsing internet from time to time"
3502720,3511840," and see occasional you and uip it's actually quite funny anyway so uh what we have to do in here so"
3511840,3519200," it's going to be rand uh float and we're going to define this into i think rand uh i think max right"
3519200,3526720," and the the way this works it will return a random number from zero to one right uh random number from zero"
3526720,3535920," to one so uh and uh right and so here's the interesting thing what if we want to initialize"
3535920,3543520," the matrix uh actually within a certain range right not just zero and one but like a very specific range"
3543520,3548720," so i usually like to specify that range in here so this is going to be low and this is going to be high"
3550000,3554320," right so it's going to initialize the center i think like in that specific range and essentially"
3554320,3561200," what we do we multiply it by high minus low and just add low right and that can i assume that low"
3561200,3567920," is less than high but if you do the other thing well you just you'll just get garbage right so but"
3567920,3573680," nothing bad is going to happen right nothing back bad is going to happen so i think that's fine we're not"
3573680,3580400," going to do any additional checks in here i think uh right and think they are very much necessary uh"
3580400,3589200," okay so then here uh we're going to do mat rand and then we go so that's kind of kind of nice so we"
3589200,3593760," allocate the matrix 10 by 10 we randomize the matrix and we're going to print it so let's see how it's"
3593760,3600080," going to look like well i mean we have to randomize it with like let's say from i don't know from zero to"
3600080,3604320," ten right so that's going to be the values that we want to randomize all that stuff with let's go"
3604320,3612160," through the compilation errors uh correcting types uh i suppose mat rand yeah so we have to"
3612160,3620400," have a difference yeah there we go look at that so this is a random matrix with values from zero to ten"
3620400,3625680," uh that's pretty convenient but it's going to be the same random values right because we don't really"
3625680,3634480," initialize the uh the seed right so we need to set the god seed right so we are setting the god seed"
3634480,3641680," and that requires including the time all right there we go so as you can see god seed works every second"
3641680,3648640," right so god seed updates every second because this it's the time returns your seconds uh all right"
3649840,3655760," so cool that's pretty cool in my opinion i think so let's go ahead and implement some stuff right so"
3655760,3660560," the easiest thing to implement in here is some uh you just need to add two matrices but you can only add"
3660560,3667360," the matrices if uh the there the you know the shape of the matrix is the same right so you can only do it"
3667360,3675120," like that so we probably want to do assert right that the dst rows is equal to a rows right and"
3675760,3681760," also correspondingly the columns right so we want to check that uh after we check that right so"
3681760,3688640," essentially what we can do we can start iterating this entire stuff right dst rows plus plus i and then"
3688640,3697600," j since the matrices are very much dynamic we can't really check the shapes of the matrix at compile time"
3697600,3708400," right it's very much runtime thing uh right so and uh let's uh go ahead and do that dst ij and we do"
3708400,3716800," addition so it's plus mat at a ij there we go so that's the addition right so that's the addition of"
3716800,3724000," this entire matrix isn't that poggers isn't that poggers i think it's pretty freaking poggers so we"
3724000,3730480," we probably maybe want to test that somehow uh i don't even know so we can allocate a bunch of"
3730480,3738000," matrices right so let's say that we're going to have matrix two by two right and let's keep it sort"
3738000,3745840," of like um zero and then we're going to have another uh b uh right and we probably want to fill the entire"
3745840,3759760," matrix of b with sort of like um once right so maybe you can do matrix few b uh and yeah one okay so maybe"
3759760,3770400," we also want to fill it with once here right and we want to print uh a right then uh we're going to print"
3770400,3773760," something like this in here so we're going to separate this entire thing then we're going to do"
3773760,3781760," matrix uh sum a and b and we're going to print this entire thing yet again right uh so here we just"
3781760,3788640," allocate this entire thing fill it with once this thing fill it with once print a uh add them together"
3788640,3794320," and then print uh the a again just to see if this entire thing works uh that would probably require"
3795600,3803040," uh implementing something like matrix fuel uh matrix m i'm not sure if i'll ever use that operation ever"
3803040,3810960," again uh but it's probably nice to have it anyway i suppose right though it's kind of similar to just"
3810960,3816960," matrand right because essentially what you do is just instead of random value you just put x in there"
3816960,3823280," and that's it right so that's basically what it is and yeah there we go so this was a then we added"
3823280,3831760," another thing in here now it's b or something like that so yeah that's cool we have addition for the"
3831760,3836640," for the matrices we're almost there the the framework is almost finished right as soon as you have these"
3836640,3842560," two operations dot and sum you basically have a neural network framework i don't know why you need a"
3842560,3848640," tensorflow that takes like you know 24 hours to build how many times it requires you to build it's just like"
3848640,3860080," it's two matrix operations uh right i don't freaking know so it's so simple haha okay so here is an"
3860080,3870400," interesting thing right so matrix multiplication is not defined unless uh the inner sizes are equal right"
3872080,3880000," uh the inner sizes are equal people say something uh i'm wrong with index j no i'm not"
3880000,3890960," um you're looping rows twice where where i'm looping rows twice on which function"
3893200,3898720," on which function okay so if chat basically debates me okay so they didn't debate okay so this is"
3898720,3902960," the course okay thank you uh all right"
3902960,3915280," anyway uh okay so what i was doing i was explaining something important i was explaining the matrix"
3915280,3922080," multiplication uh the matrix multiplication right the inner sizes has to be equal right so this is going to be"
3922080,3930480," assert uh a uh so first rows then the columns right so we have to say that the columns are equal to the"
3930480,3939920," rows of b right so essentially uh uh if we have matrix one by two and two by three right they're"
3939920,3946000," multipliable because their inner sizes are equal to two right and so this is the columns and this is the"
3946000,3951920," row rows this is the columns and this is the rows very important but most importantly is that the"
3951920,3956720," size of the size of the final matrix is going to be the outer sizes right so the uh the final matrix is"
3956720,3962400," going to have one row and three columns we also want to check that we also want to check that the"
3962400,3972320," final matrix is dst right so that means that assert um dst the amount of rows is the same as the amount"
3972320,3981280," of rows or a right so this is the amount of rows in here and the amount of columns of this thing"
3982000,3988640," is three which is three which is the amount of columns of b right so it's like a little bit symmetrical"
3988640,3994320," right it's a little bit symmetric and essentially when we do this sort of like a uh you know intersection"
3994320,4003040," thingy we are iterating we are iterating the uh destination table right so each point in here"
4003040,4008240," is the point in the destination table so that means the two outer loops are going to be"
4008240,4016080," b over dst right so this is going to be size t uh less than dst rows right plus plus i right then for"
4016080,4017840," size t"
4017840,4031920," j j dst columns plus plus j and within within that loop right within that loop uh here we iterating the inner"
4031920,4038080," sizes right so since the inner size is the same we're iterating that inner size right so that's specific"
4038080,4046320," value so once we check that these these two values are equal we may probably take uh that value the"
4046320,4052240," value that is equal and just put it into a separate variable so we can use that variable inside right"
4052240,4057600," when we are iterating the corresponding row and corresponding column of these two matrices right so"
4057600,4066240," we can do something like uh size t k less k less k less n right so n is specifically inner size uh plus plus k"
4066240,4076880," there we go so and the uh the value of the destination matrix of i and j because again we are iterating"
4076880,4083120," the destination matrix right so initially it is going to be equal to zero right initially it is equal to zero"
4083120,4089680," and what we're adding we're actually adding the multiplications of these elements so the"
4089680,4096240," product of these two elements plus product of these two elements and so on and so forth"
4096240,4108400," right so the first element is matrix a uh i k multiplied by the matrix uh b k j right and look how we are"
4108400,4116720," determining the coordinates of this element uh their inner sizes their inner sizes are the same so the"
4116720,4125200," first one is i k the second one is k j right and k iterates the inner size and that's the uh this is"
4125200,4130640," where we use that key right this is where we use that key and that corresponds uh to the requirement of"
4130640,4135840," the inner sizes to be equal to each other right so that's one of the reasons why they have to be equal"
4135840,4142240," because we're iterating that inner size right it's it's kind of important right and the uh rows and"
4142240,4147600," columns are outer things and the outer things things this is where we're iterating and that's the"
4147600,4153200," entirety of the matrix multiplication believe it or not if i didn't make any mistakes that's basically it"
4154720,4160640," right that's basically it so we're just like accumulating this entire thing and yeah let's go"
4160640,4166880," isn't it cool does it make sense because uh people find matrix multiplication really scary"
4166880,4173120," but if you think about matrix multiplication in terms of this sort of like um crossing of rows and columns"
4173120,4177280," you can derive all of these things like quite easily and you can see that this entire thing is"
4177280,4184240," sort of like very much symmetrical uh right so in it kind of does make sense all that does make sense um"
4184240,4190720," all right so this is the matrix multiplication"
4190720,4197360," okay uh so what we're gonna do in here what we're gonna do um"
4197360,4206160," would be kind of nice to test that somehow uh but i don't know we can have like a matrix a"
4207040,4212160," which is uh let's say this is gonna be oh yeah we can do something like"
4212160,4225440," yeah one two right this is that and this is gonna be matrix random right so this is a random matrix a"
4225440,4232960," and then we're gonna do two by two and let's fill this one with just zeros right so this one is gonna be"
4232960,4238480," just zeros and we need to have a destination matrix right so and the destination matrix is going to be"
4238480,4246640," uh i suppose uh alloc one by two right because this one is one by two two by two the final is going to be"
4246640,4256960," one by two uh all right so and we can then uh do mat dot dst a and b right dst a and b and then we can just"
4256960,4266720," print uh the final matrix print dot dst and just see how it looks like uh right so let's see if it's going"
4266720,4272400," to to work so something doesn't particularly work we have to provide the values from uh let's say from"
4272400,4280960," five to ten because why not matt uh print i meant print in here and uh yep so that basically filled this"
4280960,4287120," filled this entire thing with zeros right so that filled it with zeros we might as well probably"
4287120,4296160," matt print a and just split this entire thing by a line all right so this was the matrix so we"
4296160,4301680," multiplied it by zero and there we go this is what we got if we fill it with one right if we fill it with"
4301680,4309200," one uh it just like a kind of doubled it but not really doubled it but yeah so i suppose one of the"
4309200,4320000," things we can try to do we can try to define maybe um like id metrics right so float id data right and"
4320000,4327920," this is going to be like a four of these things uh zero like one zero zero one right and the reason why"
4327920,4337680," it's like that is because it's like this and b is essentially going to be something like uh rows two"
4339200,4346880," columns two and elements is just id data right so we're gonna do something like that and that thing"
4346880,4354080," should actually yeah okay it worked so this is id matrix it's just like it keeps it the same keeps it"
4354080,4360960," the same all right so you know what that means chat you know what that means we already have a neural net"
4360960,4369280," framework don't we yeah so because we implemented all of the necessary operations all of the necessary"
4369280,4376880," operations how about it how about that we can try to port uh some of the um some of the models right"
4376880,4382720," or maybe like something like zor model to uh to the specific framework right so we can try to do that"
4383840,4389600," so the question is how we can do all of that right before we do all of that i think i want to make a"
4389600,4395360," small break right because i ran out of t and i also streamed for almost one hour so i think the time has"
4395360,4402080," come and after the break we're gonna take our zor example right so where is the zor example uh and"
4402080,4407440," we're gonna port it to just matrices right so we're gonna do the same thing the same stupid thing uh in"
4407440,4414160," here but in terms of matrices right so in terms of matrices and we'll see how it's gonna go the cool"
4414160,4419120," thing is gonna be is that uh all of these things are parameterizable so if you want to just add more"
4419120,4424480," layers or add more neurons to a particular layer it's just going to be changing the the amount of"
4424480,4429440," those things right that's going to be basically the idea right all right let's make a small break and um"
4429440,4438320," all right let's try to port the zor model to our new framework that we just created"
4438320,4446720," yes we created a brand new uh machine learning framework we it took us one hour and one-handed"
4446720,4458080," lines of code can your tensor flow do that okay i'm sorry uh so uh as we already you know um discussed"
4458080,4464720," so the entire model consists of actually like two layers right the first layer uh is basically uh four"
4464720,4472560," weights right and these four weights uh are effectively a matrix two by two right uh so let's allocate the"
4472560,4480480," matrix uh two by two right so this is the weights of the first layer and on top of that we have biases"
4480480,4486960," of the same layer right and the biases uh is actually one by two because we add them in here right so we"
4486960,4494480," we literally add them in here so this is going to be uh matrix a lock uh one by two this is our first"
4494480,4500720," layer right this is our first layer we can try to indicate that maybe with one right uh this is our first"
4500720,4505040," layer i want to maybe just cross it out but maybe i can just remove all of that because i don't think"
4505040,4514800," it's it's useful in any way right so this is half of our model the next layer right uh w2 right the next"
4514800,4521520," layer has only two weights right it has only two weights uh right and how do we organize all of that so"
4521520,4528080," if you have this thing uh that means it's going to be two by one right so if the activation is a single row"
4528080,4534960," uh the weights has to be a single column which will effectively compress the final result into"
4534960,4541760," single value right so that means uh this thing has to be two by one right it has to be two by one and"
4541760,4547360," the biases right so because there's only one neuron it's going to be a single bias but we can just say"
4547360,4554960," okay so it's going to be one by one right it's a matrix one by one so and this is basically the entire"
4554960,4562000," model right isn't it i think it is uh and we probably want to initialize this entire model"
4562000,4567600," we want to randomize all these things uh right so it's going to be b1 uh right then with two"
4567600,4574640," motor b2 and after that i want to print all of them right after that i want to print all of them"
4574640,4579920," so i'm going to well when we are randomizing all that so it's probably going to be something like"
4579920,4587680," let's say from zero to one i think that's going to be fine uh then uh what are we doing we i want to"
4587680,4594720," see this entire model so i'm going to copy paste this random shite and i'm going to say print this thing"
4594720,4601920," please pretty please boom uh but here's the interesting thing it's not going to be readable"
4601920,4608720," whatsoever because we don't really know the uh like what to what corresponds to what right if you"
4608720,4612720," know what i mean we don't really know what corresponds to what uh right it's like what the"
4612720,4620800," fuck is going on in here like i don't understand so maybe we kind of want to extend print to actually"
4620800,4626480," accept some sort of information but uh i don't know right so when we're printing one of the things we"
4626480,4635920," can do we can maybe wrap this entire stuff into something like this right uh huh right so this is"
4635920,4644240," going to be something like that and let's indent one two three four uh with four spaces right so it looks"
4644240,4650960," a little bit better maybe so we also have to put this thing uh and this thing of course that's already"
4650960,4655680," better right that's already better so this is the weights of the first thing then the biases of the"
4655680,4659200," first thing then the weights of the second thing and the biases of the second thing that's already"
4659200,4665040," better we can see some shit but we don't really know what corresponds to what maybe we can make uh"
4665040,4670560," matte print accept the name of the matrix right so let's accept the name of the matrix so we can do"
4670560,4679120," something like s equal and the name right s equal and the name and we probably want to modify this thing"
4679120,4687680," because it's c right and now we'll be able to maybe even do some epic shite uh like so"
4687680,4697120," yeah so we specify the name uh and there we go so now we can see all of that stuff in that isn't that"
4697120,4704560," cool isn't that cool i think it's pretty freaking cool uh so yeah we can clearly see everything now"
4705040,4712400," so another okay so this one is kind of funny right so it puts a lot of though it's kind of nice that it"
4712400,4717520," actually puts so much space in here i think right i think it's kind of i'm gonna keep it like that for"
4717520,4723680," now uh right so it would be kind of nice like if i didn't have to provide this name somehow"
4723680,4731840," right we can always do something like matte print right and let's accept the matrix in here and let's just"
4731840,4737840," forward it forward it to matte print but also in c preprocessor there is a very cool thing where you"
4737840,4743280," can take the input token so this is actually a collection of tokens so you can say okay stringify"
4743280,4749760," that and it will turn it into literally a string literal that way you can just do something like this"
4749760,4757920," all right and effectively it will do that for you yeah there we go so and no matter what kind of"
4757920,4761920," expression you put in here you can actually do something weird like you can take a pointer to"
4761920,4768480," this thing and then dereference this pointer right and it will actually include taking the pointer and"
4768480,4774080," dereferencing the pointer and use it as a string in here right so you we can put like an entire expression"
4774080,4779840," in here uh just to sort of like indicate uh what exactly we're printing in here and this is rather"
4779840,4784400," extra convenient we can actually make it like a part of the framework why not who said we can't do that"
4784400,4792400," right so it's just like an additional thing uh right can your tensorflow do that i don't think so"
4792400,4798720," i'm sorry like i hope it is obvious uh that i'm joking"
4798720,4806960," i hope it is obvious then when i say this stupid i'm joking because here's the thing"
4808800,4814640," i quite often uh cut out these kind of moments when i say some stupid controversial controversial"
4814640,4820640," shit on twitch and i post this clip on eclipse on twitter and there is a quite a few people on twitter"
4820640,4824720," that come to me to tell me dude this is so fucking dumb you're fucking like what the"
4824720,4829120," fuck is just like they're fucking triggered or something is that like isn't that obvious that i'm"
4829120,4835920," joking right or or this is something specific to people using twitter like i don't fucking know"
4835920,4842960," i think this is something specific to twitter right because sometimes it's just so fucking funny yeah"
4842960,4852240," and the twitter makes you question are you like is it really obvious that you're joking or maybe"
4852240,4856320," like i'm just saying that too seriously to the point that people cannot tell that i'm not joking or"
4856320,4863040," something it's just like it's probably twitter right so yeah i should not probably think too much about"
4863040,4868880," shit i see on twitter twitter twitter is very very dangerous um anyway so we've got some sub i think"
4868880,4874320," i didn't acknowledge nazriel x uh thank you so much for what you want subscription uh thank you thank you"
4874320,4882320," thank you the read already right uh like when you posted the teapot implementation with twinkles yeah exactly"
4885600,4894480," um"
4894480,4896640," um i made a tweet where i said if you write this uh that you're weak and don't trust"
4896640,4902080," yourself and have gotten 50 hundred replies telling me that they write tests to make sure things don't break"
4902080,4909520," oh yeah you you can't joke on twitter like it's just like twitter does not allow you to do that right so"
4909520,4919840," all right so let's continue um"
4919840,4927920," anything said on twitter loses its constant immediately as soon as opposed to them twitter"
4927920,4934480," like you cannot have contacts on twitter like at all uh all right so we'll get this thing so the question is"
4934480,4940320," how do we even forward information in that neural network like how do we even do that"
4940320,4945520," so since forwarding information is actually matrix multiplication we need some sort of a"
4945520,4949680," matrix that represents the input right so let's actually call it something like x"
4949680,4955840," right and uh yeah we'll already call it x and it's one by two right so here is one by two"
4957200,4964160," uh let's do matt alloc uh one by two and this is where you're gonna put uh your sort of input"
4964160,4974160," so forwarding all right so how would you do that so matt at uh row zero x row zero zero let's put uh"
4974160,4982960," zero in here and maybe one in here right and to forward that information to the next layer you would need"
4982960,4993680," to multiply that by w1 and add b1 and then uh sigmoid f this entire thing so this is more of a pseudocode"
4993680,5000960," right it's it's not going to work this is not how it works uh excuse me uh so but here's an interesting"
5000960,5007440," thing we need an intermediate matrix into which we're going to be multiplying things because i've already"
5007440,5012080," said multiplication uh like it requires you in intermediate matrix right so because i don't"
5012080,5017600," want mathematical operations to do any memory management i want to be all the memory management"
5017600,5022560," to be concentrated in a single function right so this is the only function that does any memory management"
5022560,5027920," uh the rest of the things don't even allocate anything so this is to keep things in control"
5027920,5034960," so to speak if that makes any sense right so that means that we need an additional sort of like a"
5034960,5041280," matrix in another matrix right for the first activation right so this is going to be let's"
5041280,5047440," call it a1 right and this is going to be the matrix for uh for this thing right so this is going to be"
5047440,5053280," that so matrix a look and it's going to be one by two right and this kind of represents like the full"
5053280,5058400," layer so the layer not only will contain the weights and biases it will also contain the sort of"
5058400,5064880," intermediate activation things right intermediate activation things uh right so and if i want to"
5064880,5075040," forward something i'll have to do mat dot all right the activation and i'm multiplying x by w1"
5075040,5084800," right so that's what i do uh and as far as now a1 right a1 has the same size has the same shape as b1 as"
5084800,5091280," the biases right so there's the same actually swapping them uh like swapping them out and the"
5091280,5098400," only thing that changes is the is the letter in here right so that means now i can do another operation"
5098400,5109840," mat uh sum a1 b1 right there you go so here i set up the input right i set up the input and i passed x through the"
5109840,5115680," first layer i passed x through the first layer uh the only thing that is left to do in here is to"
5115680,5122960," iterate through all of the elements of a1 and activate them meaning uh apply sigmoid on them or any other"
5122960,5129360," activation function right so uh let me try to do that so maybe we're gonna have i don't know first of all"
5129360,5138560," we need to have a sigmoid sigmoid f float f i'm gonna accept so sigmoid is a function that takes any"
5138560,5144720," value from minus infinity to plus infinity and squashes that to arrange from zero to one"
5144720,5150320," literally that's the that's everything this function does so the closer your value to the"
5150320,5156320," minus infinity the closer the value of sigmoid is going to be to zero the closer the value to the plus"
5156320,5161600," infinity the closer the value of sigmoid is going to be to one right whatever value you you put it's"
5161600,5167360," going to be uh basically from zero to one it doesn't matter uh right so which is very useful for neural"
5167360,5171680," networks it has its own problems when you start doing back propagation but we are not doing back"
5171680,5179360," propagation yet so that's why we don't care that's the way to address problems of vanishing gradient just"
5179360,5186160," don't use back propagation i mean yeah it is slow just buy more computer right so buy more compute"
5186160,5193600," who cares just convince microsoft to give you like billion dollars or something and it worked out for"
5193600,5202640," open.ai infinite descent yeah uh okay so and the formula is rather simple i think it's like one"
5203680,5214000," divided by one plus exponent uh exponent minus x i think that's basically what it is right so that's"
5214000,5222000," basically the sigmoid and uh the thing we want to do we want to have a special thing that activates the"
5222000,5229600," matrix right so uh i'm thinking so i'm gonna just probably call it mad sick right so and essentially"
5230160,5234560," it's very simple it's very simple right it's gonna iterate through all of that and apply that function"
5234560,5257760," there so let's go and then for size t j j less than coms uh plus plus g right mat at m i j uh sigmoid"
5260720,5265520," just the same element in here there we go so regardless of the shape is gonna just go through"
5265520,5271200," all of the elements and just apply this sigmoid on them so uh when we're gonna have more activation"
5271200,5277840," functions i think i'm gonna try to abstract that away somehow and we'll see how we can do that also"
5277840,5284800," when we're gonna start doing like a proper gradient descent with back propagation um so having like a"
5284800,5289200," variety of activation function is gonna be kind of difficult because different activation functions have"
5289200,5296880," different derivatives and derivatives of activation function it makes code not really flexible right so"
5296880,5303600," because they are kind of bake a specific formula into uh into the algorithm but we'll see what we can do"
5303600,5308880," like we'll see what we can do so we're gonna have a separate stream for uh back propagation and stuff like"
5308880,5317360," that okay so uh we do that and after that we want to do a mat sig a1 let me go so this is a single path"
5317360,5325440," through a single layer right so we had uh essentially input x we passed it through the first layer and we"
5325440,5334320," got a1 right we got a1 so now what we have to do we have to take a1 and pass it through the second layer and"
5334320,5340080," what is the second layer right so the second layer is uh is this thing but again uh to pass it through"
5340080,5344560," you need another intermediate sort of matrix because it's it's actually going to be one by one matrix"
5344560,5350000," because it's a final thing in here right so because of that this matrix is not going to work for that"
5350000,5358480," uh right so let's just allocate a2 mat a log and you can kind of already see how you can uh create an abstract"
5358480,5366320," representation of a neural network right so uh because i'm doing that explicitly uh so we can clearly see"
5366320,5374160," how you can abstract that away into more like higher level structure right so um because essentially neural network"
5374160,5380640," it's going to be array of weight matrices array of bias matrices and array of this intermediate"
5380640,5386480," activation values matrices right so and once you you have that you can just like have a very abstract"
5386480,5395760," algorithm of forwarding and back propagation and stuff like that um so okay so let's go so it can be one"
5396560,5407600," and use use you use okay so mat dot so a2 we're going to take the previous activation activation from a"
5407600,5414320," previous layer apply the weights of the current layer right there we go then we're going to apply"
5414320,5422080," the biases for the current layer and then we're going to activate a2 and there we go so essentially here"
5422080,5428800," it was the input so access we pass the axis through the first layer then we pass it through the second"
5428800,5437760," layer and at the end we got the final result right uh we've got the final result and a2 as you can see"
5437760,5443840," we allocated as metrics one by one uh right is the final result since it's a zorn a neural network right"
5443840,5450480," two values got compressed into into one right so and you can get uh that final value since it's a one by one"
5450480,5456160," you can just like literally take the element from here and just like dereference it and this is basically"
5456160,5469680," your y right if essentially you have values x1 uh right zero and x2 one you put them uh in here one two"
5469680,5477600," you follow this entire process and y is going to be here right why is going to be here"
5479840,5487760," so we can even maybe abstract that away to a separate um a separate like a forward function"
5487760,5494320," but that separate forward function is probably going to accept this entire model right is going to accept"
5494320,5501600," this entire model uh so maybe it makes sense to maybe create a structure in here so let's actually"
5501600,5513280," call it soar uh like that and maybe we want to define all of these variables in here so you can think of"
5513280,5520880," the x layer as just like also intermediate activation uh activation matrix so maybe because of that we can"
5520880,5527520," call it a zero right so this is going to be sort of like a zero so this is the first one uh right after that"
5527520,5535120," we have uh things like b1 and b2 one of the things we can do is just like this right so this is the first"
5535120,5543040," layer we can clearly see that the first layer and uh the second layer is just like this too right and"
5543040,5548800," if you want to add another layer well you know what to do right so you just add another thing in here but"
5548800,5556240," then you'd have to take the uh the shapes of these mattresses right so and so the cool thing here is"
5556240,5561920," that it's super easy to just stack layers on top in here and again so you can clearly see how you can"
5561920,5568160," uh simplify all that you can turn all of these things into arrays right you can turn all these"
5568160,5573680," things into arrays so now it becomes super flexible but we're not going to make them into arrays yet right"
5573680,5578080," because we want to like have a working model first and then we'll see how we can how we can improve"
5578080,5586080," all of that so uh okay so let's basically create a model m in here right and for all of these things"
5586080,5588560," we're going to initialize it like so"
5588560,5599040," so this is going to be so and this one is probably has to be like a zero then we need to initialize the"
5599680,5605040," these things uh so the intermediate activation things don't really need to be randomized because"
5605040,5610720," they are intermediate values anyway so who cares uh right and this entire thing can be turned into"
5610720,5618720," like a forward function right so let's actually take this entire thing and let's say float forward"
5618720,5626080," zor right we're going to accept accept a model zor uh we're going to accept x actually x1 and x2 right we're"
5626080,5631600," we're accepting x1 and x2 uh right and in here we just like return this final thing in here"
5631600,5639440," right and we can only take these matrices all of these matrices from m itself so it's going to be like this"
5639440,5644560," right it's going to be a actually a zero right so this is a zero"
5647040,5654000," um a one right uh so this is m a zero"
5654000,5664720," and m a one you can you can already kind of see how you can just wrap this thing into"
5664720,5671440," into a for loop right so if you have like several layers like n layers you could have done something"
5671440,5678320," like less than n right and in here you would say okay like i and this one is going to be i minus one"
5678320,5682880," like a previous layer and stuff like that you can already see a pattern emerging in here so"
5682880,5689520," uh right so you can already clearly see the pattern emerging here and this is the reason why i'm writing"
5689520,5696080," all of that copy paste explicitly so the pattern becomes as explicit and as loud as possible to sort of"
5696080,5702960," like make a point yeah that's how you're supposed to do that like uh this is one of the powerful"
5702960,5708800," things about copy pasting people are afraid of copy pasting people hate copy pasting but it's a very"
5708800,5713840," strong indication of patterns that needs to be factored out it's sort of like a strong confirmation"
5713840,5719600," like a loud signal yes that's how we have to do that and keep confirming yeah you can see how you can"
5719600,5727920," abstract that away but let's keep driving it in that direction right so uh right anyway so we also have"
5727920,5734880," a print it would be also kind of nice to to have like a print of this thing but whatever okay uh so uh"
5734880,5741280," let me try to compile this entire thing just to see if it uh if it compiles so exp we don't have really"
5741280,5753280," exp so let's include the math uh right math uh let's include the math so x one uh okay so we don't have to"
5753280,5761120," do that stuff and uh okay so what else do we have in here a2 this one is available in here there we go"
5761120,5769840," everything is uh fine everything is fine so now we just got a random zor model right so we just allocated a"
5769840,5779200," bunch of stuff we randomized it uh let's try to see how it works at all right uh then uh we're gonna"
5779200,5786160," just like iterate through like we are building a truth table like we're building a truth table uh j"
5786160,5794320," just to see how this entire model performs uh right so we're gonna do forward zor i provide the zor"
5794320,5801040," model uh i and j right and that gives me something so i might as well actually print this entire thing"
5801040,5810320," right away all right so f uh so it's gonna be like this uh i and j and let's see how it performs it"
5810320,5816800," should perform like a garbage yeah there we go so it basically returns like you know the same stuff"
5817840,5823280," right so but it works right so we essentially just perform a sequence of metrics multiplications"
5823280,5829040," and additions uh and then activate them and that's the the whole passing through this entire stuff this"
5829040,5835520," is the whole passing uh all right so let's try to compute the cost of this entire function to compute"
5835520,5841200," the cost of this entire function we'll need like a training uh training data so what i'm thinking is that"
5842240,5850640," what we can accept right so let's do cost uh we excited accept the model and we can accept a couple"
5850640,5859920," of matrices right so matrix training input and training output right so training input and training output"
5859920,5866400," and essentially uh training input is going to be the stuff that you are going to put in here right and"
5866400,5873680," training output is basically what you expect in here right so and one of the things we want to confirm"
5873680,5882720," by the way is that the amount of rows between them is the same right so essentially each row is like a"
5882720,5888160," training sample right so input and output and it has the same amount of rows like input and output has the"
5888160,5894960," same amount of rows so we can basically now start iterating those things right so we can start simply"
5894960,5904160," iterating those things and just working with them right and just working with them uh so less than n and"
5904160,5913680," there we go so interestingly we could have actually accepted huh we could have yeah"
5918080,5921760," what if we don't accept what if we don't accept any stuff in here"
5921760,5933040," yeah because that's that's kind of funny but you can just like set those things directly you can just"
5933040,5940640," set the input and then get the output directly right so this entire thing it just sets the intermediate"
5940640,5947600," things and you set the input and output directly so here then uh what we do we just do we"
5947600,5955680," we set up the input right then we forward that zor and then we take the output which is located at"
5955680,5964480," a2 es in here right so there we go so we we supply the data for the input we forward it and then we took"
5964480,5971040," the data out of the other end right so that's that's an interesting api i suppose we can work with that"
5971040,5979040," because why not right and that thing is just like forward yeah all right so and uh the thing we can do"
5979040,5986080," in here right i think we can do in here i want to take the current row right i somehow need to take the"
5986080,5990240," current row and i need to set that current row to here"
5990240,5994960," yo wait a second"
5994960,6003600," this is where the idea of sub matrices comes into play actually"
6003600,6012320," what if i okay so i know that ti is a bunch of rows and each row is a sample right so we can say that ti is"
6012320,6021760," roughly something like uh right so zero uh one and blah blah blah blah right can i have something like"
6021760,6031840," mat row mat row which accepts the uh the number of the row and turns that specific row into uh a vector"
6031840,6044480," into the matrix so then i can take the the input matrix from the model and say okay copy uh copy that row"
6044480,6050960," into that matrix and it kind of abstracts away the specific sizes and stuff like that"
6050960,6058640," this is actually kind of cool isn't it so yeah so taking sub matrices is actually useful"
6059440,6063120," i didn't think i didn't know about it right so huh"
6063120,6070800," that's cool it's actually super cool so i think we need to implement this operation i think this"
6070800,6076080," separation is going to be very very useful so let's go ahead and do that so we need like a thing that takes"
6076080,6082880," matrix row which is rather easy but it's difficult to do something like matrix column right so you can't"
6082880,6089760," really take this thing as column because that already requires stride because you need to be able to"
6089760,6096880," skip some of the elements within the sub matrix and that's like yeah so if if you if you need mat row"
6096880,6101360," you may also need mad column and if you need mad column you'll need to introduce a stride so we'll"
6101360,6106800," probably need to introduce a stride uh so that's actually okay it's very interesting it's kind of"
6106800,6115840," cool like developing and just like discovering these kind of things so yeah okay uh all right so uh let's"
6115840,6125760," just introduce these things um mat row right mat m uh size t row right so that's the role we want to have"
6125760,6130240," and mat copy right so this is going to be destination and this is going to be the source"
6130240,6138720," right so we're copying two things uh right so let me find mat sum and this is where we're going to"
6138720,6149840," put all these things in here right uh huh so metro how can we take a metro we can return uh we can return"
6149840,6162480," something uh like this okay so row and zero and since mat at actually uh returns like an expression"
6162480,6168160," that points at actual element in here one of the things we can do is literally take a pointer to this"
6168160,6173520," thing and that's going to be the beginning of that specific row so that means i can construct a new"
6174160,6181200," uh thing in here right so let's actually put it like this uh this is going to be the elements right"
6181200,6186880," so this is going to be the elements the amount of rows is going to be one right so this can be single"
6186880,6193760," row and the amount of columns is going to be the amount of columns within uh that specific matrix right"
6193760,6199040," and then we can just return this entire stuff and this is the matrix that we've got and this matrix is"
6199040,6206160," just sort of like a lightweight entity lightweight entity this should be a very easy operation to"
6206160,6218480," perform yeah that's cool all right um that was rather easy okay so now copy right to copy one matrix"
6218480,6223760," to another one uh we need to make sure that their shapes are equal right so we need to do something like"
6223760,6233280," and then assert uh destination destination rows equal to src uh src rows and the same goes to the columns"
6233280,6240640," all right uh and let's iterate through rows and columns in here so this is going to be dst rows"
6242160,6255360," and then size t j dst columns plus plus j right and everything uh looks okay again right so mat at dst ij"
6255360,6267600," equal to mat at src ij easy peasy lemon squeezy uh easy peasy lemon squeezy so and uh yeah so that makes it"
6267600,6274640," super easy to just take this specific sample uh this specific sample uh this specific sample and uh"
6274640,6283920," supply it in here right uh just supply it in here uh and then we can just forward zor right we're just"
6283920,6294240," forwarding zor and we get the output as a2 uh and here's an interesting thing so a2 has to be kind of"
6294240,6301120," like now we need to compute the difference between uh the output row which also could be a row right so"
6301120,6309600," we can say it's something like um row to i and this is the output that we expect maybe we can even say"
6309600,6316640," something like x like so this is this thing and then here we could have something like y which is like"
6316640,6326320," expected uh right which expected in here uh right which expected in here so this comparison right so"
6326320,6335120," we need to now compare y with uh a2 right we need to compare y with a2 which means that the amount of"
6335120,6343040," columns of this specific uh thing has to be equal to the amount of columns of this thing so this is another"
6343040,6349600," condition that is very important in here and which we probably want to put uh into assert here just in"
6349600,6357920," case it's it gets violated at some point uh right it gets violated at some point so uh yeah"
6357920,6369520," right and because of that because of that i think it probably makes sense to actually have uh this amount of"
6369520,6374400," columns as a separate variable in here maybe i'm gonna actually well it's already we already have"
6374400,6381760," m god damn it i already have m let's actually call this x right because it's zor right so this is like"
6381760,6392000," god damn it okay whatever i can uh maybe call it q so all the variables are already taken all the all"
6392000,6396960," the names so the one variable names already taken and that's why you should not use one letter names"
6396960,6404000," that's the actual reason because they you run away of them very quickly uh right so this is going to be"
6404000,6408560," something like this and uh what we need to do now we need to just compute the differences between them"
6408560,6421040," right so essentially uh at so the actual output uh a uh two right the actual output at row zero but column j"
6421040,6430720," right so row zero but column j minus uh whatever we expected in y right whatever we expected in y in here"
6431440,6437280," and that's sort of the difference right that's sort of the difference and we need to take the square of"
6437280,6441760," that different difference and add it to the cost function right so we need to add that stuff to the"
6441760,6447920," cost function uh and uh the cost function here is going to be initialized with zero so as you can see we're"
6447920,6454720," just like uh you know populating the the entire cost function where is it okay i lost it okay and after we"
6454720,6460000," populate it like the sum of all of the uh all the samples we need to divide it by n by the amount of"
6460000,6469200," samples and that gives us the cost like that literally gives us the cost um so could be kind of interesting"
6469200,6479440," so what effectively this is this is a square distance of the stuff right between these vectors can we"
6479440,6493360," do something about that but maybe not anyway so uh now what we can do we can just print the cost of this"
6493360,6501920," model right so let's actually print the cost so cost uh what does accept cost and uh yeah so we have"
6501920,6507360," to provide the model and we have to provide ti and to all right so but we don't have them right we don't"
6507360,6515840," have them so let's define right so let's define the training data right so maybe we can define it"
6515840,6523360," like ti uh right which is going to be just this right so this is going to be just this um"
6523360,6534960," yeah so zero zero i i would like to have like all of the training and expected data in here i feel"
6534960,6539440," like i feel like i feel like i want to have that like this uh right and then we're going to swap it"
6539440,6545040," around and since it's exover like here we have to have a zero so this is the initial training data"
6545040,6554000," and one of the things i want to be able to do right i want to be able to uh take sub sub canvases in here"
6554000,6560560," right so not sub canvases but sub matrices right uh sub matrices so i want to be able to take this as a"
6560560,6565280," sub matrix and i want to be able to take this as a sub matrix as well but it's not impossible"
6565280,6570800," column wise because we don't have a stride maybe one of the things we have to introduce in here we"
6570800,6580000," literally have to introduce a stride right size t uh stride okay and but that requires setting up that"
6580000,6586320," stride every time we are constructing anything right so we need to find all the places where we for example set"
6586320,6592080," the rows okay so that's cool when we're allocating something we can always set the stride to the"
6592080,6598160," amount of the columns right so the actual width is there any other places okay so this one is interesting"
6598160,6607680," all right so when i take a row right i think i have to uh reproduce the same stride as this original"
6607680,6613360," stride of the matrix right because here i effectively take a sub matrix anyway right if i had this a certain"
6613360,6621120," stride yeah that makes sense right so though since it's a single row anyway stride is not particularly"
6621120,6628240," needed right stride is not particularly so is there any places where we set the rows yeah so that's the"
6628240,6636800," only place so yeah that's cool and essentially when we do at instead of doing columns we have to set the"
6636800,6644720," stride right so i'm really curious like i hope we don't try to access es directly and in the place"
6644720,6650000," places where we access es directly we do take stride into account so here we do take stride into account"
6650000,6656640," so everything seems to be fine here we also do take stride okay so all of the access to the elements here"
6656640,6661280," is actually uh abstracted away and i'm super happy about that so"
6661280,6675120," i'm super happy about that cool uh so and i wonder like now we need to do um you know a sap matrix"
6676000,6684240," so this is the row right so can we do something like mad uh sub right matt m uh size t"
6684240,6694080," how can we take that i i don't really know how to come up with this thing in here but i think i'm"
6694080,6700560," i can just do that manually i think right let's just define it manually i think it's gonna be easier"
6701280,6713040," um right we need one way to decide the amount of samples in here right so we can say that the n is"
6713040,6721680," actually size of this thing divided by the size of this thing right actually td zero right so that gives"
6721680,6727040," us the amount of elements in here and we have to divide it by three and that gives us the amount of"
6727040,6732480," samples and that's very important because this is the amount of rows right then i can take the ti right"
6732480,6738480," and here i'm gonna say okay the amount of rows is the n right the amount of columns though is two"
6738480,6749680," uh stride is three stride is three uh so maybe we can even have this as a stride so we can reuse it"
6749680,6757680," like that so this is going to be the stride so this is a stride and the elements start at td right so the"
6757680,6768800," elements start at td so and then i'm going to do to uh right so rows is n columns is two right stride"
6769600,6778720," is the same but we start at a different place we actually start at um zero one two right i suppose"
6778720,6784880," we start at two or maybe we can even do something like td plus two so that's it uh we can even confirm"
6784880,6793920," that everything's fine so let's do matrix print print ti so we print ti and to all and let's see if it"
6793920,6800880," actually successfully split this data into two separate matrices so the first matrix has to be this"
6800880,6805920," uh the second matrix has to be this right so that's what we're trying to do here let's see if it's going"
6805920,6811680," to work doesn't really compile which is fine conflicting types uh math row why are they"
6811680,6820080," conflicting oh okay why was it okay before uh that's weird i don't know why i don't know what"
6820080,6828960," happened maybe it never worked closer closer all right so a forward implicit declaration uh that's very"
6828960,6835040," interesting so why is it implicit oh this is because it's defined down there so we have to do that in a"
6835040,6844160," different order we have to do that in different order and uh boom did that work i don't think so"
6844160,6850480," i think that didn't work because we have two columns in here and i think i did an oopsie doopsie and"
6850480,6856880," potentially a stupid fucky wacky uh right and i literally put columns two in here but i have to have"
6856880,6864080," one and there we go i think i fixed the problem so let's actually compare this kind of shit uh yep so the"
6864080,6871520," actual data here looks like this right so it's continuous in memory but our structure for"
6871520,6878480," matrices allows us to carve this as a separate matrix and this as a separate matrix without doing any"
6878480,6884160," copies or anything like that so we have some data in the memory and matrix data structure right this"
6884160,6891680," specific matrix data structure let's actually find it is just a view on that continuous data of flows right"
6891680,6896960," and we can basically represent them however we want right so i just basically carve out this as inputs"
6896960,6901920," and this as outputs but i can actually store that like continuously in memory if i want to so i think"
6901920,6909040," this is a very interesting approach it's kind of like similar to canvases in all of c um so that's"
6909040,6913840," actually very interesting like a flexible way of working with that so yeah so you you have all of the"
6913840,6918880," parameters of the parameters of the neural networks like continuously in the memory and then you just"
6918880,6925600," like take sub views for specific layers and stuff like that furthermore the entire data for this"
6925600,6931520," neural network could have been just a continuous array of floats we can just pre-allocate continuous"
6931520,6938160," array of floats for all of that and for these specific matrices we can just like specify sub like views on that"
6938160,6946480," specific memory and you know why this is cool because it will allow to just dump the current"
6946480,6953520," neural net state into the uh hard drive and that's it without creating any special formats or anything like that"
6953520,6957040," you just like dump it into the hard drive and that's it and then you load it up again"
6958640,6963440," very easily because it's a continuous array of floats anyway it's just like how you view that array that's"
6963440,6968240," what makes the topology and stuff like that but you also need to store information about how to do topology"
6968240,6972880," like at least the uh the sizes of the layers and stuff like that to pre-allocate all that"
6972880,6975280," it's actually kind of cool"
6975280,6982720," well you don't you probably don't want to save the intermediate values in here but you can actually"
6982720,6991520," you can actually have this intermediate values like a1 a2 a3 right so in a separate continuous space that"
6991520,6999120," you don't dump right so you have a special continuous space of uh parameters that you do dump but you you"
6999120,7004720," have a separate thing for these intermediate things and they don't really intersect too much uh yeah"
7006240,7013200," it's actually kind of cool yo this gives me so many this is so cool why am i so fascinated by such a"
7013200,7021280," simple thing it's just like yeah and because like this this is cool because it gives you serialization"
7021280,7027840," automatically it gives you serialization automatically uh the beauty of the matrix for"
7027840,7033120," propagation is that you can use matrix instead of a vector as the input and you'll get a vector of"
7033120,7039360," outputs uh if you fix the uh adding of the bias well yeah that's literally what we're doing here right"
7039360,7046400," so that's that's basically forwarding so multiplication bias activation multiplication bias activation we haven't"
7046400,7052880," abstracted away the amount of layers yet but we're getting there so"
7058800,7065200," okay so let me let me let me see what we've got let me see what we've got um"
7065200,7073920," yeah so everything seems to be working so now i can just like pass ti and to into the cost function"
7073920,7078800," right so i was doing that specifically because i wanted to compute the the cost function uh right so"
7078800,7087760," maybe i want to uh disable the final verification of the model right the final verification uh and let's see"
7087760,7089920," how it's gonna go so this is the cost function that's cool"
7089920,7098560," why don't you store only one activation layer and use it as an accumulator because it has to be"
7098560,7104320," different shapes right because the the activation layer changes its shape depending on the specific"
7104320,7110400," layer right so because once you pass the input through this layer you have activation of like you"
7110400,7116000," have two activations once you pass two activations through that layer you have one activation uh if you"
7116000,7120960," have more complicated things so as you can see these things sort of varies and stuff like that but you"
7120960,7129760," can probably have a single um a single continuous buffer buffer that should vary right that should vary"
7129760,7137280," uh right but i don't want to do that yet because actually having these buffers being independent things"
7137280,7143600," is very useful for back propagation but i don't want to go into that yet right even though they're intermediate"
7143600,7148000," and for activation they're actually very useful when you start coding back propagation because you can"
7148000,7154000," store intermediate things in there as well right you can use them not only for just activation but you"
7154000,7159760," can go with them in a different order uh so i'll demonstrate that on the on the next streams right so"
7159760,7175520," now so we've got the cost right so the time has come to implement like finite difference for this"
7175520,7181600," entire cost right so let's let's find how we can do that uh finite difference so the thing about finite"
7181600,7188560," difference is that uh it kind of returns a gradient in air quotes because it's not really a gradient right so"
7188560,7193360," let's find a difference because of that we need to accept two things in here so accept the model first"
7193360,7198720," and then we accept the the gradient that we're going to have in here and also an epsilon by which we"
7198720,7206320," wiggle the parameters in here so by which we wiggle and what we need to do in here we need to do"
7206320,7213200," copy paste again right so we need to do some copy paste again uh so this is going to be a safe parameter"
7214000,7219760," all right let's iterate through the first uh weights right so we're going to be basically"
7219760,7223440," iterating through all the parameters of a neural network and we're going to be wiggling them and"
7223440,7230800," competing the cost of them so to actually have the original cost let's do cost uh then the model"
7230800,7245600," and ti to which we'll probably want to pass in here as well uh so let's iterate through the m uh w1"
7245600,7251360," rows right so iterating through this entire thing so we'll still need to do a couple of like a copy paste"
7251360,7254240," and stuff like that all right so it's going to be j"
7254240,7268160," so let's save the current parameter matrix at m w1 um i n j so we save this entire thing then we are"
7268160,7277040," wiggling it by that specific epsilon right and then we compute the cost after modifying this entire thing and"
7277040,7285120," subtract the original cost and divide it by that little wiggle and we set it to the gradient right"
7285120,7291680," so it's a very dumb way of doing that i know but i do that um intentionally right for the sake of"
7291680,7298400," simplicity right so we're not doing back back propagation yet and then i'm restoring the uh the"
7298400,7305520," original value saved in there right okay so that iterates through all of the weights of the of the first"
7305520,7313200," metrics and we kind of want to do a similar thing for the for the biases right so in here we'll have to do"
7313200,7326000," w1 b1 similar thing right so and now we want to do this for w2 right we want to do that for w2"
7326800,7335200," let's quick replace w1 w2 boom and guess what we want to do that for b2 right"
7335200,7350960," uh i know what i'm doing i'm a professional software developer trust me i know what i'm doing right so as"
7350960,7357840," as i already said copy pasting is useful because it's a strong education of places that needs to be factored"
7357840,7362160," out right so no i know i know it's beautiful uh"
7362160,7374880," anyway so okay so we have a finite difference and yeah now what we need to do we need to sort of pre-allocate"
7374880,7384480," a similar thing for uh for the for this thing right so maybe we can have something like so we have zor"
7384480,7396080," right we can create zor zor alloc right and it's not going to accept anything what it's going to do"
7396720,7405760," is just this right and just return this entire thing so we can reuse it at least two times right so this"
7405760,7415280," is the original model zor alloc uh and this is going to be a gradient right gradient zor alloc"
7415280,7420240," so we don't really need to uh you know initialize the gradient it doesn't really matter"
7420240,7424880," and uh what we want to do we want to do finite difference in the finite difference we provide the"
7424880,7431440," model the gradient the epsilon which will be go i usually like to pick something like uh one tenth"
7431440,7440800," and uh the input output data right so that's what we got so after that we need to uh basically apply"
7440800,7446160," that gradient right so we need to apply that gradient with a certain learning rate so let's create a"
7446160,7453440," function learn which accepts the model and the gradient right uh and the the rate with which it is learning"
7453440,7461040," right right so basically the rate uh we're gonna do another naughty thing another notice thing in here"
7461040,7473120," like so uh where we take the parameter and we subtract the gradient parameter multiplied by the learning rate"
7473120,7477120," like so it's a little bit smaller but still kind of a little bit smaller but still kind of naughty"
7477120,7484560," kind of naughty so we we don't really need this stuff anymore and then this stuff anymore and"
7484560,7493120," uh here we have b1 this one is w2"
7495200,7501840," b2 so that's how we apply that all right that's how we learn uh i can say maybe it could be called"
7501840,7510000," zor learn uh right so we've got the original model we computed this cost then we found the finite"
7510000,7519120," difference gradient so to speak then we apply the gradient right so learn m g with a certain rate i found"
7519120,7526720," that the rate of the same side as epsilon kind of works well and i want to now print uh you know the"
7526720,7536000," second cost after the modification uh sorry i mean the compilation uh will tell me okay so saved uh yeah"
7536000,7542400," that's why you should not copy paste chat never copy paste because you copy paste errors right"
7545600,7555760," all right did that reduce the cost function i think it did i think it kind of did reduce the cost function"
7555760,7562880," i'm reporting you to uncle bob with all this naughtiness uh i will complain to casey"
7562880,7566720," so he will protect me from uncle bob anyway so"
7566720,7574720," yeah we can maybe increase the learning rate but we'll see so and what that means is that"
7575440,7582240," we can uh we've got uh we've got somebody throwing money at me without knowing that i don't get any of"
7582240,7590080," that for some reason i can't see money money money money where twitch to this this page is dead"
7590080,7604880," yeah this page is dead uh i wonder oh i see okay nice um i have to reload everything now"
7605280,7622240," you thought machine learning is hard you know what's harder web applications it's the most difficult"
7622240,7630560," program uh computer science problem in planet earth so uh thank you so much max nothing for uh 1000 beats"
7630560,7635280," thank you for giving gifting money to jeffrey bases because i don't get any of that but thank you thank"
7635280,7641440," you thank you thank you uh sam was in lar thank you so much for five community gifted subs uh thank"
7641440,7648880," you thank you thank you thank you already already already already already already cool"
7650880,7659680," so what do we have uh use you so that means we can just repeat this process right we can repeat this"
7659680,7668160," stuff several times and drive the entire cost maybe down maybe i don't know let's repeat that 10 times"
7668160,7673440," uh right let's repeat that 10 times is it going to work it's kind of going down not going"
7675840,7682240," uh we can probably also print uh the current iteration just to see how well it goes and let's"
7682240,7688640," try to do that a thousand of times is it going to at least it went below 25 which is already good"
7688640,7696320," indication right so if you usually if you don't have enough uh neurons or will just stay at 25 i think"
7696320,7702000," right it will just stay at 25 so that's that's actually a good indication that we're doing something"
7702000,7708400," right okay so i'm gonna use the different terminal because emacs is kind of slow as a terminal uh right"
7708400,7723120," so and let's do that maybe 10 000 times right 10 000 times uh this is not okay all right it's just went"
7723120,7731280," somewhere uh okay and now i want to enable verification if you know what i mean so essentially just you"
7731280,7734800," iterate through all of like truth table build the truth table with that specific thing"
7734800,7745280," right so just build the truth table uh and does it look like zor yeah so this thing is supposed to be"
7745280,7754480," zero it's below uh 0.5 this thing is above 0.5 that means it's one above below so if we just like train"
7754480,7759520," it a little bit more it will be closer to zero this one has been closed to one and it kind of converges"
7759520,7767360," right so it goes somewhere so we basically migrated this model to mattresses right and it still works"
7767360,7775520," right but now it is more kind of like flexible we uh can add more neurons for instance right so but we"
7775520,7782800," to add new more neurons right we'll have to change these things right so in the main problem here is"
7782800,7788640," keeping track of all the sizes and stuff like that and it's kind of painful to do that if you know what i mean"
7788640,7796560," it's just like kind of painful so it would be nice right to actually go one step further in a level of"
7796560,7804640," abstraction right and introduce the notion of the neural network as a collection of matrices right so"
7804640,7809920," right now zor is basically a fixed collection of matrices but what if we want to build any kind of"
7809920,7815280," sort of like neural networks with any collection of matrices uh let's go ahead and try to abstract all that"
7815280,7821920," away so we can say that this is a neural network like abstract neural network uh right and here we have"
7821920,7829680," array of weights matrices and array of bias matrices right so we need to kind of know how many uh of these"
7829680,7834720," things we have like how many layers we have so let's have some something like count right so this is going"
7834720,7844240," to be the count and then uh here we have weights biases right and we also need to have inputs right we also"
7844240,7852640," need to have inputs but in case of inputs uh we have to have one additional thing in here for like the initial"
7852640,7861600," the input uh so maybe this specific array uh is going to be uh one element bigger than these two"
7861600,7867360," or something like that right so because i don't really know how to organize it better we can say"
7867360,7877120," uh the uh amount of activations is count plus one right so some sort of a note right to remember so this is a"
7877120,7882400," count but this is specifically this thing specifically is count plus one because like you need an additional"
7882400,7889120," thing in here uh right and there you go so this is our neural network right this is our neural network"
7889120,7897680," uh and we want to be able to allocate neural networks as easily as we do with a matrices right so we would"
7897680,7906800," have we want to have something like an and a lock and here we want to specify uh specify the you know"
7906800,7911520," the architecture of this entire thing right the architecture of this entire thing how can we do all"
7911520,7916560," that that's a very interesting question so first of all we need to supply count like how many of these"
7916560,7924000," things we have right we need to supply count then we need to know the sizes of this entire stuff right"
7924000,7932080," so we know that the input is um two like right there's two input neurons then there's two hidden"
7932080,7941200," and one uh output it would be kind of nice if i could just allocate new neural network by doing stuff like"
7941200,7951280," this right i could just say something okay uh right so this is architecture of the neural network go and"
7951280,7959680," configure and allocate matrices uh according to this specific architecture uh right so that would be"
7959680,7970400," kind of cool but we are talking about uh c we're talking about c we could try to do some variadic"
7970400,7978880," shit right so we could try to do some variadic shit but uh unfortunately it's very error porn i mean prone um"
7978880,7984960," and i don't really want to do that so what i'm thinking is that maybe we could accept count and"
7984960,7993360," array of layers right something like that but this thing right this thing also returns you the input so"
7993360,8001360," you actually have two layers in here but one input layer right so this one is complicated so if there's"
8001360,8008480," the three things in here uh there's actually two weights and biases so maybe we could accept okay so"
8008480,8014800," this is how many inputs we provide and then the count of how many layers and then the layers themselves"
8014800,8022480," or something like that right maybe uh i still haven't decided on how exactly i want i want to do that but"
8022480,8035440," yeah so there's inputs and the inputs will define the like basically an n s zero right and n s zero so we"
8035440,8042160," can do something like this and uh here that means how we're going to do that so it accepts two parameters"
8042160,8048880," then we could have additional thing in here which is basically layers and layers are two and output one and"
8048880,8057760," then you can have something like count uh side well you have to do some on this size of layers divided by"
8057760,8067120," size of layers uh zero right so two input parameters counts and layers right so with c it's kind of"
8067120,8075680," difficult to to specify all of that but it would be kind of nice if we could just"
8077360,8080240," do it like this right"
8080240,8087360," but i mean it has to be like this because it's it's the count in here"
8087360,8098000," so the count is not equal to the count and n because it's plus one so maybe i can say something like"
8098000,8103840," yeah it's not layers but let's say this is a description right so this is the picture or maybe arch"
8103840,8110640," architecture right uh architecture and then architecture count right so that's all we have"
8110640,8118720," in here and it contains like everything that you need right everything that you need uh right so that"
8118720,8123360," looks like a reasonable thing to have right that looks like a reasonable listen to have on top of that"
8123360,8130800," so i know that quite often i've seen uh some c code define this kind of macro uh right"
8130800,8140240," all right define this kind of macro where it takes sizes uh which is basically access like so so then"
8140240,8147440," you can quite easily just do like this which makes this entire stuff even simpler right so if you want to"
8147440,8152800," allocate it like that and here you specify the uh the architecture we can even say arch"
8152800,8160160," okay so i think i came up with a good api right how do how do you guys like this api right you have"
8160160,8167120," an architecture of the neural network so there's two inputs uh two hidden and one output and you sort of"
8167120,8171920," pass this architecture like that uh with this macro it makes it a little bit easier and it just like"
8171920,8180560," allocates enough matrices and configures the sizes and stuff like that uh right so that's actually kind"
8180560,8187360," of cool yeah so and it makes it super easy okay well i see that this neural network performs poorly"
8187360,8194000," right so i i can add another like layer in here and it will just automatically do that uh like reallocate"
8194000,8199120," reconfigure and it will just allow passing through this entire network very easily and then no it's"
8199120,8207040," just it's overfitting it's too many parameters let's actually remove that um yeah that's cool yeah let's"
8207040,8213600," actually try to to implement that so this is a very good api i think okay so and uh let's transform this"
8213600,8223040," entire stuff into that so this is going to be and then and then right so the count here right so the count"
8223040,8229840," is actually the amount of inner layers uh right but the arch count includes the input layers so because"
8229840,8236000," of that arch count uh right minus one equal count right so that's that's how it works essentially"
8236000,8247280," uh okay uh so what that means what that means we need to pre-allocate arrays of matrices in here right"
8247280,8252960," so we need to pre-allocate arrays of matrices so we need to allocate ws which is going to be a size"
8252960,8266800," of um ws um m and n ws right i'm allocating array of matrices uh malloc malloc uh multiplied by an"
8266800,8274320," n count right so that's what we do so this is an n count uh we might as well assert saying that this"
8274320,8282320," thing should not be equal to no right next thing is going to be bs right i'm allocating bs in here and that"
8282320,8289120," thing should also not be equal to no and the last thing i'm allocating in here is actually as but count"
8289120,8296960," here is plus one right or we could have said arch count minus one but i don't know so it depends on"
8296960,8305440," what kind of intentions i i have in here uh okay so pre-allocated arrays of those things and then assert"
8305440,8312400," oh yeah so we actually have an insert but that means i have to move this entire stuff to uh to nnh i want"
8312400,8316800," to do that after i finished implementing all that but i mean it it probably makes sense to actually start"
8316800,8326480," using the uh the frameworks things uh as soon as possible yeah thank you thank you so much let's do"
8326480,8332640," assert and there we go so we pre-allocated all these things and we need to start configuring this entire"
8332640,8337680," stuff so let's actually start with the first input layer right so the first input layer is very important"
8337680,8349920," uh right it is very important and we need to do mat a log all right mat a log and the input is just a"
8349920,8357200," vector right it's just a vector with a single row right with a single row one but the amount of um"
8357200,8367040," the amount of columns in there is actually arch zero which also implies that arch count"
8367040,8377760," assert can never be equal to zero right it should be at least one otherwise it kind of doesn't make any sense"
8379920,8385120," kind of doesn't make any sense so this is the first allocation that we have so and then we need to"
8385120,8391600," start allocating the rest of the layers sort of speak right the rest of the layers so let's do the"
8391600,8397520," iteration and then count right plus plus i so in here we're going to be allocating and then ws"
8397520,8404720," and then ws is going to be i uh and what's going to be the size of the entire thing"
8405840,8415840," so we have the previous layer uh and its amount of uh columns should be the amount of rows"
8415840,8427760," its amount of columns should be the amount of rows so that means and then s i columns is the rows for this"
8427760,8435760," thing but in terms of columns it is the architecture of this thing in here so"
8435760,8446800," that means we have to do arch arch arch but already plus one because we already used up the zero"
8446800,8452240," which actually brings us to a very interesting question could we just do something like one"
8452240,8461360," and arch count okay so and that means this thing is has to be like minus one"
8463360,8469840," uh this one is minus one because we kind of take the previous one uh and arch is this is that a good"
8469840,8481760," idea generally i don't know we'll see we'll see so uh there's that then uh we need to allocate the biases"
8481760,8490160," right we need to allocate the biases uh all right so this is a lock and the biases need to have essentially"
8491200,8500720," the same uh architecture uh architecture the same shape as the multiplication of activation the previous"
8500720,8504400," activation and the weights right so essentially"
8504400,8509600," yeah it needs to have the same shape as this thing"
8509600,8518240," so which means that it's probably going to have one row but how many how many columns i think it's going to be"
8518240,8524560," like literally the same amount of columns as here okay so and what we do is the next uh"
8524560,8533280," the next activation in here right the next activation in here which again is going to be one"
8537040,8543520," oh since there have to be okay i see so they literally are the same as the"
8543520,8548720," as the activation in here so i think that should work"
8548720,8556160," i think so a bit complicated but i guess that's fine"
8558480,8566400," i guess that's fine uh all right all right all right all right all right i would like to test"
8566400,8574000," that thing already somehow uh but unfortunately if i try to compile this entire stuff it's going to crash"
8574000,8581760," because i got rid of the sore a lock right so i got rid of this sore a lock i should have not done that"
8581760,8590480," right so uh because all right it doesn't allow me to migrate this entire thing easily so i have an idea"
8590480,8598320," i'm gonna move this entire stuff inside of the framework right inside of the framework uh so i need"
8598320,8606320," to find an end dot h uh and then dot h and let me find it in here so this is going to be that this is very abstract"
8607840,8610240," macro so we're going to put it in here"
8610240,8616880," so because i want to have something that is basically abstract up top then"
8616880,8621600," matrixly related and then neural network related right i think that would make sense"
8621600,8628880," so i'm going to take this thing and this is allocation and then the implementation of this entire stuff"
8628880,8634160," is going to be down there all right"
8635920,8642640," all right so and now i want to do control z right so because um i want to restore the original code"
8642640,8651360," because i want it to be compilable uh right because i want to gradually migrate towards a new architecture"
8651360,8657040," and by just jumping and removing everything that already works it's not gonna go really well if you"
8657040,8667840," know what i mean right so let's go back uh huh so yeah yeah there we go so that looks all right so now"
8667840,8674240," if i try to build this into i think it seems to be working so here it returns uh m it should return and then"
8675600,8686240," okay okay okay cool nice so it should be uh should be working now so what i want to do i want to be"
8686240,8692560," able to also print neural networks similarly to how i print matrices in fact i want to have a similar api"
8692560,8698640," for neural networks as for matrices like i really literally want to be able to do uh and then"
8699440,8704560," print where i just accept neural network and i probably will need to have some sort of a name"
8704560,8711120," for a neural network so it makes sense to put it in here so that then later i could have just a single"
8711120,8718320," macro and then print uh right that just does everything for me uh like so right we want to be able to do"
8718320,8730160," this kind of stuff uh if you know what i mean uh us all right uh so let's go ahead and implement that"
8734240,8747360," this one's gonna be really interesting so print f um name a new line name new line and then let's close"
8747360,8751760," this thing so how are we going to be doing all that i suppose we just need to iterate through the weights"
8751760,8759040," and biases and that's it uh right so and then count not const count i said"
8762160,8770480," and we want to do matte yeah matte print this one is interesting though yeah i suppose we want to do"
8770480,8779280," matte print uh like a big one but since it's a name yeah i have an idea how we can do that"
8779280,8786320," because i want to have specific name like"
8791440,8800320," all right so let's have matt ws right so matt ws and then ws and then bs like so"
8800320,8812720," so then later i can do wsi and then bsi and the problem here is that this thing"
8812720,8820720," yeah it's not gonna work well because it's literally gonna give you this expression uh uh okay so let's"
8820720,8825040," just try that and you will see what i mean in here right so and because of that it doesn't really"
8825040,8830880," matter so we can just do and then uh like so right so let's just do it and then like so"
8830880,8839360," okay so this is name this has to be a name uh yeah it's going to include the i right right you're"
8839360,8845520," right it's going to include i and that's that's the problem right you get it right uh okay so let's do"
8846320,8851360," uh size it's going to include the network uh size to arc all right and we want to have two two one"
8851360,8858880," uh right and then uh let's allocate the neural network all right let's just allocate it"
8861760,8868640," so this is the arch array length of arch"
8868640,8873840," and then and then print"
8873840,8885680," and then okay so let's try to do that and let's try to run this entire thing and what do we get yeah so"
8885680,8892000," that's the problem so i can see two problems in here right i can see two problems uh first of all i"
8892000,8899760," i literally want to have zero one and so on and so forth so that creates some sort of little like"
8899760,8906720," challenge second of all i would like this thing to actually be indented a little bit right if you know"
8906720,8911600," what i mean like it would be nice it would just like indented because i can't see uh in this mist so"
8911600,8918400," i don't think it's particularly useful so what i'm thinking is that maybe uh we could extend"
8918400,8927360," the print a little bit right so first of all uh first of all we're going to use this print directly"
8927360,8932320," uh for the name i'm going to just put ws and bs in here"
8932960,8937520," and we probably want to add another thing that adds the padding right so some sort of a padding"
8937520,8943520," let's say it's going to be four uh four things in here uh right and we can say size t padding"
8943520,8947680," and by default for this macro padding is going to be zero we're not going to add any padding so it's"
8947680,8956480," going to be hidden away uh right so in this one size t padding uh and we just add this padding in front"
8956480,8964960," of all of these things right so it says ti zero padding so maybe there is a way to do that with"
8964960,8971280," like printf formatting things i think i think there it's it's actually better to do it with printf"
8971280,8978640," formatting things right we can do i think you do it like that and then you say okay padding and just"
8978640,8983760," nothing or i don't quite remember right so but i also remember that you have to do it like that"
8985040,8991360," all right and yeah it's gonna be s"
8991360,9008880," so we'll see uh okay so printf c uh pad left pad with spaces i remember there was a trick to do that"
9008880,9011440," i remember there was a trick but i don't quite remember"
9011440,9016800," uh left pad printf spaces blah blah blah"
9016800,9022080," uh-huh"
9022080,9030960," okay oh yeah i was actually kind of right so yeah so this is the star because i kind of vaguely remember"
9030960,9038400," right i kind of vaguely remember uh but then padding padding goes yeah so then this is so essentially it"
9038400,9045120," makes this string of a certain size of the padding right but then the string is empty so that's why"
9045120,9052800," it's going to just create this amount of like um what is it called uh this amount of spaces so to speak"
9052800,9056640," all right so uh let's put this stuff in here and"
9056640,9066720," so it's kind of weird it's the weird way of doing this kind of stuff but it is what it is and it isn't what it"
9066720,9072240," isn't is it not i think it is the missile knows where it is because it knows where it isn't"
9072240,9075040," by subtracting the deviation okay"
9075040,9080400," uh so let's try to recompile this"
9084400,9093920," i think so let's see what's going on here so i think i need to address that actually um so i think"
9093920,9102320," we only need to add this sort of padding at the beginning of the stuff right and for the rest of the"
9102320,9112400," stuff don't even care brother brother don't even care so we're going to build and then and and look at that"
9112400,9119120," beautiful neural network look at that beauty doesn't look cool i think it looks kind of cool and it"
9119120,9125200," actually precisely allocated like the things that we wanted right so because the uh this thing has to"
9125200,9130320," be two by two and then bias is one by two and then the weights here two by one and then the final thing"
9130320,9137120," is one so that's perfect uh right so that's pretty progress but this is not enough"
9137120,9142000," because what i want you to have in here is like the numbers of those things like the actual numbers"
9142000,9146480," the question is how can we achieve that we can probably achieve that by"
9146480,9154160," uh s printf probably right so we can allocate some sort of like an inner buffer uh right so 256"
9155120,9166480," and we can just do snprintf uh ws and we can put this thing like so uh i send printf accepts the"
9166480,9171920," buffer and size of the buffer right and it's supposed to return well it doesn't really return anything but"
9171920,9177600," effectively what you can do you can just then put this buffer in here and then you can do bs in here and"
9177600,9183600," put this buffer in here right so we just allocate a little bit of memory on the stack we render some"
9183600,9193600," names into that memory and uh it should basically create a pretty cool where you have ws0 bs0 ws1 bs1"
9193600,9205280," and all of that is with pure c how about that how about that so isn't that poogers i think it's pretty"
9205280,9211680," you freaking bs0 so uh so and if you want to have more of these things right so essentially"
9211680,9218720," let's say uh what if i want to have another layer another layer of two and i'm gonna just add it in"
9218720,9226320," here so is it going to work all right so yeah that created another one thing here what if i want this"
9226320,9234400," inner layer to be like 10 so it's going to create like a um matrix of 10 by 10 literally i'm not even"
9234400,9241600," joking look at that actually no not really 10 by 10 but uh it depends on if you if you're going to"
9241600,9249280," actually have two of 10 yeah this is when it's going to create yeah so this is basically representation of"
9249280,9254160," this neural network in terms of matrices and look how flexible this right so we can just like specify the"
9254160,9261840," architecture and there you go it's pretty cool right so but we don't really need that powerful over"
9261840,9270800," of a thing uh right so what if we wanted to do the classical mnist thingy right with the three blue and"
9270800,9276880," brown uh architecture three blue and brown right did architecture where mnist is 28 by 28 right so this is"
9276880,9286320," the input then he had 16 inner layers and another 16 inner layers and out there was from zero to nine so this is the"
9286320,9292720," uh the architecture of the neural network that the three blue and brown used for his uh neural network series right"
9292720,9300240," so how would it look like uh right it's fucking enormous actually if you think well i mean it's not really enormous but yeah"
9301200,9308240," so that's a lot of different parameters in here that's that's it imagine training something like"
9308240,9315120," this uh imagine training something like this but though it's kind of weird all right there's one oh yeah"
9315120,9321440," okay so that's fine so there's three three layers in here right so and all that is parameters so as far"
9321440,9329600," as i know it basically has 13 000 parameters right so listen to i think it's 13 000 parameters um right"
9330400,9336320," so we're not going to be doing that this is going to be two inputs two inner layer and one output and"
9336320,9341520," that's it and this is a very small unique thing so another thing we need to be able to do we need to be"
9341520,9346640," able to randomize this in desktop right i want to be able to do and then rand similarly to how we do"
9346640,9353040," that with the matrices right i want to be able to just say okay randomize this into image uh neural network"
9354480,9362800," uh randomize it so uh so we have already met rand as you can see right i'm going to copy paste this thing"
9362800,9367600," uh here and it's going to be and then neural network"
9367600,9376800," so and what we're going to do right so we're going to iterate through all the layers right so these are just"
9376800,9387760," the layers just the layers uh nn wsi randomize this matrix and randomize this matrix there you go"
9387760,9396160," we just randomized this entire we also have to provide low and high this is low this is high boom"
9396160,9400880," no almost so we have to provide this thing here so let's say zero and one"
9401840,9409520," okay so this is a random uh random uh neural network so we probably want to"
9409520,9423760," randomize the seed got seed okay cool so that's nice now we need to be able to forward the neural network"
9423760,9430240," right so here as you can see we had coded these two iterations let's actually go ahead and simply"
9431040,9437440," you know abstract uh like abstract this thing away right so we can say and then forward right and then"
9437440,9443600," forward we're going to accept the neural network right and we're going to start iterating through"
9443600,9453360," its uh its layers right so this is going to be size t zero and then count and this is what we're going"
9453360,9463920," to be doing here right so essentially uh essentially we take the um the first activation layer which is"
9463920,9473040," the input action right and we're multiplying this thing with the first weights matrix right it's actually"
9473040,9480320," has to be it has to be i and we're doing that into the next activation layer right we're doing that into the"
9480320,9488560," next activation layer like so yes into the next activation layer then to the next activation layer"
9488560,9496000," we're adding the current biases right and then we're activating well it has to be some we're activating"
9496000,9504240," that next layer and since the amount of activation matrices is n count plus one this is not going to"
9504240,9511920," overflow at the end you know what i mean right and that's it right so this replaces this entire thing"
9511920,9517840," so basically it's the same thing but in a loop right and it accommodates any amount of layers uh"
9517840,9522800," right it accommodates any amount of layers as you want so this is how we're going to be doing forwarding"
9522800,9528320," so as you can see we're rising the level of abstraction in here we keep rising the level of abstraction"
9528880,9535120," so it becomes more and more abstract which is freaking a poggers isn't it i think it's pretty"
9535120,9542720," freaking progress uh okay so we can try to forward some into this neural network but i'm not sure if it's"
9542720,9551360," going to give anything meaningful right if it's going to give anything meaningful so let's take uh matte print"
9551360,9558720," right so we already kind of prepared the input data right so let's actually take this entire stuff"
9558720,9564640," all right move all of that stuff in here uh and this is where we're going to allocate the neural network"
9564640,9570880," so this is the uh input data right so this is the input data this is the output and uh you know what i"
9570880,9577520," want to do i want to take the mat row ti zero right so let's take a look at it mat print"
9577520,9586400," mat print doesn't work oh yeah because this has to be and then"
9586400,9594400," and then just a second going through the compilation errors that's why you have to program in python you"
9594400,9598880," don't have to worry about compilation errors there we go so this is the first row look at that like it"
9598880,9607200," literally gives you the the expression this is why i used that uh thingy mat print which stringifies"
9607200,9611200," because it literally stringifies your expression and it gives you like information what exactly"
9611200,9617120," you're printing look at that this is so cool right it's a very useful feature and this is c this is pure"
9617120,9623920," c uh right anyway so let's actually find something right so if we take a different row yeah this is a"
9623920,9633360," different row right so let's copy that row right mat copy into the input layer of the neural network and"
9633360,9638880," the input layer is the first one right so this is the zero this is the input layer right uh would be"
9638880,9647200," kind of interesting to maybe have something like and then input and also and then output"
9649360,9677680," it's just like semantically nice you know what i mean right so essentially i take the first row from the"
9677680,9685120," the training data and copying it into the input of the neural network then i forward the neural network"
9685120,9690800," and then i'm just printing the output of that neural network"
9690800,9700480," yo what the this is so nice i mean that's a that's a useful framework like syntactically even"
9701120,9710400," like even syntactically i just look at that and this is the output right this is the output uh maybe we"
9710400,9717680," can we can just take a look at that right so this is basically row uh all right mat print row"
9717680,9728640," uh so we're gonna print the input forward it and this is the output right so we have the input in here and"
9728640,9735600," this is the output we can go through other things right if you if i do two all right so this is another"
9735600,9743680," one and three uh yeah it just makes it easier to debug and you know do some other stuff uh that's pretty"
9743680,9750080," cool what the is this like starting to turn into like an actual ml framework what the"
9750080,9763520," the fuck uh yo yo anyway so okay that's pretty cool and uh one of the things we probably want to do is"
9763520,9771520," essentially uh just maybe verify right so maybe turn this into a verification right so maybe turn this"
9771520,9780880," into verification but yeah whatever um so we need to have a finite difference for that specific neural"
9780880,9789120," network right uh we need to have a finite difference uh the way we implement finite difference here is like"
9789120,9796560," this right uh but we need to do that for the neural network okay so let me let me see how we can do that"
9797200,9812000," so the signature is this so this one and then finite uh diff so it will be kind of have nice to have like"
9812000,9817440," a shorter name because we have and then rand forward uh i don't know"
9824160,9834400," okay so let's let's use for finite uh yeah so let's go if anyway uh and then um and then g right so"
9834400,9842400," epsilon and the input output uh though yeah so i think that would be nice so this is more of a like in"
9842400,9850080," out all right so this is in out but this is not necessarily in out it's yeah it's specifically"
9850080,9858880," the training set right so that makes sense that's specifically the training set uh okay okay okay"
9858880,9864560," okay okay okay okay so uh let me find this thing find that"
9864560,9877040," so essentially we yeah we want to be able to compute the costs in here"
9878320,9885120," which means that we probably want to start with computing the costs right uh so find a diff let's"
9885120,9893680," introduce something and then cost which accepts this thing right the the neural network itself and"
9893680,9902880," uh you know the inputs and the outputs right inputs and the outputs right so this has to be the cost"
9904160,9916480," the cost uh so this is going to be an n g epsilon ti to so in the coast is essentially right so i can"
9916480,9922160," probably steal some of this stuff in here yeah there we go so it's quite important that ti and to"
9922160,9928560," o has the same amount of rows and columns right the same amount of rows and columns uh right then we start"
9928560,9934880," with initializing this entire thing and iterating through all of the samples right iterating through"
9934880,9943120," all of the samples uh then we're taking the rows of the samples right essentially uh this is the"
9943120,9949520," expected input and this is the expected output right expected input and expected output then we're"
9949520,9955360," copying this the input into the input of the neural network but this is kind of interesting so we can do it"
9955360,9965120," with our network uh we framework we can do it like that mat copy neural network input and then x there"
9965120,9970240," we go so this is the expected input we're copying it into the input of the neural network right so that's"
9970240,9979360," how we're going to do that after that we are forwarding the neural network so at an output we have the final output"
9980560,9989760," right so that's what we have here and uh yeah essentially uh we want to make sure"
9989760,10001280," uh so we have a bunch of actually a certain here oh yeah i see yeah so here this is the output layer"
10001280,10008320," right so we have to do an n output of the neural network has the same amount of columns as the columns for the"
10008320,10012960," output training samples right so that's basically what we have in here so that's quite important actually"
10012960,10023280," and i can copy it in here and we start iterating this stuff we start iterating this stuff and"
10023280,10034080," uh we are finding the difference so this is the and then output right yeah this is perfect right and then we"
10034080,10044240," we just return this cost divided by n so that allows us to just take a look at the current cost of our neural"
10044240,10050480," network right so what's the what's the current cost the usual thing that i do in here uh what's the current"
10050480,10056000," and cost and then cost and then cost neural network tito all right so what do we have"
10056000,10062640," so the cost is 28 makes sense okay that's cool"
10062640,10069920," uh next thing uh finite difference for the finite difference we have to do the usual thing of like"
10069920,10077520," you know just wiggling things around right so let's just do float saved uh float saved iterate through"
10078080,10085680," all of the layers right iterating through all of the layers within the layer and then w s i"
10085680,10092560," we're iterating through the rows right so this is size t j uh j less than rows plus plus j"
10092560,10098960," and then within this thing we're iterating through all the columns right so this thing becomes very abstract"
10100240,10109120," very abstract very quick uh so this is k actually this is i excuse me uh and this the single parameter"
10109120,10122240," is met at and then w s i j k right so this is like a um you know three nested loops right so we need to save"
10122240,10130640," this entire thing this entire thing then i'm wiggling this thing around right and then computing the cost"
10130640,10135600," uh of this neural network right the cost of this neural network"
10135600,10139840," and uh subtract the cost that i computed before so we have to do something like"
10139840,10149280," this oh it accepts the uh the training parameters right so mpti to uh so the cost here and then cost"
10149840,10158960," and then to i to right so there's that and then we divide by the size of that wiggle and um"
10158960,10166320," we essentially save it to the gradient we essentially save it to the gradient"
10166320,10173920," there we go and then we need to restore the saved value and to restore the same value this is saved so it's"
10173920,10179840," kind of similar to what we were doing uh in here when we were copy copy pasting but now we manage to"
10179840,10183920," compress this entire thing because we can iterate through all of the layers but we still need to copy"
10183920,10190560," paste this thing a little bit for the biases specifically right and this is by the way what"
10190560,10196480," would have been super useful if the all the parameters were in a continuous space so we could compress"
10196480,10202240," everything into a single continuous array and just wiggle on the elements of the array and it will just"
10202240,10208080," work automatically so it would be kind of cool to have a way to just iterate all the like known parameters"
10208720,10217840," so we'll think about that actually so ws yes oh rows nice one okay so this is ws and that's"
10217840,10227520," basically it i think yeah so we're just like wiggled things around and we uh populated the um populated this"
10227520,10239440," thing uh populated this thing so all right so looks okay uh let me try to compile this entire thing so"
10239440,10250720," let's go through the compilation errors and uh this is what we've got we we've got the gradient the thing is"
10251360,10262240," we still need a thing that applies that gradient somehow right uh okay so here is the thing itself"
10262240,10267440," right let's allocate the gradient luckily it's super easy we just call this thing twice"
10267440,10273280," so this is the second gradient in here uh then we compute the cost okay so we've seen the cost let's do"
10273280,10282560," and then find a difference and and then j epsilon do we have the epsilon anywhere defined yeah here it is"
10282560,10290320," so let's actually bring that thing in here it's going to be useful uh ti to so that gives us the gradient"
10290320,10298320," which we want to apply right so let's say learn and then j with the rates right the certain learning rate"
10299600,10307680," so and then find the difference and then learn and then has to be m in my opinion"
10307680,10313120," and then g and learning rate"
10313120,10319920," okay so here we kind of have to do the same thing"
10324560,10334000," i know uh and it's just like easier right so right multiplied by the gradient"
10334000,10342560," and that's it i wonder if we can like the only way we can compress that"
10342560,10350480," even further right is only if we were able to iterate the parameters of the neural network in a"
10350480,10355440," continuous space right because that already is like really well compressed in my opinion"
10355440,10361760," right it's already really well compressed so the it would be better to just have like one single"
10361760,10366480," foreign you know but i mean once we start doing back propagation once we start doing back"
10366480,10370720," propagation all that will go away because it's a very stupid way of computing gradient anyway"
10371360,10385520," uh right so it's very stupid okay so i hope i didn't make any oopsie doopsie uh and a potential a"
10385520,10394160," fucky blacky right okay so let me recompile this entire stuff it doesn't really compile uh"
10395760,10405200," because why oh it's already it's already taken yeah so yeah but maybe we can simply yeah we can"
10405200,10410480," probably get rid of all that already so i don't think uh any of that zora stuff is needed anymore"
10410480,10416960," all right because we're almost done we're almost done there we go okay if i try to run this entire"
10416960,10425280," thing let's see does that reduce the cost function it freaking does which means i can run that in the"
10426480,10435440," loop right i can run that in loop for uh size zero thousand of thousands of times right thousand of times"
10435440,10437440," and there we go"
10437440,10450320," uh and it kind of did it does it work but it's yeah sometimes it gets stuck all right so i think i need to"
10450320,10457120," do that in here right uh huh so thousand is just like not enough in my opinion right so"
10457120,10463040," at least for the for the way we do that right for the way we do that thousands not enough but it's usually"
10463040,10468800," revenue enough uh okay i hope i didn't make any stupid freaking mistakes man"
10468800,10478560," uh-huh so rate is as usual yeah there we go it goes down okay so we didn't make any stupid mistakes"
10480080,10488320," yo and it went went relatively well so let's try to verify this and i think the way we verify that"
10488320,10495840," uh right the way we verify that is by iterating two by two right so this is a two by two j"
10495840,10503680," and what we have to do in here is we need to take the input of the neural network right and it's a"
10503680,10510800," matrix uh so it's a row right so that means the row is going to be zero but uh the element the column"
10510800,10518320," zero is going to be i and the column y uh you know this one is going to be j right so this is the input"
10518320,10526800," and uh the output all right the output is going to be interesting so we're gonna uh we're gonna and then"
10526800,10532880," forward the stuff into the neural network right we're forwarding this and then we want to print the"
10532880,10538960," output so we're going to print the values that we put in there though interestingly yeah it will be"
10538960,10549520," convenient to actually just do it like that z u equal f so i j and how can we get the output we can do"
10550160,10558960," output of the neural network matrix at uh zero zero right because the output is the matrix so and it has"
10558960,10563840," only one number so that's how we can do that this is really nice right so this macros kind of create"
10563840,10569840," this sort of like a syntax uh domain specific language i would even say it's almost as easy as"
10569840,10576800," but yeah but though i never actually used any python frameworks do they work like this is that how you use"
10576800,10583760," them right because i don't really want to touch any of the python frameworks unless i can implement"
10583760,10588080," this from scratch myself unless i fully understand it i don't like to use things that i don't understand"
10588080,10594160," right so until i finished with this thing i'm not touching anything from like tensorflow or anything"
10594160,10598160," like that i want things i want to understand things first"
10600320,10608240," uh right but you don't do a lot of memory dialogue and but why do you need to deallocate any memory in"
10608240,10614800," here uh because it's going to be delegated by operating system anyway and you know what's funny"
10614800,10621840," the memory is allocated only in these two places so there is a reason why i said that i want the memory"
10621840,10627920," allocation to be concentrated in a very small place so you allocate as much memory as you need for up front"
10628640,10635920," and then none of that process down below allocates any memory like none of the memory management involved"
10635920,10638720," in this specific region of code none of it"
10638720,10640800," um"
10640800,10646080," right literally none of it and that's the beauty of it so i can"
10646080,10649040," increase the amount of iteration it's not going to leak anything"
10649680,10655760," right uh so okay that's kind of cool"
10655760,10663760," and it works right look at that so it does act like zor that's cool so"
10663760,10671680," now let me remove all of the zor related stuff which is not needed because we abstracted all of"
10671680,10677840," that away none of that matters anymore it was just like intermediate thing that we don't need so the"
10677840,10683440," whole thing is here's the training data right we're slicing the training data into these two"
10683440,10688240," like input and output matrices right so then we define the architecture of the neural network"
10688240,10694800," two inputs two inner things one output right and we just allocate all of that here and then we just"
10694800,10700240," like training that with a very dumb approach of finite differences and whatnot but it still drives the"
10700240,10703360," cost function down which is what we want anyway so who cares"
10703360,10709120," uh right so sometimes it doesn't really work the way i want right it gets stuck in a really weird"
10709120,10710720," place but anyway"
10710720,10712720," uh"
10712720,10713840," right"
10713840,10718640," so and why i want you to do all of that right because now i can quite easily change the"
10718640,10723760," architecture of the entire neural network so for instance if i want to go back to a single neuron"
10724480,10728800," which cannot represent zor i can just remove this entire thing and there you go"
10728800,10735120," so i don't have to rewrite the entire code or anything that i just like remove the entire layer"
10735120,10743440," right so if you were to use this kind of approach where you would explicitly"
10743440,10748640," put all of the arguments like adding or removing layer would be a huge pain in the ass now here i can"
10748640,10754400," just remove one thing and that's it so and it's just like it's a different neural network uh differently"
10754400,10759920," connected and stuff like that right so the whole neural network network is described like with this"
10759920,10765840," single line and it can modify all that and it can for example confirm that uh it can not really yeah"
10765840,10773920," so i'm trying to uh you know do zor and it gets stuck on 0.25 it's in all of the all of the"
10773920,10779760," activations here are the same because zor is not representable uh like with this amount of neurons"
10779760,10787680," like you just can't do that so though you can do something cool right so okay here's let's say this is"
10787680,10796480," td zor let's introduce another one here right so let's say uh this is going to be td or right how about"
10796480,10809440," that's td or uh right and essentially essentially uh this is going to be zero zero zero right so one one"
10809440,10819360," one okay uh i wonder if i can okay so this one is a bit painful i think"
10821920,10834160," damn damn damn so we know that the stride is three we can okay so n is the amount of samples right is"
10834160,10839840," the amount of rows maybe you can just hardcore that because i know that it's equal to four anyway so who"
10839840,10847360," cares right it's always four four rows uh and it's sort of going to be like a standard here so that then i"
10847360,10853600," can do float td uh right so and then just swap between these things like so quite easily right"
10853600,10859920," so let's take a look at zor it doesn't work and now i'm going to switch to or and or with a single thing"
10859920,10868960," is representable so it goes towards or right but it can't do zor right it can't do zor but as soon as you"
10869680,10876640," add add another layer in here right we just edit another layer in here it works"
10876640,10886640," uh do we have a biases segment yes we do so let me let me demonstrate"
10886640,10890640," so and then forward"
10894320,10900400," here's the bias and here's the segment so this is the forwarding it's pretty straightforward"
10900400,10911760," and now we can uh we don't have a relu yet but we're going to add it a little bit later"
10911760,10917840," so what if we actually say okay what if we're going to have four of this four of the inner like"
10917840,10923120," layers it's going to change anything probably not uh yeah it doesn't really change anything but it"
10923120,10929760," still still still works right it was just like it's bigger uh we can also at the end of training we can"
10929760,10935520," maybe print uh the entire neural network that we managed to you know train"
10935520,10946640," yeah so that's the entire neural network uh yeah so but this is with four hidden things"
10946640,10951360," right with working things which is completely redundant i don't think it's needed but that's the"
10952000,10958320," that's the things in here does adding more neurons increase learning time yeah i'm pretty sure because"
10958320,10962800," that will uh basically means that you need to spend more time computing the cost function"
10962800,10968720," because we're using finite differences and if we were not using finite differences we were using"
10968720,10975200," backpropagation it's still more parameters to iterate through anyway it's just like purely based on"
10975200,10984240," the amount of iterations you have to make so you you you want to like i guess find the size of the neural"
10984240,10994560," network that kind of fits your purpose uh so yes yes yes so now we can do more interesting cool"
10994560,11001920," shit for instance one of the things i wanted to do is to have a training data right that looks like i don't know"
11001920,11014080," some right and essentially i could say okay so here i have uh like two bits right so this is an input and"
11014080,11022560," then two bits as an output and then if i put one in here i would expect this thing to be this if i put one"
11022560,11031280," to here i expect this thing to be this right and if i do it like that i expect this thing to be this"
11031280,11039280," do you guys know what it is right you guys know what it is it's a sum right so it would be kind of cool"
11039280,11049280," to train a neural network that acts like a sum circuit right of like a fixed amount of beats right uh"
11049280,11060320," it would be kind of interesting i think and if we take a if we google up the uh you know uh circuit"
11061040,11070880," adder i have a feeling i have a strange feeling that circuit adder may require the depth of the neural"
11070880,11080560," network equal to the amount of beats it's working with so for each bit it may require an additional layer"
11080560,11092400," of neural network who knows why who knows why like a full error i have a feeling it's my hypothesis i don't"
11092400,11099040," know for sure i don't know for sure because you chain them together"
11099040,11110000," because we chain them together like the output it goes into the input another one and so on and so forth"
11110000,11116000," and i feel like each individual bit may require like literally additional layer but i don't know that"
11116000,11124560," yet and i want to explore that because i already kind of off screen right tried to uh train it and"
11124560,11132480," with just few layers or just with a one hidden layer it was not really training well"
11132480,11135120," so my hypothesis is that you need a lot of layers here"
11135120,11141120," that's why humans are bad at math too many neurons need yeah exactly"
11141120,11145760," uh but i want to do that already next time because i'm already streaming for three hours"
11146800,11152480," right so we're going to leave that for the next time we're going to leave that for the next time"
11152480,11158640," so what we managed to do in here we managed to uh basically take everything we learned in the previous"
11158640,11166000," stream and turn this into a separate framework which i would like to maybe put onto github but i'm going"
11166000,11170880," to do that after the stream right because uh i'm already a little bit tired so i'm going to just finish the"
11170880,11177280," stream and you know um you know take a break and then i'm going to put it on on github"
11177280,11184320," so let me take a look did we have any additional subs i don't think so nobody subs or subscribed at least"
11184320,11191280," which doesn't show me uh right so it's i'm just refreshing it's super quick uh yeah all right"
11191280,11196560," uh that's it for today thanks everyone who's watching me right now i really appreciate that"
11196560,11205200," hope it was interesting uh right and i see you all next time so yeah love you"
