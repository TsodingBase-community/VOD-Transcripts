start,end,text
640,6000," Hello everyone and welcome to yet another recreational programming session."
6000,11520," How about that? So today we're going to continue working on NN.h which is a deep"
11520,15600," learning framework in C that we've been developing for quite some time. You can"
15600,20400," find the source code of this thing here in the description. And today I"
20400,26680," wanted to focus on something rather interesting. So in the third episode of"
26680,30560," machine learning in C which you can find in the description. So it's called the"
30560,34900," most important machine learning algorithm. It was brought to my attention that the"
34900,39340," way I do back propagation is not particularly traditional. It was brought"
39340,44920," to my attention by Francis Kim Johnson. I hope I ran your name correctly. Thank"
44920,51940," you so much for pointing that out. So essentially if we take a look at a very"
51940,57820," simple model which is basically a sequence of neurons. So we have M neurons"
57820,61960," and each of them is connected with a certain connection with a certain weight"
61960,67360," and each of them have a certain bias. And for the sake of simplicity let's"
67360,70300," actually say that we're not going to have too much data. Let's say we're going to"
70300,76360," have like one single sample of data so we don't have to do the sum and then take an"
76360,80500," average of it. So there will be no sigma sign and divided by n. So it's going to be"
80500,82240," super simple. Right."
82240,89200," So if you use my approach where I defined a sequence of cost functions for each"
89200,94420," individual layer you're going to end up with something like this. Right. But if you"
94420,100180," consider a cost function as a single entity and you literally just tank all of the"
100180,105460," math and compute each individual partial derivative manually, you will end up with"
105460,109360," something like this. And this is what was pointed out by this specific pull request."
109360,119140," Right. So this is pretty much the same thing except, right, we do not multiply by two on each"
119140,124600," individual layer. Right. So here we only multiply by two the first time for the"
124600,129820," derivative of the activation of the last layer. And then we continue just like"
129820,136220," multiplying by the corresponding derivatives of the current layer. Right. So there's no"
136220,141940," like constant multiplying by two. And my approach actually results in something very"
141940,147660," interesting. And it results in the parameters of the first layer being"
147660,154500," modified too quickly. Because as you progress towards the beginning of the neural network,"
154500,161660," you keep multiplying by two. So the parameters start to basically change faster and faster and"
161660,169200," faster. Right. So you can even see that effect in the simulations that we actually implemented"
169200,176400," specifically in image.nn. Right. So where we train the neural network to act like an image,"
176400,181700," actually like two images. So then we can interpolate between them. Right. So let me actually"
181700,185860," maybe rebuild this entire thing just in case, so we have a fresh build and stuff like that."
185860,200860," There we go. I'm gonna sip my tea while everything is building. And let's actually start the image to nn."
200860,205360," And let's provide two images that we want to interpolate. Right. So I have a little bit of a"
205360,212520," nist database in here. So we're gonna interpolate from eight to words six. I'm not sure if you can see"
212520,217520," everything. I think I forgot to put something behind my camera. Yet again, I really apologize for that. I should"
217520,226520," probably put that into my to-do list or checklist of things I do before the stream. So anyway. Right. So here is the"
226520,232680," sort of the simulation, the gym in which we're training neural networks. Right. So in here we're training neural network to act"
232680,238680," like both of these images. And as soon as it learns how to act like both of the images, we're able to interpolate"
238680,246680," between those images. So it's a very cool animation generator, I would say. Right. So you cannot see the effects of"
246680,253000," our backpropagation approach in here unless the neural network is deep enough. Let's say let's actually make"
253000,260680," it a little bit deeper than whatever we have in here. So image to nn.c and where is the architecture? So here is"
260680,266680," the architecture. So let's actually make it a little bit deeper. Something like that. I think that's a"
266680,274520," that's a good architecture for the neural network. And let's maybe rebuild everything like so. I'm gonna"
274520,285480," rebuild and run simultaneously. All right. Looks deep enough. Looks deep enough. Okay. So let's"
285480,290920," actually let it train for a little bit. And you'll notice something interesting. Right. So we'll have"
290920,296200," to wait a little bit until it just like kicks in. If it doesn't kick in for too long, maybe it will"
296200,303880," probably reduce the depth of the neural network. But I think it should kick in any minute now. Any minute now."
304520,311720," It's having a hard time kicking in. So maybe we should reduce the amount of the layers. Let's remove one"
311720,317880," layer. Okay. Sure. The neural network is too deep for this entire thing to learn. And we're using sigma"
317880,324200," activation function. So maybe we have a vanishing gradient stuff. You know, a thing may happen."
324200,327480," So this is machine learning. In machine learning, you never know what can go wrong."
327480,332680," So to fix things, you just need to. Yeah, there we go. You can clearly see. Look at that. Look at what's"
332680,339400," going on. Like only the layers towards the beginning are being modified like crazy. But as you go towards"
339400,345400," the end, nothing really changes at all. And like you can clearly see because we color this entire stuff."
345400,352680," Right. And as you can see, these things are just flickering and going crazy. And this is the result of"
352680,359400," our approach of that extra two that we add on each iteration on each layer. Right. So if we restart,"
359400,366040," you will see that again. So just to confirm that it's not a flick or flock. How do you say that?"
367000,376600," I don't know. So, and yeah. If we try to use a traditional approach now. Right. If we try to use a"
376600,384520," traditional approach now, it's going to be a completely different picture. Right. So let's go to nn.h."
385400,393560," Right. So the header specifically. And then backprop. All right. And so basically what we have to"
393560,399960," change in here. So according to this thing. So the first thing, the first partial derivative has to"
399960,407240," be multiplied by two. So this thing should be multiplied by two. Like so. And the things that"
407240,411560," were multiplied by two should be multiplied by one now. So this is a very simple change."
412680,418680," Right. So I was very close, I think. So I hope I didn't actually make more mistakes in here. And I"
418680,424120," interpreted the pull request correctly. So I think this is what we have to do in here. If I misinterpret"
424120,429160," the pull request, please let me know in the comments. All right. So I'm going to restart this and I think."
430520,443080," And we'll see how it goes. Right. So it is doing something, but now it is not really learning"
443080,451560," anything. Right. And it can go like that for quite some time. It can go like that for quite some time."
452760,459640," But this doesn't mean that this approach is wrong. Right. So if we reduce the depths of this entire"
459640,465480," neural network. Right. So if we go back to what we had before, this entire approach is going to work,"
465480,471000," actually. Right. So let's actually remove it like this. And let's restart it."
471000,485640," So, so this is the traditional approach. There we go. So, and as you can see, it is learning. Right."
485640,491960," But you may probably notice that it's learning a little bit slower. It's kind of difficult to compare. I think we"
491960,497000," need to have some sort of a mechanism that will allow us to flip flop between traditional and"
497000,501080," non-traditional approaches. So let's actually do a little bit of modification of the source code."
501080,508840," All right. So I want to introduce some sort of a macro and let's call it nn_back_prop_traditional."
508840,513960," Right. So let's call it traditional. And basically, if this macro is defined, we're going to use"
513960,517480," traditional approach. And if it's not defined, we're going to use non-traditional approach. Right."
518040,526440," So, nn_back_prop. Let me find the function. So here is nn_back_prop. Right. And if we are"
526440,532600," nn_back_prop_traditional, right, we're going to be doing this thing. Otherwise,"
532600,539880," otherwise, we're going to be doing the thing without multiplying by two. Right. So essentially, it's going"
539880,547240," to be just like this. And in here, we're going to have end_if. There we go. So, and in here, we just"
547240,554920," have to switch between this sort of like a factor. One of the things I'm thinking is that I can introduce"
554920,562520," a variable called s. Right. And I can define it some way here. Right. So essentially, if we are"
562520,570120," in a traditional mode, it's going to be just one. Otherwise, it's going to be two. Right. It's going to"
570120,579640," be essentially two. And if like so. Right. So right now, we are in traditional approach. Hopefully."
586760,592760," Okay. So notice the length of this thing before it actually starts to go down and learn anything."
592760,600200," Right. So it's quite a long thing. If now we're switching to non-traditional approach that we had"
600200,606280," before. Right. It's going to actually learn faster. Right. It's going to learn faster."
606280,616600," You can clearly see that. It learns faster, but at the same time, it's crazy. Look how crazy it becomes."
617240,622280," When it's like find something in here. Right. By this point, I actually want to reduce the rate"
622280,629480," to sort of calm it down. And this craziness kind of helps this entire approach, non-traditional approach,"
629480,634600," to penetrate like a deeper neural networks. Right. So if we have deeper neural networks,"
634600,642040," this sort of added craziness. Right. It allows it to penetrate all of these layers and learn at least"
642040,647720," something. I don't think it's not particularly good way of training the neural network because it doesn't"
647720,653400," like utilize like further parameters in here. Right. It only focuses on these parameters,"
653400,659880," but at least it just starts doing something. Right. Whereas with such depth of neural network,"
659880,666360," if you switch to a traditional approach, it's incapable of penetrating that at all. Right. So even though"
666360,676120," our non-traditional approach is actually crazier. Right. It's way crazier, but it's suitable for like"
676120,683640," training like a deeper neural networks, especially the ones that use sigmoids. Right. So I don't know,"
683640,692040," like I wouldn't rule out this approach completely. Right. So this kind of situation left me confused a"
692040,699320," little bit. And as far as I know, the creator of this pull request actually, yeah, so they actually share"
699320,704680," this sentiment. Basically they said that I'm hesitant to make the corresponding changes to the actual source"
704680,709240," code because when I run the test, the conventional approach actually seemed to worsen the performance."
709240,715880," Yeah. For this specific setup, for this specific neural network with sigmoids and stuff like that,"
715880,723240," this, you know, additional multiply by two on each layer kind of helps to like drive the cost function"
723240,729080," down a little bit faster. And this is the thing about machine learning. Right. So the method of training"
729080,735160," the neural network does not have to be mathematically correct, whatever that means. Right. So it has to be"
735160,741960," something that just drives the cost function down. Right. Whatever you can come up with that drives the"
741960,749800," cost function down, it works. Right. So that, and that's the main goal anyway. Right. So what's"
749800,756680," interesting is that this only works with that specific activation function. Right. And sigmoid activation"
756680,764680," function is very like, you know, how to say that safe in the sense that like whatever crazy values you throw"
764680,770760," at it, it's, they still going to end up in the range of between zero and one. Right. It can, it cannot go"
770760,777320," super crazy. It's kind of limits itself, if that makes any sense. So our non-traditional approach with"
777320,784520," constant multiplication by two may not work with other activation functions that are not as limiting as a"
784520,791960," sigmoid. And this is something that I would like to explore on today's session. Right. So let's just have"
791960,798600," this, you know, switch between traditional and non-traditional approach and see how it performs on different"
798600,804200," activation functions. Right. And this is an opportunity to explore some other activation function, as you can"
804200,810520," see. So we just, it cannot just penetrate this entire thing. Even though this is like the traditional correct sort of"
810520,816920," approach, it still cannot do that. Anyway, so let's actually introduce a couple of more activation"
816920,824200," functions and see how they react to our training process. Right. So I think it's going to be interesting."
824200,832920," So as I already mentioned, we use sigmoid activation function. So we have a function sig, right, which applies"
832920,843400," sigmoid to the matrix. And the other very popular activation function is called relu. Right. So let's actually google it up, relu."
843400,854280," So it's a rectifier, a neural network. So what's cool about this function is that it's super simple, actually."
855000,864120," So essentially, so it accepts input x. And if x is greater than zero, it just returns x without any"
864120,870600," modifications. And if it's less or equal than zero, it returns zero. Right. So it has a shape of"
870600,877000," sort of these things, as you can see, just straight up, like if it's below zero, it's just like a straight line in"
877000,884200," here. If it's above zero, it's just like the same line in here. So, and the derivative of this function"
884200,890520," is also pretty straightforward. If x is greater than zero, the derivative is one. If it's less than zero,"
890520,896200," it's equal to zero. It's not defined at zero, but usually people just straight up ignore that, and they pick"
896200,904360," like arbitrary value. They pick basically value either zero or one at x equals zero. Right. So just pick one,"
904360,910440," and it kind of works usually. So let's go ahead and implement that. So we have a sigmoid. Maybe what"
910440,918520," I want to introduce, I want to introduce relu.f. Right. So usually in c, f denotes that we're working"
918520,929400," with floats as opposite to doubles. Right. So let's actually do float x, sigmoid f. Right. And essentially,"
929400,938760," if x is less or equal than zero, we want to return zero. Otherwise, we want to return x. So this is"
938760,943720," how we can implement that. There is another interesting way of implementing this entire thing. So essentially,"
943720,952360," if x is greater than zero, if you take this boolean value and reinterpret it as float, it's going to be"
952360,958920," basically one. So essentially, you can just say, OK, multiplied by x. Right. So in that case, if this"
958920,964760," thing is true, this entire expression is going to be x. Right. This entire expression is going to be x."
964760,973400," And if it's false, it's going to be zero. Actually, I'm not 10% sure. So is the..."
973400,978040," Well, I mean, yeah. So essentially, this thing is going to be integer one. It's probably going to be converted"
978040,983400," to float and it's going to be one. So this entire thing is going to work out correctly. But I'm actually"
983400,989720," going to double check that. Right. So let's super quickly double check this entire thing. So I'm going"
989720,1001000," to just iterate maybe float starting from minus one up until one. And we're going to just plus zero one."
1001000,1007960," Right. I think that's a reasonable thing to do. And let's just do f to f. And it's going to be x,"
1008680,1014440," relative x. And let's just try it around and see what's going to happen."
1014440,1026040," All right. So what do we have? This one is interesting. So it's minus zero. Okay. I mean,"
1026040,1033320," this is still zero, but it's minus zero. But if it's a ball, it's the same thing. Okay. It seems to be"
1033320,1041720," working. It seems to be working. All right. So what we need to do now in mat_sig, we have to actually"
1041720,1047720," apply a reluv. But that kind of means that we probably want to change the name of this function"
1047720,1053160," because it's not going to be about sigmoid anymore. In fact, I think it would be better to call this thing"
1053160,1059640," something like mat_act. So instead of like applying specifically sigmoid, we're going to be activating that"
1059640,1066600," function. Right. I think that makes a little bit more sense. I hope you agree with me. So, and in here,"
1066600,1074600," let's just apply reluv. But that's not enough. Right. Because in the backprop, right, in the backprop,"
1074600,1082120," we are also computing the derivative of the activation function. Right. So, and the derivative"
1082120,1089720," of the sigmoid is a sigmoid multiplied by 1 minus that sigmoid. Right. So, this value is the derivative"
1089720,1096040," of sigmoid. Right. So, and I would like to maybe factor it out to a separate variable like this. That"
1096040,1101640," was actually kind of weird, but anyway. So, something like this. So, it's going to be a separate variable q."
1102760,1114120," And essentially, since we're using relu now, what I can do. Right. So, basically, if a is greater or equal"
1114120,1120760," than 0, we're going to be returning 1. Otherwise, it's going to be 0. To be fair, we can just like"
1120760,1127720," do something like this. And of course, here it's not a, it's q. Right. So, that's basically the derivative"
1127720,1131960," what we can, that we can pick in here. So, and let's give it a try. Let's see if it's going to work"
1131960,1141400," or not. So, now we're not using sigmoid. Hopefully, we're using the relu. And the problem with the relu"
1141400,1147480," is that I never could actually make it work. So, as you can see, it sort of, excuse me, jams this entire"
1147480,1154200," thing and the cost function becomes none. Right. So, since relu is sort of like very much unbounded on the"
1154200,1160280," plus side, you don't really want to use this like huge of a rate of learning. So, you probably want"
1160280,1167400," to actually reduce it like very, very low. Something like this. And even with such a rate, it still kind"
1167400,1172760," of like goes over like, oh, did you see that? That was actually, yeah. So, this is actually pretty cool."
1172760,1180760," Yeah, I really like that. So, and yeah, it doesn't really, it doesn't really do anything. So,"
1180760,1186520," let's actually make it super, super small. Oh, it's doing something. It is doing something. But"
1186520,1192200," it's going crazy. And I wonder what kind of approach do we do we use? I think we're using a traditional"
1192200,1201320," approach. Right. Yeah, it's well, at least it found something resembling eight. Right. It found something"
1201320,1207880," resembling eight. And here's interesting thing. Look how blocky everything is. Look how blocky everything is."
1207880,1215720," So, I'm going to actually make a screenshot. And if you look at it, it's not particularly as smooth as the"
1215720,1222920," as the sigmoid. If you take a look at the sigmoid, it was actually very like ghosty and smooth and stuff"
1222920,1228120," like that. I wonder if I can just switch quickly between these things. I think we need to introduce"
1228120,1232280," a similar mechanism that allows us to switch between activation functions, if you know what I mean."
1232280,1243000," Right. So, it would be nice to have something like, I don't know, let me see. So, maybe we could define"
1243000,1249160," something and then act. And here we could specify like what kind of activation function do we want to"
1249160,1255800," have like maybe we're going to introduce something like enumeration. So, this is going to be activation,"
1255800,1262840," and we can have activation function which is a sigmoid or the relu. Right. So, and in here I could specify"
1262840,1271720," something like act sigmoid. Right. And everything related to the activation is going to use a sigmoid and stuff"
1271720,1282120," like that. So, met act. Right. And in here we probably want to do something like switch and then act. Right."
1282120,1290280," So, to be fair, it would probably be better if it was like some sort of a runtime value. Right. So, act act."
1290280,1300120," So, let's go to the signature in here. So, act act act. Let's go back. And here we're gonna do switch upon act."
1301000,1309320," So, case act sig. Right. So, and in here this is how we are activating this entire thing."
1309320,1318520," So, this is the break. And then here we have act relu. Right. So, this is very well. And this is break."
1318520,1323320," And if we encountered something that we didn't see before, we're gonna, I don't know, throw some sort"
1323320,1329560," of like an exception. We don't have exceptions in C, but you know what I mean. Like unreachable. There we go."
1329560,1335000," So, this is basically what we can have in here. Hopefully, the compiler is smart enough that it"
1335000,1340840," will basically put these, you know, loops inside out. Right. It's not going to do a switch on each"
1340840,1346280," individual iteration. I hope at least in 2023 the compilers are smart enough to not do that."
1349480,1356280," But you never know. But who cares? I mean, in 2023 you have powerful computers. Okay. So, this is what we"
1356280,1364360," have. And we also need to modify the derivative in the backprop. Right. So, here where do we compute"
1364360,1371000," derivatives? So, here is the derivative. And the backprop. Do we want to pass this entire thing to backprop?"
1371640,1381080," Probably not. So, let's actually do switch case. Switch and then act. All right. So, this is going"
1381080,1387800," to be case. Act. Of course, basically, this is actually a very important disclaimer. This is not how"
1387800,1395240," it's going to end up being. Right. So, obviously, I want to have a much nicer like interface for like"
1395240,1400360," choosing activation functions and stuff like that. The only reason I do this like sort of hackish thing"
1400360,1405640," is because I want to quickly introduce new activation function without refactoring too much."
1405640,1411480," It's just like doing that properly and making it so pretty and stuff like that just requires too much"
1411480,1416840," refactoring that I don't want to do on a recreational programming session because I'm exploring things."
1416840,1422120," Right. So, what I'm doing, I'm just exploring and just want to see like how different activation"
1422120,1428520," functions affect different things. Right. So, that's why it is what it is and it isn't what it isn't."
1428520,1434360," So, maybe I want to define this thing in here. So, then later I can do something like this."
1434360,1440360," All right. So, this is going to be like that. And this is ReLU."
1440360,1449880," So, let's run with that. So, this is going to break and by default in here, we're going to say something"
1449880,1458120," like and then assert and then we're going to say unreachable. Right. So, this is unreachable. Cool."
1458120,1463400," Okay. Maybe because of that, since I already sort of like switch on a compile time value,"
1463400,1470200," maybe I'm not going to accept additional thing in here. Right. Because I already do that anyway."
1470840,1480520," So, who cares. And then act. And let's not pass this entire thing in here. All right. So, hopefully"
1480520,1486840," that will make it easier to then add more activation functions. So, we can play with them and see how they"
1486840,1494120," go. Right. I think that's a very useful thing. So, yeah. All right. So, this is a sigmoid. Right. So, this is a sigmoid."
1494120,1500920," Right. So, this is a sigmoid. And what kind of training approach we're using? I think we're using traditional"
1500920,1506760," one, which is not going to work. So, we want to use a non-traditional one. So, let's rebuild this one more"
1506760,1512360," time. Because traditional one is incapable of penetrating this many sigmoid layers."
1516360,1523320," All right. Let's use a non-traditional one. Okay. Hopefully, it will start penetrating very soon."
1523320,1531160," There we go. It penetrated stuff. So, it's going too crazy, in my opinion. So, let's reduce its craziness."
1531160,1539240," And if we save the screenshot. So, right now, it is saving screenshot. Right. So, if we take a look at the screenshot."
1540280,1547400," It is actually very smooth. Look how smooth it is. It's very smooth. Let's actually do sigmoid. Right."
1547400,1551560," So, let's actually rename this entire file. So, because we want to save it. And let's go back to"
1551560,1556440," to a different activation function. Let's go back to ReLU."
1559960,1574200," Let's go back to ReLU. And for ReLU, we actually want to make this thing very, very small. And even..."
1574200,1580440," Yeah, there we go. Right. If we take a look at the results of this activation function, they're not that"
1580440,1587000," smooth. They're very much blocky. Right. So, let's actually save this thing. So, it's still saving the"
1588920,1594760," screenshot. Yeah, there we go. So, as you can see, it looks like a polygon. It's still gradientish,"
1594760,1603320," but you can see these clear lines. You can see these clear lines. As with sigmoid. Right. So, where is the..."
1603320,1608520," Yeah. There's no such clear, like, sort of edges. That's what I was looking for. Not lines, but edges."
1608520,1616840," Right. So, we can even compare them. Maybe. Is it possible? Probably not. Thank you, Max. Right. So, you can see"
1616840,1623320," these clear edges and it looks like it's constructed out of polygons, whereas this thing is actually super"
1623320,1629640," smooth and feels constructed out of these curves, the BZI curves or something like that. And this is"
1629640,1637160," basically different activation functions. And why activation functions actually have such a profound"
1637160,1643240," effect on the final result of the neural networks? This is because the neural networks actually use"
1643240,1649960," activation functions as building blocks of the function they're trying to interpolate."
1649960,1656520," Right. So, essentially, like, each neuron is basically one piece that the neural network"
1656520,1661960," can use to stitch together the actual thing that you're trying to train it to behave like."
1661960,1666600," And in case of a sigmoid, sigmoid is a very smooth thing, it's going to consist of, like, these smooth"
1666600,1672360," curves. And in case of a ReLU, which is just like this, like, angle, right, like, angle thingy,"
1672360,1677160," it's going to try to construct the stitch together, like, different lines and try to construct out of"
1677160,1682920," that. And that's why this thing is super blocky. Right. So, there's a very cool video that I found on the"
1682920,1688280," internet about that. So, the video is called ""Why Neural Networks Can Learn Almost Anything"", I think,"
1688280,1693960," by Emerging Garden. Right. So, I'm going to leave the link in the description in here. And they explain"
1693960,1700920," how the neural network actually approximates those, like, different functions that you're trying to"
1700920,1707640," train them to behave as. Right. And depending on the activation function, they're going to basically have,"
1707640,1715800," like, different shapes and stuff like that. Right. So, it's actually very cool. Anyway. Right. So,"
1715800,1721880," unfortunately, ReLU doesn't really work as well as I would like it to work. Right. It's just doing something,"
1721880,1731400," but having a hard time to learning anything. So, what people usually suggest, and what people suggested"
1731400,1739640," to me on Twitch, is just like, try to use leaky ReLU. And leaky ReLU basically doesn't, like, never makes"
1739640,1747400," the negative part of the function super flat, like in classical ReLU. What it essentially does,"
1747400,1758280," let's actually go here, and then that page. MatAct. Where is MatAct? So, maybe I probably want to go to here,"
1758280,1766200," to this ReLU F. Where is the implementation? Right. So, essentially, what it does, right,"
1766200,1776440," it multiplies x by this small parameter. Right. So, it's not going to be super flat. It's going to be,"
1776440,1784920," like, very steep. Is that a good word to use in here? But yeah. So, essentially, I think even on Wikipedia,"
1785640,1794200," there is a description of a leaky function, I hope there is some sort of, like, a plotting of this"
1794200,1807720," thing. Probably not. We can actually draw that. All right. So, let me just draw the axis. So, here are the axis."
1809480,1817320," x and y. And so, essentially, the classical ReLU is going to look like this. Right. So, it's super flat."
1817320,1823960," But the leaky ReLU is going to look like this. And the negative part is going to be sort of like this."
1823960,1831960," Right. So, essentially, the value of the function still affects, the input parameter of the function still"
1831960,1838840," affects the value of the function on the negative side, but not as much on the positive side. Right."
1838840,1845960," And that kind of, that is more flexible, I suppose, and it actually helps the neural networks learn a bit"
1845960,1854680," faster and easier. Right. So, let's actually try to use a leaky ReLU and see if leaky one will actually"
1854680,1862760," learn anything. Right. So, we modified the value in here. We can actually take this parameter and move it"
1862760,1873400," sort of like to a separate macro and then ReLU program. Right. Put it somewhere here. Define and then"
1873400,1880200," ReLU param. So, in here. And essentially, if you want a classical ReLU, just set it to zero. Right. So, it's"
1880200,1886760," actually super easy. So, it's more general that way. Right. And regarding the derivative and the back"
1886760,1895720," prop, let's see what we have to do in here. So, where is the derivative? Okay. So, in case of that,"
1895720,1903480," this is going to be one. Otherwise, it's going to be an n ReLU param. Okay. So, that one will allow"
1903480,1913960," to learn a little bit more precisely, hopefully. A little bit more precisely. Okay. There we go."
1913960,1922040," So, and of course, we still need to make it learn a little bit slower. And all right. And it's actually"
1922040,1929720," learning. Look at that. So, and it actually looks like, yeah, it looks like the shape of these things."
1930840,1935720," Right. It actually does look like that. What's interesting is that, what approach do we use?"
1935720,1942040," What approach do we use? I think we're using a traditional one. Look at that. With a traditional"
1942040,1947560," approach, we're actually able to penetrate, like, all of these layers super quickly if we just use"
1947560,1954440," the ReLU activation function. Specifically, LeakyReLU. Because non-leaky one, like, never works"
1954440,1959000," for me for some reason. Right. Maybe because, like, the neurons dies out super quickly. But this one"
1959000,1963960," actually kind of does something. But you can see that the background is kind of weird. Right. So,"
1963960,1970200," and this is a bug of the rendering. Right. So, let's actually take a look. So, there's a bug of the"
1970200,1978760," rendering. Where do we render the previews? Preview. Somewhere here. So, here are the three previews."
1978760,1986680," So, essentially, what we do, we take the output activation of the neural network and multiply it by 255,"
1986680,1993800," and assign it to 8 pixels. This would work if the activation function is a segmoid that keeps"
1993800,2001480," the activation between 0 and 1. So, that means it will never be less than 0 and greater than 235."
2001480,2009080," In case of a ReLU or LeakyReLU, this thing can be way over 1. So, it overflows. And because of that,"
2009080,2015480," the background is actually kind of weird. Right. I think background literally underflows. Right. So,"
2015480,2022680," and that's why it's super wide. What we probably want to do, we want to put this entire thing into"
2022680,2029160," some sort of like activation, like in here. Right. And we want to clamp it. Right. So, if it's less than 0,"
2029160,2034840," it's going to be 0. If it's greater than 1, it has to be 1. Right. And only then,"
2035480,2041080," we want to multiply it like this. Right. So, that's basically what we want to do here. Right. That's"
2041080,2046200," basically what we're going to do. And let's actually go through all of the all of the previous"
2046760,2055320," and just replace things like this. There we go. And let's recompile this entire thing. And that should"
2055320,2067320," fix the rendering, hopefully. That should fix the rendering. All right. Oh, it's yeah. Sure. Just a"
2067320,2074840," second. I need to make it small by default. Is it learning? Okay. So, it is learning. Look at that."
2075480,2082120," So, it is doing things. All right. And we can try to interpolate between these images. And"
2082120,2090360," this actually scrolls between them. Since it is so linear, instead of actually like morphing different"
2090360,2096360," curves and stuff like that, it just scrolls between them. Like why not? You can just scroll between them."
2096360,2102520," We can even try to maybe render this entire thing. Right. And we'll just actually see how it looks like."
2102520,2108920," Yeah. It looks super blocky. Like it can consist of these like ReLU blocks and stuff like that. And it"
2108920,2115320," looks nothing like the segmoid one. The segmoid one is actually super smooth. Right. But no matter"
2115320,2121160," what kind of activation function you choose, you're still interpolating some other function. It's just"
2121160,2127240," like you're interpolating it with different chunks. Right. Maybe with curved chunks or maybe with like"
2127240,2132040," straight chunks or whatever. Right. So, it's still interpolation, but just with different pieces."
2132040,2137880," And it managed to do anything. And does it still scroll? It does, in fact, still scroll. It's actually"
2137880,2148280," pretty cool. That's super cool. And that is with the deep... Well, it's relatively deep. As far"
2148280,2154200," as I know, people make even deeper neural network. And this is with deep neural network and a traditional"
2154200,2159080," approach. So, traditional approach with a sigmoid couldn't penetrate this neural network. But with the"
2159080,2166280," ReLU, it does penetrate this entire neural network. But what if we use a non-traditional one? Right. So,"
2166280,2173400," we use a non-traditional one. So, let me actually see. Right. ReLU, but non-traditional one. The one"
2173400,2183160," that we came up with on the third episode of this entire thing. So, is it going to also penetrate or"
2183160,2190600," maybe even learn faster? We'll see. We'll see. Okay. So, of course, it's too fast. Let's reduce the training."
2194200,2203720," Okay. It's still able to do that. But it's way more crazy. Yeah. It's too crazy. Even though we"
2203720,2213800," set the learning rate smaller, it is still a little bit too crazy. Right. So, essentially, I guess this"
2213800,2219560," non-traditional approach is very useful when you have deep neural networks with a sigmoid activation"
2219560,2224920," function because they allow you to like penetrate like deeper and deeper and just like drive this entire"
2224920,2231080," thing faster. But as soon as they reach some local minimum, they get too crazy and too difficult to"
2231080,2238600," control. So, I'm not sure if you want to use that non-traditional approach standalone. Maybe with a"
2238600,2244840," combination of other approaches, it might be useful. Like just like as initial learning breakthrough, just"
2244840,2252200," like quickly drive somewhere and then switch to a traditional one to like refine this entire thing. So,"
2252200,2258600," that could have been actually rather useful. Right. But as you can see, it's too crazy. Right. It's too crazy."
2259400,2266760," And I suppose this non-traditional approach with this additional like two coefficients"
2266760,2274920," on each individual layer. It acts more like elevated learning rate, probably. But I'm not a specialist"
2274920,2280200," in machine learning and stuff like that. So, I don't really know. You know what's interesting? It's actually"
2280200,2287560," learned. This is a relio. Right. So, it managed to interpolate relio quite well. Right. It's relio."
2287560,2293320," Right. So, let me double check. It's not the sigmoid. Yeah, it is relio. I wonder how it's going"
2293320,2297560," to look like if we try to render a video out of that, because we recently implemented like a video."
2297560,2301960," How it's going to look like? I'm really actually curious. So, I'm going to start the video."
2303240,2312520," And I think... Oh, it's going to actually do that very slowly. Yeah. So, this is because we are doing"
2312520,2320120," full HD rendering. We're doing full HD. Yeah. This is full HD. So, one of the things I want to do probably"
2320120,2327880," is do something smaller. Right. We can do 412 and 512. And let's restart this entire thing."
2331320,2335240," It's actually pretty cool. All right. All right. All right. All right. All right."
2335240,2346040," Okay. So, I'm going to make a very small learning rate. And let's try to learn something."
2346040,2351720," So, is it... It is learning. It is learning. Well, it's just going... All right. All right. All right."
2351720,2360760," It's doing things. Okay. Let's just let it cook. Let's just let it cook. So, I really want to see the"
2360760,2367240," moment when... Yeah. So, here it scrolls. Okay. Let's actually try to make it interplay. But, you know,"
2367240,2371640," in terms of like scrolling. Because I think scrolling is actually kind of cool. I want to"
2371640,2378200," see how it looks like with like 60 FPS and like in higher resolution. So, here's the rendering. So,"
2378200,2385160," we're feeding the frames into FFmPack. All right. And FFmPack should create a video for us. So, the progress is"
2385160,2391640," actually this thing. As soon as A becomes 1, it means 100%. So, we managed to render"
2391640,2396120," 100% of the video. We're just doing animation and stuff like that."
2396120,2407960," Okay. Yeah. So, maybe I'm going to cut at this point. So, I don't know what to say right now. So,"
2407960,2414520," okay. So, it's finished rendering. And let's take a look at the final video. I'm going to use MPV."
2415880,2424440," And let's see. Okay. So, this is how it looks like. So, it's basically trying to scroll between"
2424440,2431400," two scuffed numbers. Right. So, and this is because of a different activation function. Right. That's"
2431400,2443480," actually super cool. All right. So, yeah. What if we try to use like a more shallow neural network,"
2443480,2449880," if you know what I mean. The original one, but with ReLU activation function. Right. So, let's actually see."
2449880,2454760," Right. So, let's use traditional approach. Right. Because it's more controllable"
2454760,2458280," in case of using ReLU. And let's go ahead and do that."
2461400,2474680," All right. So, let's reduce this thing. Let's make it more small."
2474680,2483160," All right. So, it is learning something. Okay. It's very blocky though. Is it going to make another"
2483160,2489800," breakthrough at some point? I really hope so. I'm hoping for a breakthrough. Maybe there will be... Okay."
2489800,2493000," So, it's going down. Going down. Is it going to make something?"
2493000,2504920," I feel like it doesn't have enough elements to construct the things out of. Right."
2504920,2514200," Maybe we should give it to more neurons in the layers. All right. What about just making it 28?"
2515400,2518360," Right. So, 28? 28?"
2518360,2534520," All right. So, that's very interesting picture in here."
2535720,2540920," That looks rather cool, I think. All right."
2540920,2544440," Yo, what the fuck? It's actually going crazy."
2544440,2552600," Okay. So, it's too small, I think. All right. This is going to learn to behave like the numbers."
2552600,2557080," Hopefully. Because I want to see something."
2559560,2560600," This actually looks cool."
2560600,2568520," All right. It's learning. Like, on average, at least it's going down. Right. It might be..."
2568520,2573000," It might feel like it's stuck, but on average, it's slowly going down. That means at some point,"
2573000,2578520," it may perform some sort of like a jump. Like a breakthrough. Hopefully. I don't know."
2579240,2585480," I'm just coping, I suppose. All right. But it kind of stopped on average. Right. Okay. Let's give it another try."
2585480,2589960," All right. So, as you can see, sometimes it just does these breakthroughs."
2589960,2596920," Looks like something out of horror. Right. A little bit."
2598680,2606120," Okay. So, 6 kind of resembles the 6. Hopefully. It will... Okay. So, it's going somewhere. It's going somewhere."
2606120,2610520," We just need to, like, let it cook for a little bit."
2610520,2614520," It's a pretty cool transformation."
2614520,2625960," It's a pretty cool transformation."
2625960,2645960," So, yeah, you get the point. So, there is... It's not the only activation functions that we actually have."
2645960,2652840," So, one of the other very popular activation functions, as far as I know, is called 10H."
2653560,2658680," And it's another sort of, like, smooth activation functions. It's a hyperbolic function. Okay."
2658680,2666600," We can try to add support for this function as well. Right. So, let's take a look at the definition of"
2666600,2672120," this entire thing. And it's rather straightforward. Right. So, we can pick maybe this definition. Right."
2672120,2680040," So, this definition looks pretty easy to implement. Right. And what kind of shape does it have? So,"
2680040,2687240," 10H is... Oh, yeah. Okay. So, it's the one which is blue and dotted. Right. Which is blue and dotted."
2687240,2692200," And it kind of looks like a sigmoid. Right. It kind of resembles the sigmoid, but it's slightly different"
2692200,2699240," shape. Right. It's another... It's actually minus... from minus one to one. In case of a sigmoid is from"
2699240,2704360," zero to one, and this one is from minus one to one. So, that means it will be able to output negative"
2705240,2712440," values if we ever need this thing to do that. Right. So, we can give it a try. And what's the..."
2712440,2720040," I think derivative, if I remember correctly, is also predestined for it. Right. So, 10H derivative. Right."
2720040,2727560," So, what's the derivative? Okay. I think... I'm pretty sure Google should be able to just tell us..."
2728280,2734600," what's the derivative. Oh, it's basically one minus square of 10H. So, that means we only need the value"
2734600,2739880," of this function. We don't really need X for this function, which makes it super easy to integrate"
2739880,2744360," into our current system without too much refactoring. So, I'm actually super happy about that."
2744360,2751160," So, let's go ahead and try to introduce the 10H and see how it performs. Right. So, this is a 10H."
2752680,2764440," All right. So, let's find and then act. All right. So, case act 10H. So, this is when we're activating"
2764440,2772360," the matrix. So, maybe we'll have to introduce a function for that. 10HF. I'm not sure if C already"
2772360,2777000," have this function. And if C does already have the function, I suppose it's gonna, you know, complain about"
2777000,2792040," that. Right. Reluf 10HF. And let's find the implementation of Reluf. So, what's the value of this function?"
2792040,2800440," So, we essentially have these two values, like e to the power of x and e to the power of minus x, which probably"
2800440,2806600," means that we may want to have like separate variables in here. So, this is x and then"
2806600,2815480," yanks. Right. So, it's basically negative. Right. So, this is a negative one. And essentially what we do,"
2815480,2821960," we take the difference between x and yanks. Right. And divide it by the sum of these things. Right. So,"
2821960,2829000," x plus yanks. And that's basically what we want to have in here. Right. So, this is the value of the"
2829000,2835960," function. So, what about the derivative? What about the derivative? What about the derivative?"
2835960,2850120," Okay. 10H. So, if I remember correctly, it was one. Actually, one minus a multiplied by a. Right. So,"
2850120,2857960," yeah, it's this one. So, that's basically the derivative. Okay. So, let's give it a try. So, now we are using"
2857960,2866440," ReLU. Let's make it use 10H. Right. So, I don't know if 10H is even used. Right. So, what I hear this day,"
2866440,2873800," people use either ReLU or Sigmoid. The other thing people use this day is also GELU. Right. So,"
2873800,2881080," it's like another hot thing in machine learning, which is like a ReLU, but smooth. Right. So, the thing about"
2881080,2889720," ReLU is actually it's very angular. Right. So, but GELU is like ReLU, but smoother. It doesn't have this"
2889720,2897080," like very sharp edge or anything like that. So, we may add it at some point. Right. So,"
2897080,2901560," let me add it at some point. I forgot the break. So, this is the reason why I put asserts in here."
2901560,2906920," This is literally the reason. Right. Just in case I forget the break or something like that. Let me double"
2906920,2911880," check if I didn't forget the break on the other side in here. No, I didn't. Okay. So, that's cool."
2913480,2920040," That is very, very cool. That is very, very cool."
2920040,2930520," All right. So, I suppose the final result here is going to be... Okay. So, you also probably need a very"
2930520,2937160," small learning rate for that specific... Okay. Right. Very small learning rate. So, it is learning,"
2937160,2941960," but maybe a little bit faster. Maybe we should let it learn a little bit faster. Is it going to"
2943080,2947880," do something? Well, it is learning. It is actually learning, which is kind of cool."
2947880,2954120," So, though, it's kind of different, especially in the background. Right. It is still smooth,"
2954120,2959480," but at the same time... Yo, this is actually kind of cool. What the fuck."
2966520,2972600," I want to see this shit in higher resolution. Okay. So, let me actually render the upscale version."
2972600,2979960," Right. So, let's take a look. Let's take a look at upscale. Yeah. It kind of looks like a sigmoid,"
2979960,2984760," but the transition is actually different. It's not a sigmoidish transition. It's actually..."
2986120,2992680," This is actually way cooler. I really like that. Look at that. So, let's let it cook for a little bit more."
2992680,2998520," Right. So, I want to drive the cost function a little bit more down, because six is kind of meh,"
2998520,3004680," in my opinion. Right. So, let's take a look at six. It's kind of close. Right."
3006920,3011960," But maybe if we let it, you know, cook for a little bit, it's going to be even better."
3011960,3018920," So, it's actually kind of cool how trainable it is. Right. It's kind of cool how trainable it is."
3018920,3025400," And the approach we're using is a traditional one. Yeah. It is a traditional. Right. And with a"
3025400,3030520," traditional approach and the hyperbolic function, it works really well. Right. So, maybe we also want to"
3030520,3035960," take a look at non-traditional one. Let me actually quickly render the video. Right. Because I want to see how"
3035960,3040520," the transition just looks like, in case of a video. And of course, it's probably going to take some time."
3040520,3047480," So, let's quickly cut it. All right. So, it finished rendering. And let's take a look at the transition."
3047480,3053880," So, the low resolution transition should look like this. I really like how there is sort of like the"
3053880,3059080," whoosh effect throughout the entire sort of screen. I really want to see that in a higher resolution."
3060280,3064200," All right. So, I'm going to use MPV. And let's loop it as well."
3064200,3073960," That's actually super cool. And I suppose the whoosh effect throughout the entire screen comes"
3073960,3080280," from this like long things that goes towards infinity from left to right. But, I mean,"
3080280,3085960," sigmoid is kind of similar to that as well. Right. I don't know. Right. It's kind of cool to see like different"
3086760,3092520," results depending on the activation function. Right. So, and yeah. You can literally see"
3092520,3098840," the effects of activation function and why it is important. Right. And so, maybe depending on how"
3098840,3104360," exactly you want to interpolate the final result, you may just make different decisions on the activation"
3104360,3109640," function. Right. Maybe you don't want to have the sharp edges. And because of that, you may not want to"
3109640,3116280," use ReLU. Right. For whatever reason. Right. But the cool thing about ReLU is that it's super simple."
3116280,3122440," Right. It's like one comparison. It's literally one comparison. And if it's parametric ReLU, it's actually"
3122440,3130440," also one multiplication. In case of sigmoid, it's just like it's par to the epsilon. Right. So,"
3130440,3136360," there's also then division and the same goes for the hyperbolic function and stuff like that. So,"
3136360,3142360," it's like they're more costly. Right. The cool thing about ReLU is that it's super cheap computationally."
3142360,3147320," Right. It's extremely cheap computationally because it's just comparison and multiplication. And that's it."
3147320,3153800," One comparison, one multiplication. Right. So, for faster learning, I think it's a little bit better."
3153800,3160600," Right. So, but we were using a traditional approach to running the hyperbolic function. Right. So, let me"
3160600,3167240," see. What about the non-traditional one? Right. Is it going to go crazy? It's probably going to be crazy."
3167240,3176200," Right. So, because you have like just faster learning for the, for the left, for, for the layers that are"
3176200,3181960," closer to, to the input. Let's, let's call them like that. Right. So, and this one is just like"
3182680,3189880," kind of jammed. So, let's actually put the learning rate somewhere here. And it's not that crazy. Right."
3189880,3195400," I wouldn't say it's that crazy. So, we're using the non-traditional approach. Yeah. We're using a"
3195400,3200440," traditional approach. And this is because the neural network is not super deep. Right. So, it's not"
3200440,3208760," super deep. Let's actually make it a little bit deeper and see, right, if it's going to go super crazy."
3208760,3215160," Right. Okay. Deep neural network. Deep, relatively deep. And non-traditional approach. So, this thing"
3215160,3221960," should go, start going crazy. Right. According to my understanding of how all of that works. Right."
3221960,3229560," So, let's actually make it super small. It doesn't really go that crazy. It actually goes fine. And"
3229560,3235080," learns super fast. For real. Okay. So, this is a non-traditional one."
3235080,3241000," This, this is such a culture. Look at this trends. What the fuck."
3241000,3246280," Look at this transition. Holy shit. I've never seen such transition before."
3246280,3254600," Yo. I want to render that. Okay. I'm going to quickly render that. Right. So, I just want to see that at"
3254600,3263000," higher resolution. All right. So, it's done. And let's take a look at this beautiful, beautiful transition."
3263000,3267000," This is the most awesome transition I've ever actually came up with."
3267000,3274040," This is so cool. It's just like this, this arm detaches and attaches again. So, the aid is kind"
3274040,3281640," of scoped. But this is so cool. Yeah. I'm sure. Yeah. It's probably due to the different activation"
3281640,3289720," function. But this is so cool. Okay. Right. I need to save that. So, I'm going to call it cool."
3289720,3297560," So, all right. So, what do we have here? We have a deep neural network and we have a non-traditional"
3297560,3303640," activation function. Right. And it learns relatively fast, I would say. It learns relatively fast. Right."
3303640,3312040," Probably due to, like, sort of increased learning rate. Right. So, what about a traditional one with"
3312040,3318920," hyperbolic function and a deep neural network? Right. So, let's actually see. So, the traditional"
3318920,3323880," should be a little bit slower. Does it mean that the non-traditional approach actually kind of helps"
3323880,3332600," for this kind of neural network to learn faster? Actually, yeah, let's say it's a bit slower."
3332600,3342680," It's learning a bit slower. It's more like, you know, calm, I would say, but it's a bit slower."
3342680,3353000," So, okay. So, rough conclusion is that the non-traditional approach with, like, faster learning"
3353000,3359800," rate kind of helps on these, like, very smooth functions that squash the activation between, like,"
3359800,3365720," zero and one and minus one and one. Right. So, especially on, like, deeper neural networks because"
3365720,3373240," they actually help to penetrate this entire stuff faster. Right. But on ReLU, such non-traditional"
3373240,3379880," approach is just too crazy. Right. Because ReLU is unbounded and squashed. So, it just goes too"
3379880,3386520," crazy and you probably want to use traditional approach there. Probably. So, non-traditional one,"
3386520,3392120," look how quickly and beautifully it learned, like, very precisely as well. Right. But again,"
3392120,3396760," it utilized only the things that are closer to the input. It's kind of, these things are kind of"
3396760,3401480," underutilized. But maybe that's what you would expect for this kind of, like, problem. I don't know."
3401480,3406600," Somebody needs to actually research this kind of thing. I think. Maybe. I don't really know if it's worth"
3406600,3413880," researching right. But I kind of find this thing rather interesting. Right. So, it's, like, less"
3413880,3420360," precise. It's a little bit crazy. But it just finds something very, very fast. Especially if you have a"
3420360,3425320," lot of, like, layers and stuff like that. And as far as I know, like, I remember hearing that there is,"
3425320,3432760," like, a problem with vanishing gradient on deeper neural networks with sigmoid. Does that help with that?"
3433400,3438440," Maybe. I don't know. I'm not a machine learning person. So, I need somebody. I need an adult to"
3438440,3443720," actually educate me on all of that. Right. So, I don't really know what the fuck I'm doing. Right."
3443720,3448360," That's for sure. I'm just playing with different things with different activation functions and stuff"
3448360,3455000," like that. And sharing my discoveries. But I'm not a scientist or mathematician or anything like that."
3456040,3462920," The happy-looking one is actually kind of cool. This transition is cool. Yo. The tanh comes up with"
3462920,3468760," way cooler transitions than sigmoid, in my opinion. Look at that. Or maybe I'm imagining that. I don't know."
3468760,3476040," I don't know. Right. So, this one is actually kind of cool. So, there is another interesting"
3476040,3483560," activation function that I heard almost nobody uses. Like, I heard almost nobody uses that activation"
3483560,3491800," function. And that is a sign activation function. Right. So, and the problem with sign is just it's"
3491800,3498120," kind of funky. Right. So, it doesn't really necessarily squash. Right. If the value gets too big,"
3498120,3505400," it may drop from one to minus one. Right. And it keeps like oscillating and stuff like that. But people"
3505400,3510760," say that you can like train very interesting neural networks with a sign activation function. So,"
3510760,3516120," let's actually add a sign activation function and see if it does any good. If it does any good. So,"
3516120,3523640," let's switch back to traditional approach. And, right. So, let's introduce sign. Because why not? Right."
3523640,3533640," We're just experiments. Right. We're just farting around. Just farting around. So, let's go to matact."
3533640,3541080," Right. Matact. And introduce the sign activation function. Right. So, in here, we're going to be just using"
3541080,3549160," sinf. Right. So, but there are some problems with derivative of such function. Right. I think there are some"
3549160,3558440," problems with derivative of such function. Because, yeah, we only know the value of the function. Right."
3558440,3566280," So, we set up all of the things so we know only the value. And if you can't compute the derivative"
3566280,3573560," by the value of the function, we can't really integrate this entire thing. So, we can still add such"
3573560,3578440," feature in here. But it will require a lot of refactoring, which is kind of outside of the scope"
3578440,3584200," of today's session. So, I don't really want to do that. Right. So, what's the derivative of sine? I"
3584200,3592440," think it's straight up cosine. Derivative of sine. It is cosine. Right. So, and to compute the cosine of x,"
3592440,3599800," I need to know x. But I don't know x. I know only sine x. Maybe I can use arcsine or something like that."
3599800,3609320," Right. So, but to be fair, one of the things we can do, actually, we can just say ""arcsine"" and something like"
3609320,3619560," unsupported. Right. And then, when we're using this activation function,"
3619560,3627800," with, like, when we're using this activation function, we can essentially just use finite differences. Right."
3627800,3631800," So, instead of biopropagation, we can use finite differences. And finite differences are really"
3631800,3636840," interesting because they don't require a derivative. They're actually approximating the derivative by"
3636840,3642760," wiggling around the parameters of the model. Right. So, we can try to do that. But it's going to be super"
3642760,3648920," slow on such big neural networks that we're already working with. But we can try to do that on smaller"
3648920,3653560," neural networks. So, the reason why I wanted to look into the sine activation functions, and this was,"
3653560,3660680," by the way, suggested in the comments of, I think, a second video or maybe first video in the series. I"
3660680,3667880," really forgot who suggested that. But essentially, with a sine activation function, you can solve the ZOR"
3667880,3677560," problem, the ZOR neural network with one neuron. Right. So, we recently discussed that with one neuron, it's"
3677560,3687720," impossible to solve the ZOR gate because, essentially, the values of this function are not linearly divisible, super"
3687720,3697560," easily. But because sine is such a funky function, you can solve ZOR, you know, ZOR problem with"
3697560,3703000," with just one neuron, but with the sine activation function. And this is what I want you to try."
3703000,3709080," Right. So, let's actually give it a try. So, since it's such a small neural network, we can just replace"
3709080,3714040," the backprop with the finite differences. Right. So, let's just replace finite differences back. Right."
3714040,3718680," Finite differences require an epsilon. Let's set epsilon to like one thousand or something like that."
3718680,3728600," Right. So, and yeah. So, let's actually make one neuron. So, essentially, this ends up being one neuron,"
3728600,3738680," two inputs and one output. And, right. So, in the library itself, we are using, let's use sine. Okay. So,"
3738680,3745320," let's rebuild everything. And instead of running image2 and n, we're going to do ZOR. Let's see how it's going to go."
3747560,3760120," It's kind of interesting, actually. Alright. So, that's free. It learned instantly. The fuck. Wait a second."
3760120,3771000," Yeah. It just learned it. It just freaking learned it, apparently. Right. So, I would like to know the"
3771000,3776760," parameters of the... It was too quick. I didn't even expect that. Like, what the fuck. Alright. So,"
3776760,3783720," can we just add additional key in here? Right. So, for instance, if is key pressed key s, let's say,"
3783720,3788920," we can try to print the neural network that we're currently training. Right. Can we do something"
3788920,3793240," like that? That would be interesting, I think. Right. Just to see the parameters. Just to confirm"
3793240,3801160," what the hell is going on. Right. Okay. Okay. Sometimes it gets stuck, but sometimes it just finds"
3802440,3809800," just to write configuration and here it is. This one is interesting. So, all right. So, essentially what"
3809800,3817400," you're telling me... Essentially what you're telling me is that you... Like, as you can see,"
3817400,3823400," the bias is straight up zero. Right. Or something very close to zero. It is something very close to zero. If"
3823400,3833960," you just take x, multiply by 157 plus y, multiply by 15... Let's call it 57 as well, because if you round"
3833960,3841640," it up... Okay. 157, 157. So, this acts like XOR. Basically. That's what you're telling me. Because"
3841640,3849000," this is what we found, apparently. Okay. So, let me see. I'm going to start, you know, Python. I'm going to"
3849000,3857400," import math. All right. Import math. And we can... Do we have XOR? XOR is not a function. So, let's"
3857400,3864200," actually define XOR, which is going to be X, Y. And let's simply just return this entire thing in here."
3865000,3875560," I want to copy paste this entire stuff, please. So, this is sine. Can I do math sine? Okay. XOR, 0, 0. Okay."
3875560,3889960," 1. Very close to 1. Very close to 1. 1. 0. Okay. So, as you can see, it is possible to, you know,"
3889960,3894920," come up with a XOR neural network that consists of one single neuron. If you use"
3894920,3900680," very special activation function, a sine activation function. That's actually kind of cool. I didn't"
3900680,3910360," expect that. That was too easy. What the fuck? All right. So, that's why activation functions are"
3910360,3920360," important, right? That's why they're important. So, sine is so funky, right? It is so funky that it can solve"
3920360,3924200," XOR with just a single neuron. That's freaking amazing."
3924840,3930600," All right. So, that's basically everything I want you to explore today, right? So, I want you to"
3930600,3936760," explore this, like, non-traditional approach to backpropagation that we accidentally came up with."
3936760,3944840," And I just wanted to see how different activation functions affect that approach, right? But at the"
3944840,3950360," end of the day, in machine learning, right? So, if you came up with something that drives the cost function"
3950360,3956280," down, that works, right? So, it's not about being mathematically correct. I even heard something,"
3956280,3964920," I think, on Twitter, that machine learning is not really about math or computer science. Machine"
3964920,3971560," learning is more like biology. You're not really building this system. You're just, like, growing them,"
3972120,3977880," like plants, right? And essentially, what you do, you just, like, drive different fertilizers,"
3977880,3984680," right? So, some fertilizers work better, some humidity, temperature work better, right? So, it's not"
3984680,3990920," about mathematical correctness. It's just about driving the cost function down. It's about growing the plant,"
3990920,3997800," the cat-dem plant that interpolates the function, right? So, yeah. It will be interesting to know"
3997800,4003320," the opinion of, like, people who actually do professionally, like, you know, neural networks"
4003320,4010680," and machining and stuff like that, like, why this works, and is it even useful, right? What's interesting"
4010680,4017480," is that we ended up with a very weird approach, right, where we just do extra multiplication by two"
4018120,4024360," on each individual layer in here, right, which is not particularly necessary, but does it really have"
4024360,4034680," to be by two? What if we make this thing, I don't know, half, right? That would be interesting. So,"
4034680,4039400," we can take a look at this thing, right? So, I'm going to go back to the sigmoid activation function,"
4039400,4045240," right? So, and we're going to use non-traditional approach, sigmoid, non-traditional approach, and let's"
4045240,4054040," actually do the image stuff. So, the usual thing that we had before today's session, right? So,"
4054040,4059080," that's basically what we have, and this thing should penetrate the layers of these neural networks"
4059080,4064280," relatively quickly, I think, hopefully, it will. Yeah, there we go, it started to penetrate, like,"
4064280,4070120," around here. Okay, and then it goes too crazy, we can just comment down a little bit, maybe it will learn"
4070120,4076200," something at some point. Yeah, it's doing something. So, what if instead of two, right,"
4076200,4085080," s=2, I'm going to put half in here. Isn't it going to be more calm, right? Maybe, I don't know."
4086440,4093560," Let's just stir the pile of linear algebra around one more time. Just stir it around."
4093560,4102520," Okay. So, well, let's just do and think. I can expect that what it's going to do is just like,"
4102520,4114200," it's going to drop, but much further, much later. Okay. So, it's still not dropping. But is it going to drop at"
4114200,4123320," some point? Oh, it's going to drop. Right. So, it just basically, that parameter delayed"
4123320,4128280," the moment until it actually started to learn something, but it's a little bit more calm,"
4128280,4133240," as you can see. It's a little bit more calm. That's actually kind of cool."
4133240,4142280," All right. It's still learning. It's completely freaking incorrect in terms of, like, actual partial"
4142280,4147160," derivatives or anything like that, but it's still learning. It still drives the cost function down."
4147160,4153400," Okay. What if we say it's going to be three? What if it's not two? What if it's three?"
4153400,4161400," Like, even crazier than that. Right. Is it going to learn super fast, but then it's going to go all over the"
4161400,4165640," place? Right. I don't know. Let's give it a try. Okay."
4165640,4175320," It's not doing anything. It's going even up. Okay. Multiplying by three was a mistake."
4175320,4181240," It's just like it had some stroke in here, and it's just like it doesn't learn anything. Look at that."
4181240,4192920," Now it fails to learn. Two seems to be like a magic number for this specific thing. Right. So two is actually kind of perfect."
4195480,4199880," I don't know. It's kind of weird."
4199880,4201800," Okay."
4201800,4211720," And it starts learning and it goes to crazy."
4211720,4217000," It's kind of funny how it just like goes all over the place."
4217960,4220440," This is the images having a stroke, having a stroke."
4220440,4223560," It's doing something."
4223560,4228520," That's a pretty cool interpolation."
4228520,4234360," That's a pretty cool interpolation."
4234360,4245320," So yeah. Anyway, so I guess that's everything I wanted to check today. Right. So I wanted to take a"
4245320,4253400," look at some other activation functions. Right. And I think having support for different activation function is going to be very useful in the future."
4253400,4257080," So one of the things I heard people say is that"
4257080,4262600," quite often people use different activation functions on separate different layers."
4262600,4265800," And this is something that I would like our framework to"
4265800,4268440," support at some point as well."
4268440,4273960," But that will require refactoring some things. So I'm thinking to introduce some sort of like"
4275080,4278680," maybe notion of the layer. Right. Maybe some sort of a notion of the layer."
4278680,4284440," Right. So if you take a look at the neural network. So right now neural network is just like a collection"
4284440,4290760," of like weights matrices and bias matrices and activation matrices and stuff like that. Nothing particularly special."
4290760,4293720," We could have introduced something like a layer."
4293720,4299960," Right. So a single layer which consists of a single weight matrix and a single bias matrix."
4300440,4306920," Right. So in the sizes of the weights matrix define the amount of inputs and outputs of the layer."
4306920,4310120," And here we could store the activation of that specific layer."
4310120,4318200," Why not? And then we can say that the neural network is actually not a collection of those things, but it's a collection of layers."
4318200,4321560," Right. So we can put a lesson here and the count is the amount of layers."
4321560,4327000," Right. And so that will allow you to stack together like different layers with different activation."
4327960,4332200," And maybe in the future when we start supporting like more different kinds of layers,"
4332200,4336840," we can have a like literally kind in here, maybe layer kind."
4336840,4341560," And one of the kinds is going to be just like a fully connected layer."
4341560,4357560," And another one is going to be something like a convolutional."
4357560,4359880," Right. Convolutional. I don't know."
4359880,4363080," I never actually like developed convolutional neural networks."
4363080,4365720," So I don't know really how to organize all of that."
4365720,4368760," But I hope to learn that as soon as I start doing that."
4368760,4371320," Right. But that's roughly the idea that I want to have."
4371320,4375160," Right. So I want to basically have like different layers with different activations."
4375160,4377160," And maybe in the future, even different kinds."
4377160,4382520," Right. So that will allow us like stuck together different layers of these things."
4382520,4386680," Because with the convolutional neural networks, it's just like first you have a bunch of convolutions"
4386680,4391640," that sort of filter out like noise and reduce the amount of information."
4391640,4397800," And then at the back end of that neural network, you have a fully connected one that makes a logical decision"
4397800,4402280," on the perceived information by the convolutional layer or something like that."
4402280,4404680," Quite often people use only convolutional layers."
4404680,4405960," I don't really know."
4405960,4409960," So, but for the convolutional neural network, we're going to have a separate stream."
4409960,4412920," And we definitely, I definitely want to have support for this kind of things."
4412920,4418200," All right. So that's basically everything that I wanted to explore today."
4418200,4425240," Thank you so much. Yet again, Franz Kim Yonsei, I really hope I pronounce your name correctly,"
4425240,4429480," for this public question and for letting me know that I'm not following the traditions,"
4429480,4432840," the traditions of machine learning."
4432840,4434840," I really apologize for that."
4434840,4437400," I'm not a machine learning person."
4437400,4440200," I'm just, you know, screwing around."
4440200,4440920," Thank you so much."
4440920,4442600," But yeah, that was interesting, actually."
4442600,4449240," And yeah, it's kind of cool to see how different activation functions like affect the results."
4449240,4453720," Right. So they're not just something that you slop on top of the neural network."
4453720,4456520," It's something that you may want to carefully choose."
4456520,4463160," But I'm not sure if I'm at that level yet where I can confidently say that,"
4463160,4466840," okay, for that specific problem, we want to use ReLU."
4466840,4470120," For that one, we want to use hyperbolic one."
4470120,4470760," Like, I don't know."
4470760,4474280," Like, I don't have enough experience, but I'm getting there."
4474280,4475240," I'm getting there."
4475240,4477160," So yeah, that's it for today."
4477160,4479320," Thanks everyone for watching right now."
4479320,4480920," Really appreciate it."
4480920,4482520," Have a good one."
4482520,4486200," And I'll see you all on the next recreational program session."
4486200,4488600," Bye."
