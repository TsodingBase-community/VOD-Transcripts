start,end,text
800,6800," looks like we're live hello everyone and welcome to yet another azuzian session"
6800,14320," today is an azuzian session let's make a little bit of an announcement and officially start"
14320,21120," the stream so uh let's wait until discord really you know recomputes its layout as usual so it"
21120,27760," will take a little bit of time i don't know my my laptop is rather slow so uh live on twitch i'm"
27760,33280," actually considering to just start using something like the alternative discord client if i got the"
33280,41040," name of it uh right so what are we doing today uh what are we doing today on twitch.at television"
41040,46640," today we're developing a search engine in rust holy fucking that sounds actually very"
46640,50560," epic i really like that title that's a very good title that's a very strong title i would say"
51360,59440," so essentially uh what's the idea for today's stream i've been browsing uh the documentation"
59440,66960," for the open gl uh for the open gl lately i think it's called dogs gl and i noticed some cringe in"
66960,77040," this website like a little bit of a cringe and the cringe is essentially uh uh you can only search by the"
77040,84720," names of the freaking functions by the names of the freaking functions you can't really search by the"
84720,90400," uh descriptions of those functions for instance and that's kind of cringe right so look how much text"
90400,96720," you have in here look how much you have in here you can search by all of that stuff uh and so i decided"
97840,105440," hey some time ago uh i did a pretty interesting project uh which is called uh which which was related"
105440,114880," to xkcd i do remember that uh it was related to xkcd let me actually find that xkcd xkcd indexer so"
114880,124240," the idea of this project is rather simple uh basically download uh all of the metadata from xkcd so there is"
124240,132080," the xkcd json.html uh which basically explains the um the api of xkcd you can actually download all of"
132080,140240," the like metadata and whatnot and it gives you like uh alternative text of the xkcd you know the usual"
140240,145200," thing that you have in there and also like some sort of like description or maybe some metadata i"
145200,152240," don't quite remember i think yeah it has a title and alternative text yeah so and essentially it basically"
152240,158960," indexes all of that data and puts it into sql database and then from that sql database you can"
158960,169280," basically search for certain xkcds from the keywords right so and the model that we used to index the xkcd"
169280,176240," was called i think tf-idf all right so the basically the term frequency inverse document frequency right"
176240,182240," it's actually a very simple model i'm not really sure if it's called model but the very simple idea"
182240,188960," uh right so the idea is the following right so basically you have a set of documents"
188960,197200," you have a set of documents and a document is a set of terms in our case it could be like set of words"
197760,205360," so the term frequency is basically how a certain term aka word um how frequently it occurs in that"
205360,212240," specific document right this is a term frequency right so if you put a bunch of terms in search it"
212240,218560," basically it can take you can take your term and see how frequently that term occurs in each of the"
218560,225360," the document and you can basically sort uh the documents by that specific term but on top of"
225360,232160," the term frequency there is idf inverse document frequency and this is a very interesting idea all"
232160,238400," right so essentially you may have a term that is very frequent in uh in all of the documents"
238960,244480," like it is very high frequency in all of the documents so that may occur if we're talking about"
244480,252240," terms like uh a right so the article a right it's a term that is going to be very frequent in all of the"
252240,257840," documents right so an inverse document frequency essentially says the following thing uh right the"
257840,265200," value the rating of a certain term is how frequent it is in that document divided by how frequent"
265200,271360," it is in other documents you see what i mean right so essentially you have a very frequent term that"
271360,276800," occurs in the document but if the same term occurs very frequently in other documents it's probably"
276800,282800," noise so you want to kind of exclude it right so this is basically what idf sort of solves right so"
282800,287040," and uh what you have at the end is the term frequency inverse document frequency your frequency"
287040,291280," gives you how frequently it is in the document and idf tells you whether it's noise or not"
292000,298160," all right so i use this model for this entire thing but i don't remember if i use idf here"
298160,304000," uh i remember that i wanted to use uh idf idf and i think i ended up using only tf which is kind of"
304000,311120," weird but this thing worked really uh really well and uh we even integrated it in our bot some time ago"
311120,315120," right so it's not in the in the current version of the bot maybe i should actually port that to the bot as"
315120,323600," well uh but yeah so and what i decided to do i decided to basically refresh my knowledge of this"
323600,330960," thing and just implement the same idea for the docs gl and it's like rather easy because you have the"
330960,337200," entire website on github right so you can just grab it from the github and furthermore on the github it is"
337200,344320," in a form that is easy to parse right so you have all these sections in here which contain uh xhtml so"
344320,349920," basically as far as you know it comes with some scripts that pre-process the xhtml and turn it into"
349920,356480," a proper website what we can do we can just basically take uh these documents and basically index them"
356480,362880," with tf idf uh directly without any pre-processing and stuff like that and essentially what we can do"
362880,368960," uh right so we have some sort of xml in here we can just strip off all of the tags and leave only the"
368960,376080," text nodes and in the text nodes we can just use tf idf uh and then we'll be able to search using this"
376080,381520," sort of technique uh on all the documents right so maybe you want to search for some sort of like um"
381520,388160," um you know some operation on the texture right so you'll describe it in in sort of a sentence we're"
388160,394320," going to split it into the terms then we're going to look up the tf idf in in the indexed documents"
394320,399680," and arrange the documents how likely uh you're searching for that specific function or something"
399680,406640," like that and to maybe refresh my knowledge of rust i decided to do that in rust right so that's basically"
406640,413440," the idea so that's basically the idea and as uh as soon as i get this kind of like a simple utility i may"
413440,420160," extend it right so i may actually make it work with more than xml's maybe with also text files maybe"
420160,426800," also json files maybe html files you can make it work with html files and then you can write a crawler"
426800,432400," that collects the html's on the internet you can plug these two things together and you got your"
432400,438160," simpler search engine by the way as far as i know like early versions of google like very very early"
438160,444240," versions of google we're basically using that if i remember correctly i'm not 100 sure but i think the"
444240,450560," early versions of google were basically using that and so though the google had a very interesting"
450560,458480," revolutionary idea the idea was to actually give a feedback from the user so essentially the way the"
458480,466080," documents were sorted was not affected only by the terms but also by how often users clicked on certain"
466080,471840," documents on certain websites so essentially if the user clicked on a certain website in the ranking it"
471840,477200," would give it more priority just because other users click on it and because of that uh google at"
477200,484160," the beginning felt like it knew what you want because other people clicked on it so you're more likely to"
484160,489040," actually click on that as well so right now google works completely differently right so it actually works"
489040,496480," worse it's kind of funny have you guys noticed that google like today is kind of like worse than it was"
496480,504480," in the past i wonder if you take the old version of google would it feel better than the current version"
504480,512080," maybe this shed is actually better maybe that's the reason why google was better actually maybe that's"
512080,518080," what you want but i don't know it's kind of weird so maybe the problem right today all like a lot of"
518080,524240," websites they use shed ton of javascript and because of that it's kind of more difficult to index those"
524240,530720," website and that's why google and other search ends are engines are so shut right because there's not"
530720,538960," that many websites left that you can just index or i don't know so yeah that's basically the idea of the stream"
538960,545600," so i can see a lot of subs all of a sudden uh right so let me let me take a look oh boy oh boy oh boy"
545600,552400," where's my soy i need a lot of so if you actually read all of that so um okay apocryphos uh subscribed"
552400,559040," for um for 13 months thank you so much have you considered writing google in rust this is precisely what"
559040,566640," i'm doing today that's right um so maybe i'm gonna make it a little bigger marcia ven netto thank you so"
566640,574640," much for five months uh xia yang thank you so much for 43 months holy look at that 43 months it's insane"
574640,581840," pocchamp iya pocchamp indeed matt kruger thank you so much for three months uh says hey hello hello hello"
581840,589280," uh lumis xh thank you so much for seven months hello mr soiling hello uh i hope you're having a great day i'm"
589280,595040," having actually pretty great days so this is more of like an evening right now uh kenran thank you so much for"
595040,601440," nine months resubscription uh xarist thank you so much for 11 months we're back we're back to back"
601440,607040," champions of the arena uh neil ham thank you so much for 100 beats thank you thank you thank you"
607040,612400," that's like a lot of subs all of a sudden right so i guess i know what i have to do i just have to not"
612400,621520," stream for a week so and when i go back people start subbing like crazy thinking that i'm gone but i'm not actually gone"
621520,630080," okay so yeah um yeah that's gonna be the idea for today so we can just"
630080,637600," you know go ahead grab some data and implement like a simple indexer that also will allow us to search"
637600,642240," within that index right and if we come up with something interesting we can start extending it"
642240,647760," maybe we're gonna add some sort of a web interface uh maybe we're gonna store the index in the database and"
648480,653200," you know slowly but steadily you may actually get some sort of a search engine or maybe this project"
653200,656000," is going to end on this stream who knows maybe it's gonna be boring we'll see"
656000,663120," uh that's that's the beauty of the zozin session zozin session so i think i want to give the links"
663120,669040," to to the different stuff so here is a tf idea uh right for anyone who's interested i'm going to also"
669040,676560," copy paste it in the chat uh right so this is that uh and here is the xkcd intercept that i implemented"
676560,683840," some time ago right so it's here if anyone is interested and uh in the server and also the source"
683840,690880," code of docs.jl so this is what we're doing this is what we're doing all right so let's create a folder"
690880,696240," with a project so how we're gonna call this search engine right so let me think about the name let me"
696240,707200," think about the name for the search engine uh search engine in rust uh yeah let's replace this thing with"
707200,720800," s-e uh cc rust yeah let's call it cyroost uh i really like this name cyroost uh cyroost"
720800,727280," so i'm gonna create cyroost and i don't remember how to use cargo so it can be something like cargo"
727280,735120," emit i think that's how we do that and there we go we initialize that shite uh so the reason why i"
735120,739360," usually don't like to use like build tools and especially cargo even though i program in rust"
739360,744000," all right but today we're going to probably be using some sort of like a third party dependencies"
744000,749040," because one of the things we'll need to do we'll need to parse xml's right and to be able to parse"
749040,755200," xml so we i don't want to implement the xml parser right now it sounds like a fun idea probably i don't"
755200,761680," know maybe not considering the xml standard but i don't want to do it today okay so we have a hello world"
761680,767200," let's actually try to run this entire so this is going to cargo run so it's going to say hello world"
767200,772640," it's saying hello world would you look at that isn't that amazing so the first thing i want to probably"
772640,780720," do i want to basically clone uh the dogs gl website right so let's actually clone it i hope it's not very"
780720,789680," big uh 359 commits so it's not that big but it may take some time i don't want to wait honestly uh so you"
789680,797360," know what i'm going to do i'm going to actually set up a depth because you can clone not all of the"
797360,804560," commits you can clone a certain depth uh i think you can say depth uh one all right and it will effectively"
804560,809120," clone like the last commit and it's going to be super quick you're not going to have an access to the"
809120,815040," whole history uh right but you don't really need to right so in our case we are not interested in the"
815040,820800," history of the project we're more interested in the data that we can happen here so let's actually pick"
820800,829040," a certain file um and start experimenting with that specific file so something maybe like gl clear"
829680,841120," uh right so this is a gl clear and let's take this thing so boom uh go to src and let's say that the"
841120,847040," file with which we're going to be working right now is going to be gl clear xhtml right so the first"
847040,851920," thing i probably want to do i probably want to read that file i probably want to read that file"
851920,861920," so uh so uh we've got a bit more subs so let me actually read now so um saucy time thank you so much"
861920,868080," for twitch prime subscription uh cherry so tofu thank you so much for eight months full bar bus with a"
868080,874640," message full bar but thank you so much a lunge braga uh thank you so much for tier one subscription"
874640,880320," so i didn't weep so didn't weep indeed and packl1 thank you so much for tier one subscription 10 months"
880320,884320," uh thank you thank you thank you really appreciate all the subs even though i don't get the money"
884320,889840," but i really appreciate the gesture you know what i mean i really appreciate the gesture okay"
889840,896720," so how do you even read uh files in rust i don't remember but i remember that you can do rust up doc"
896720,901840," and it will just do a magical thing it will give you rust documentation you can search for"
901840,909600," extensive api and you got a local version of the rust api which is kind of cool right so uh i remember"
909600,917120," that there was a package something like stdfs and within the stdfs you could read some stuff uh read"
917120,924080," to i think it was to string yeah read to string so you can like read the entire thing uh and that's"
924080,931600," pretty poggers my dudes right so let's actually use stdfs right so fs read to string"
932720,938640," so we're going to do file path and then we can say content right so this entire thing of course returns"
938640,944880," results so we have to check if uh and like no error has occurred right so let's actually put some sort"
944880,950480," of garbage in here to trigger an error just to see how it's going to go so you know what i like about a"
950480,959600," result uh i kind of like the method in result which is called unwrap or else uh i think that's how it's"
959600,965360," called yeah so unwrap or else so essentially it tries to unwrap the value but if it couldn't unwrap"
965360,970800," it it allows you to handle that situation so you can provide like some sort of a closure and that"
970800,977840," way um i like to actually handle it similarly to how i handle errors in c all right unwrap or else"
977840,984480," right in here i've got an error and i usually like to report it in a very specific format i like the"
984480,992240," format that i report errors in so the usual style uh c style reports right so just error and then you say"
992240,998960," could not read file file path because of this reason right maybe file doesn't exist and then"
998960,1006000," i like to exit like uh you know exit sysco and exit with like non-zero exit code this is what i like to"
1006000,1012720," do i know this is not traditional right i know this is not traditional in rust community but this is my"
1012720,1022080," my project i like to do things my way uh right um so it's kind of funny how programming in rust feels"
1022080,1028960," like running like a um you know food channel you have to always say that this is not traditional but"
1028960,1034160," this is just like how i like to do that right so if you don't say that people is going to like in"
1034160,1039840," fucking uh downward here and stuff like that uh all right so we want to do process and we're going to"
1039840,1045600," do exit or something like this right so we've got the content so the thing i probably want to do"
1045600,1052160," uh i want to print the content and just see what's inside of that file so let's do uh cargo run"
1052160,1059920," uh and what do we have in here so we forgot the semicolon uh okay so as you can see it says could not"
1059920,1066240," read file uh no such file or directory so and this is the style of like errors that i like this is like"
1066240,1072720," a usual classical c style and just like yeah anyway so uh as you can see there was an error but if we"
1072720,1078960," change this entire thing uh there will be no error and here is the stuff here is the stuff which is"
1078960,1084400," rather convenient so this entire thing if i remember correctly right if i remember correctly"
1084400,1092560," um so we probably want to go to here it returns you literally literally a string uh right we can say"
1092560,1100960," something like um size of file path is maybe let's put it this way length of file path"
1100960,1108080," is length and here's a cool thing that i recently learned in rust is that in println you can do just"
1108080,1115440," length equal uh content length right and it will just print it like that so it will basically use"
1115440,1121040," here whatever expression you put in here but since it's named you can actually put that in any order"
1121040,1126480," that you want and file path is taken from the current scope right so it's kind of interesting way"
1126480,1132640," of doing the uh string interpolation so this is the length of the file this is the length of the file"
1132640,1138480," so as you can see we're capable of reading uh files which is kind of cool so the thing i want to do now"
1138480,1146000," i want to be able to parse the xml right so the parse to parse the xml so let me take a look uh as far"
1146000,1153600," as i know in crates.io i right before the stream i just looked up uh what are the common xml uh parsing"
1153600,1160720," libraries and i think uh we want to use xml.rs because that's the thing that all of the other xml libraries"
1160720,1165760," use right all of the other libraries are just basically wrappers around this thing and this is"
1165760,1170960," like a pure parser that is written in rust with the minimum dependencies and this is what i like to use"
1170960,1175840," i don't like to use too many transitive dependencies that wrap around different frameworks right so"
1175840,1184880," 30 xmlrs is basically like a glue between a serialization framework and xml parser and i don't"
1184880,1189520," need a serialization framework i just want to parse a little bit of xml right so why do i need this"
1189520,1194720," additional glue uh so i'm not going to use that this one is actually really nice right so it's like a"
1194720,1200800," very minimal amount of dependencies it depends on lip c for some reason as far as i know um"
1200800,1207520," it doesn't really depend on anything so there's only dev dependencies oh there's like zero dependencies"
1207520,1211760," basically yeah so which is which is kind of cool right so this is what we're going to use"
1212640,1219120," so you can add it i suppose to cargo like this you can just say cargo at and it will automatically add"
1219120,1226160," the syntax hopefully i should have not actually used this uh way of doing this so if you do shell command"
1226160,1234000," in emacs like that it actually blocks uh emacs until the commands is actually finished and if there is any"
1234000,1240320," sort of a progress right now we can't see that at all i can try to like do something like this and just try"
1240320,1247600," to do it like that and now it's updated oh that's why it couldn't yeah so you can't you can't just add a"
1247600,1255600," dependency you need to update the index right you can't just add things uh all right so i think we've"
1255600,1265520," got a bunch of uh subs yet again so let me see omega sad life thank you so much for six months of tier one"
1265520,1271360," and come composting i'll pronounce you and you can correctly thank you so much for tier one sub"
1271360,1280480," all righty all righty so we're updating the indices right so i guess if you're watching on youtube i'm"
1280480,1286000," probably gonna cut this because there's nothing much to to say so um"
1294880,1299040," so how's it going everyone how's it going everyone the youtube people cannot hear us"
1299040,1304560," so you can tell me everything that you don't want the youtube people to uh to hear"
1304560,1309600," oh look it's adding the dependencies okay don't tell me we're already done"
1309600,1314160," all right so it finally edited it really didn't took that much time"
1314160,1324240," so um let me let me see okay so here's the dependency which which is nice uh cargo build"
1324240,1335680," all right so i don't remember can you do just like doc uh and create documentation yeah so for xml rs"
1335680,1342640," and to be able to use that documentation can you just ask doc to open the browser just like rust up"
1342640,1350800," oh yeah you can do that actually that's very cool um open is it gonna open it in the browser i i don't see"
1350800,1359840," shit is it opening it it couldn't open that so i would expect this thing to use something like uh"
1359840,1365600," xdg open right which automatically like the text what kind of browser are you using but it couldn't"
1365600,1371280," really open it for some reason so i can open it myself like at least it printed uh the the thing"
1372160,1380560," right okay so here we have xml and i would like to see some stuff if i remember correctly this thing um"
1380560,1391680," is a sex browser right so sex so do you guys know what is a sex poster uh sex possible uh why is it in"
1391680,1398880," russian you should switch the language like why is it keep giving me in this weird language that i don't speak"
1398880,1406480," anyway so simple api for xml so essentially uh usually when you think about like parsing a certain"
1406480,1411360," format you're thinking about i'm going to give you a string and you're going to give me ast and then i"
1411360,1420160," traverse that ast myself so the sex parser for xml is not that it is not that it is event based so"
1420160,1429760," essentially it doesn't build any ast but it traverses the xml and every time it encounters"
1429760,1437760," for example an opened tag or a close tag or text node it basically calls your callback you provide"
1437760,1443920," a bunch of callbacks for the things that you're interested in and uh the the posse is going to call"
1443920,1451520," those callbacks for you right and um if you don't really need to um you know to parse the entire uh"
1451520,1456560," the entire xml tree and you just want to extract some some information this is actually ideal and this"
1456560,1462320," is precisely what we want to do i don't really care about the the structure of the xml right so"
1462320,1469120," the document that i'm looking at so the document that i'm looking at uh right it has some sort of a structure"
1469120,1474560," but i don't give don't give a shit about it honestly uh so i probably have to do docs what"
1474560,1484960," is that gl4 gl clear uh xhtml so the only thing is i'm interested in is uh this stuff right the text"
1484960,1492560," nodes the actual text nodes i want to extract those so if i can put some sort of a um some sort of a"
1492560,1498480," callback when you encounter a text node and then i'm going to take all of the text that i encounter and"
1498480,1503760," append it to the buffer and there you go i have a text that i can actually index with tf-idf"
1503760,1510880," right so then i can split it into the terms uh then i can um count the frequencies of these terms"
1510880,1517040," and put them in some sort of a database that they can then use to um to compute tf-idf right so that's"
1517040,1522080," basically the idea right and what's cool is that we can implement it in the future generic enough that"
1522080,1527360," maybe we'll support more formats right again so right now we're supporting xml but who said we can't"
1527360,1531760," support just text uh supporting text is going to be easy just to read the entire text without"
1531760,1537840," parsing it then we can try to support pdf who knows maybe you will find like a library that allows us to"
1537840,1545680," extract text from pdf and then you'll be able to index pdfs uh right and then html right so and maybe"
1545680,1549920," like i kind of like this idea i kind of want you to have something like that for quite some time"
1549920,1555760," so basically i would take my folder where i store different documentation that i downloaded from the internet i"
1555760,1562000," quite often by the way download documentation from the internet locally to have it locally because i"
1562000,1569040," have these interesting offline sessions when i want to work on something and to get rid of all of the"
1569040,1574000," possible distractions i basically turn off the internet completely like i imagine that i have a computer"
1574000,1580240," so it's some sort of like a post-apocalyptic scenario we don't have an access to the internet but i have my"
1580240,1587680," laptop left and uh whatever i have on that laptop only that i'm allowed to use right and because of"
1587680,1594640," that it actually creates a very interesting effect i can focus on what i need to do and because i don't"
1594640,1600640," have an access to the internet it actually forces me to think more creatively when i'm trying to solve"
1600640,1605440," problems you know what's interesting right so essentially i'm working on a project that uses certain"
1605440,1612000," dependencies certain libraries right before going offline i'm downloading the source code of all of"
1612000,1616720," the dependencies that i have and the documentation for all of the dependencies that i have right and"
1616720,1622160," because i don't have an internet i'm kind of forced to read documentation right to understand what's going"
1622160,1627280," on and quite often i'm also forced to read the source code of the dependencies so this kind of thing"
1627280,1633440," forces me to actually start reading the source code of the libraries and i actually gained more insights"
1633440,1639120," into how libraries works and how to do things without those libraries those simplify my projects"
1639120,1645200," right so and again you can't really force yourself to do that unless you like physically don't have an"
1645200,1651040," access to the internet and stuff like that it sounds weird it sounds very inefficient we're living in a"
1651040,1656800," very sort of like efficiency driven world uh that's why we develop chat gpt and stuff like that"
1656800,1663280," but but but by doing that i actually learned a lot and i gained a lot of interesting insights and it kind"
1663280,1670160," of like in some sense it's kind of therapeutic you know what i mean it's like you cut off yourself from"
1670160,1676080," the outside world and all of a sudden like all of the noise that has come from the internet it just like"
1676080,1683360," shuts off and you just like postpone for a second and you just like enjoy the moment right without"
1683360,1688080," all of the noise from the internet that is constantly driving you to be more efficient more efficient"
1688080,1695040," develop faster faster faster faster it's just like shut the fuck up just and you get some time to think"
1695040,1700640," to ponder upon the thing even though it's not that efficient but no nobody's actually driving you to be"
1700640,1706160," super efficient because you cut off all of these voices that's coming from this goddamn ethernet cable"
1706640,1712320," you know what i'm talking about right and because of that like i have a folder with the documentation"
1712320,1718080," and if i have a tool that can index that documentation i can have my own local google"
1718080,1727440," isn't that cool right i can have my own little google that works kind of like an old version of"
1727440,1735520," google with tf idf and stuff like that it it could search by xml's by html's by uh you know pdfs and stuff"
1735520,1742800," like that so i have like a bunch of also computer science white papers that are in pdf so why not"
1742800,1748880," search by in them like that so that sounds like a pretty cool idea like yeah you have like a bunch"
1748880,1754080," of stuff in the folder you can say index that folder and search in that folder like google"
1754080,1761760," and since you don't really have the amount of data of the whole internet you don't need the infrastructure of"
1761760,1765040," google you can be google on a smaller scale"
1765040,1773360," hmm can't shut off voices of the internet but can't shut off the voices in my head"
1773360,1777520," relatable truly relatable and okay"
1777520,1780480," anyway"
1783840,1791120," so yeah that's basically the stuff that i wanted to get out of my system right so because"
1791120,1795840," i kind of i don't know i'm not that old but i remember the times when the internet was"
1795840,1803760," much much much less noisy than it is right now and it was like and you could actually have a productive"
1803760,1809920," and meaningful conversations uh on the internet right now go to any social network it's just like"
1809920,1814240," people screaming at each other without even listening to each other that's what's weird"
1814240,1820000," like you you look at like two people on the internet or on twitter arguing with each other you read what"
1820000,1824720," they say you try to understand their arguments and you can clearly see that they don't listen to each"
1824720,1829600," other argument they're just yelling at each other for no apparent reason it's just like so weird to me"
1830160,1835040," that's what internet turned into just people yelling at each other uh right so"
1835040,1838240," anyways i'm sorry"
1838240,1847040," all right so let's go and let's try to parse some xml"
1847040,1850160," let's try to parse some maximals bruv"
1851040,1859120," okay so we probably need to have some reader uh yeah reader event reader would you look at that we"
1859120,1866240," have event reader and i can create uh event reader from something that is called source and source is"
1866240,1872720," something readable so it you can create like a read interface and you know what implements the read"
1872720,1879120," interface a file right so if you take a look at the file so i think it's it's located somewhere in the"
1879120,1887600," fast yeah so if we take a look at the file and what kind of interfaces does it implement uh it should"
1887600,1894240," implement the read interfaces is there a list of the interfaces yeah there we go so you can open a file"
1894240,1901200," and it implements the read interface so that means i can feed that file into the event reader um right"
1901200,1908000," and then i can start i can start pulling off oh so does this thing implement it also implements an"
1908000,1916160," iterator okay event reader accepts something that is readable and by itself it is an iterator so we can"
1916160,1923600," iterate through the xml events right so in here uh yeah xml events is one of those things right"
1923600,1929520," so we can try to do that though you can convert it into iterator right so it's not really iterator"
1929520,1934400," by itself but you can convert it into iterator so and how do you convert into iterator i don't"
1934400,1941520," remember what's the methods why are you redirecting me to this website okay so into iter right so"
1941520,1947760," that's what you have to do okay that's cool all righty all righty all righty all righty all righty"
1947760,1954160," all righty so can i just use xml right so let's try to use the xml and this is not what i want i just"
1954160,1962720," wanted to build just wanted to build yeah i can't do that isn't that poets so and essentially what we can"
1962720,1970720," do uh we can do xml uh oh you have to do something like okay reader"
1971520,1983920," uh event reader event reader so event reader new uh and might as well also include file right"
1983920,1995840," so file open this thing so but when i open the file when i open the file does that operation fail well"
1995840,2004160," it can file right so that means you can't just open but this thing also doesn't accept a result right"
2004160,2009120," so that means we we can actually just like use something like this we can take this entire thing"
2009120,2015520," and put it in here right it looks like shit by the way so maybe it will be better to actually have"
2015520,2020960," something like uh like this in here right could not open file blah blah blah and then you can"
2021600,2027920," just put the file in here and you have event reader uh and the event reader is just like yeah so it's"
2027920,2037920," called er so this is the event reader and here i can do something like in into ether right and try to"
2037920,2044240," just print this entire thing but i'm pretty sure you can't display an event but you can probably debug this"
2044240,2052000," thing right uh okay so let's try to to build it and see okay so it's actually correct code"
2052000,2056880," let's try to run this entire thing and we got all the events this is actually very easy what the"
2056880,2066960," fuck right so yeah so for example when an element starts it you get an event stat element uh when something"
2066960,2072720," ends you get end element uh and i think what we care about right so for example i saw the"
2072720,2080720," copyright or something copyright yeah there we go uh not really copyright but okay characters i think"
2080720,2086640," that's the uh event that we care about we just care about the characters you know what i mean we just"
2086640,2093520," care about the characters so maybe because of that um right i'm going to only filter those"
2096080,2108240," so interestingly uh is there any thing um i'm thinking like a haskell programmer right now because"
2108240,2113440," i feel like i want to have a sequence you know in in haskell there is a sequence which essentially"
2113440,2120000," accepts like the list of uh uh applicative or monads or something like that and returns you the"
2120000,2126240," the applicative of list uh right something like that so but i think i actually proven to myself"
2126240,2130880," that programming like that is not particularly efficient so i'm not going to do that right so"
2130880,2138560," essentially if uh event uh we can actually try to unwrap this entire thing or even unwrap or else"
2138560,2140160," or else"
2140160,2149840," so and i can do it like that so as far as i know in rust it is actually very common to"
2150560,2157440," shadow bindings right so because that's what it is common to do in ocaml uh right so for instance"
2157440,2162480," you have a binding event and you shadow it in here replacing it and it's totally fine"
2163440,2169520," uh in uh in rust as far as i know right this is totally fine in rust as far as i know"
2169520,2173920," okay so i'm just like looking at some stuff"
2173920,2182320," unfortunately twitch doesn't really have a feature that would tell me that i already acknowledge some"
2182320,2187440," of these people so in some of the other services like stream labs or stream elements you can click on"
2187440,2192400," people uh in the up i don't want to report the person but yeah you can click indicating that okay"
2192400,2197760," i already acknowledge that person but you can't do that on twitch uh right so twitch doesn't even"
2197760,2202080," understand the need of its users how unusual who would expect"
2204000,2216960," uh right so error could not read next xml event right uh notting nassau uh i'm not sure how"
2216960,2221920," to pronounce your nickname notting nassau thanks thank you so much for uh twitch prime uh with the"
2221920,2228560," message sub hello hello hello hello welcome welcome welcome to yet another azuzian session"
2228560,2233840," session i'm sorry for speaking uh all right so i'm here we're going to just exit instantly"
2233840,2241760," so here's an event and the uh thing that i care about i only kind of care about the uh the character"
2241760,2252960," event right if the characters uh so let's put it like this text if let characters text event uh right we're"
2252960,2260000," going to print that specific text right so but here is an interesting thing the characters right"
2260000,2266400," so characters uh what's the parameter for that thing so here's the characters and it's a string"
2266400,2272320," it's literally string so that means i can just like literally print like that all right so uh let's go"
2272320,2277920," characters not found in the scope and this is because it is available in the in the reader so"
2280000,2290480," i probably need to uh i probably need to go and import xml event right so this is going to be xml event"
2290480,2302720," and also i have to put xml event in here so here's the xml event uh a poggers isn't that poggers i think"
2302720,2309600," it's yeah there we go so we we have a text we basically have a text uh right so maybe we can collect"
2309600,2316400," that text into some sort of a buffer uh it's not really particularly interesting text uh but there"
2316400,2324960," are some descriptions in here right so yeah so there is a description uh sets the bit plane blah blah blah"
2324960,2333280," uh so some parameters in here if you're searching for one of these parameters so this document may appear"
2333840,2341840," in the results right so these things are kind of important and stuff like that so yeah um so what"
2341840,2346320," we can do in here we can actually build some sort of a buffer right so this is basically buffer let's"
2346320,2355360," call it content uh string new and what we want to do in here we kind of want to append uh this thing to"
2355360,2365040," the to the string but the question is uh string does it have something like can i append uh think you can"
2365040,2375440," extend uh how can you extend you can extend with an iterator so you can push str this is actually"
2375440,2385600," perfect right so content push str and it can probably just you know do like that there we go and we can"
2385600,2393280," just print the the content so and the cool thing here is that now i can wrap that into uh some sort of a"
2393280,2400000," function and it will return me the content of that xml file right so the content of the xml file that i'm"
2400000,2406000," really interested in uh right so this of course content has to be mutable sure because this is rust"
2406000,2414480," it's being anal like that but that's fine that's just rust uh okay so that's cool that's pretty cool"
2414480,2421360," actually so there's also some sort of like uh yeah uh how is that called like a template parameters"
2421360,2430160," and whatnot uh murula uh mirila zero br or obr thank you so much for uh to one subscription thank you"
2430160,2440720," thank you thank you thank you thank you thank you uh all right so let me let me see let me see okay"
2440720,2449200," that's cool so let's create some sort of a function right so read entire xml file right so in here i'm"
2449200,2456560," going to do file path and then i can say okay uh max what are you doing give me the entire"
2456560,2466240," shit right so essentially uh can i just do something like are you are your results and this is going to"
2466240,2474560," be a string so and then what i can do i can use another function on a higher level uh that finds all of"
2474560,2480000," the xml files and then i can call this function on each individual xml file and get the content of all"
2480000,2485760," of the files right so that's basically how abstracting away things works i'm not quite sure why i'm not"
2485760,2492800," using file path in here right so that's a very simple mistake uh right and essentially i want to move"
2492800,2502000," this entire stuff to here and here is okay content uh right and what's cool now is that i'm inside of a function that returns the result"
2502000,2509040," which means i can just do something like this right so i can start like using this uh question mark uh notation"
2509040,2515680," which allows me to sort of like unbrop things very easily um furthermore this allows me to inline this"
2515680,2521520," entire thing those simplifying this stuff dramatically uh right so as you can see now it's a very simple"
2521520,2529760," function right so the content is basically read entire xml file so this is a file pass so for now i can just"
2529760,2539120," unwrap it uh right but yeah there we go so we don't use an exit but we're gonna use it soon um so oh yeah"
2539120,2544880," there is some incompatible errors and stuff like that you can't easily do that so and this is because the error"
2545520,2555040," is uh yeah the trade from uh from error is not implemented and the error is supposed yeah okay"
2555040,2561680," so the way you're supposed to sort of solve that if i remember correctly you create"
2561680,2571760," an error that encapsulates both uh io error and xml reader error right and basically implement like a from"
2572480,2578480," xml error that converts it into into your more abstract error and also from io error that converts you to"
2578480,2584720," abstract error or something like that so that's how we usually solve that uh right but uh for now"
2584720,2589680," i'm not really sure if i really care about that so it's gonna be interrupt to do"
2591840,2606160," uh so in this one is uh to do uh so i think it's expect right yeah it's expect and this one is also"
2606160,2616480," expect this one is also expect and there you go so what i want to do now instead of having like a file path"
2617200,2623520," uh i want to have a deer path all right so i want to have a deer path and i want to take a look at all"
2623520,2633360," of the files within that directory right so how can we do that if i remember correctly so fs has something"
2633360,2640960," like a read uh directory does it have a read deer it does have a reader i do remember that correctly holy"
2640960,2648400," shit so you can provide the file and you get a read deer which is uh an iterator right so it is an"
2648400,2657440," iterator and the element of the iterator is a deer entry it is a deer entry and deer entry it has a path"
2657440,2663360," associated with it it has some metadata so you can have a metadata about the uh stuff you can have file"
2663360,2669040," type uh right so this is very convenient so you have a function that you can say okay read the directory"
2669040,2674240," it's an iterator you can iterate through each uh through each individual file of the uh of the"
2674240,2682880," directory so uh tronmtg subscribed with uh twitch prime saying that c is better than rust and i could"
2682880,2689280," not agree with that more i could not agree with that more i really like c actually like c more than rust"
2690480,2701680," but then even though rust fixes the safety problems of c it is not even near to simplicity of c and for"
2701680,2706800," some reason in the modern world nobody asks the question can you have a safe language without"
2706800,2711920," sacrificing the simplicity and the problem is that it's kind of difficult to answer that question because"
2711920,2719600," simplicity for majority of people is kind of like different uh right and on top of that usually like"
2719600,2728560," these kind of like rust languages attract very smart very enthusiastic uh individuals and the problem"
2728560,2736960," with very smart young enthusiastic uh individuals is that they are very easily nerd sniped they're very"
2736960,2743280," easily nerd sniped so if you don't know what is a nerd sniping uh google that so it's a xkcd joke but"
2744000,2751360," it actually describes it uh very uh very accurately and because of that they are spending too much time"
2751360,2759840," on very complicated interesting but not important things you know what i mean so essentially rust right"
2759840,2768640," now is a sandbox for very smart people to build interesting toys to play with right so the build"
2768640,2773920," like a lot of interesting abstractions on top of structures on top of structures frameworks and stuff"
2773920,2778320," like that and you're gluing together package managers all that like the castles and stuff like that but"
2778320,2782480," where's the actual work where's the freaking actual work"
2782480,2786400," yeah so"
2790800,2798160," we need a safe language but also the one that doesn't try to that cares about simplicity as well"
2798160,2805040," i know that things come at a cost right so if you want a more secure language you have to pay"
2805040,2811280," maybe with simplicity a bit but it's just like in in case of a rust the simplicity is just like thrown out"
2811280,2816240," of the window completely just that security is the most like simplicity we're not gonna even try to"
2816240,2821040," optimize these two variables it's just like we're gonna pick one and make absolutely miserable language"
2821040,2829360," but safe safe right just let's try to balance between these things like come on jesus"
2829360,2836080," anyway but it's not really possible with the current situation so it is what it is"
2844240,2851680," c plus plus is complicated and unsafe yeah exactly so it's the worst of both worlds um anyway"
2851680,2868400," i guess it's kind of like a more a psychological problem right so when people work with simple"
2868400,2874160," languages right they don't feel like they're doing something important maybe that's the problem"
2874240,2882880," um right so for instance they go go is a very simple language right on purpose and quite often"
2882880,2889680," like uh less experienced developers when they program in go they feel like they're being like a small"
2889680,2896880," cog wheels in a huge corporate machine right uh which is kind of understandable right so people want to"
2896880,2902400," feel important people want to feel special um but in reality"
2902400,2914000," okay so we got the uh holy shit we got 10 gifted subs from uh dear bear zero thank you thank you so much"
2914000,2921520," for thank you so really appreciate that so i suppose my rust rant works uh i should rant about rust more"
2925360,2929840," um"
2929840,2932160," all right"
2932160,2938000," so uh let's actually read the directory let's actually read the direct thing"
2938000,2945200," how do you even freaking read the directory completely forgot yeah you have to do the"
2947120,2952160," the usual fs read dear dear dear path"
2952160,2957520," uh dear path i wonder if i can just like do something like io"
2957520,2968240," result and yeah since i'm streaming maybe i'm not gonna spend too much time on errors and what not"
2971680,2979040," okay so uh read dear oh i remember i think you can do something like this"
2979040,2984880," which means that it will also import fs so then you can just click use fs in here"
2984880,2992160," which is kind of interesting right so okay so there was a bunch of warnings but one of the things i want"
2992160,3003440," i want to do it like this uh so dear uh so we're gonna call it dear all right so we've got the dear"
3003440,3010640," and then what we want to do we want to iterate files in dear all right and let's just bring those files in the directory"
3010640,3017120," right uh so this is gonna be file and let's see what we've got and here are the files"
3017120,3022480," it's as simple as that look at that look how easy it is to just read a folder"
3024080,3036480," isn't that bow isn't that bow i think it's okay and we'll have to index all of this um okay uh right so"
3036480,3045200," then if i want to do something like file equal file um so since it's okay right since it's okay we can also"
3045200,3060640," do something like um maybe file so question mark uh and dear entry let me find the due entry where is the"
3060640,3068720," dear entry uh a fan path yeah that's what i wanted to have i wanted to have a fan path but the fan path"
3068720,3074240," is a path buffer it is a path buffer so you can't yeah you can only print it with like um"
3075200,3082640," uh with a debug stuff you can only print like that and there you go so we have we have the strings"
3082640,3090080," we have the strings which we can then uh path through this entire stuff right we can path the read"
3090080,3100640," entire xml file there we go file path uh reading time right so we can have a content and we can say"
3100640,3109920," something like so let's call it like this file path is basically uh file path in here so then we read that"
3109920,3118160," and then we can do the following thing so file path and uh let's bring the size of each individual file"
3118160,3125600," path and the size is basically the content length all right so essentially we can iterate through each"
3125600,3135120," individual files and we can compute their lengths uh file path file path um so expected str but found"
3135120,3142720," this thing so as far as i know like quite often uh the functions do the following thing uh read"
3142720,3149840," to string they don't really accept the string but they accept something that is convertible to path yeah as"
3149840,3156320," the ref path blah blah blah blah blah blah uh we can kind of try to do a similar thing but it's like it"
3156320,3162160," the thing about the rest of that i didn't like it just adds additional noise right it adds additional"
3162160,3170960," noise for you to think about uh right so here's the file path uh let's give it a try and also i'll"
3170960,3179680," probably have to find the path uh somewhere here so it's going to be a use std path uh path right so"
3179680,3186480," you have to do it like that and let's take a look at the errors so this is a content uh length and the"
3186480,3194000," content is an error so we probably want to do something like this yeah uh what's the next error uh since oh"
3194000,3200720," since it's it's that uh we have to display it properly but here i can just do the this stuff okay"
3200720,3208000," what's the next error uh path was moved uh-huh okay so that means i want to actually borrow it by"
3208000,3210080," by this and then we go look at that"
3210080,3218480," you look how it's slow look how it's slow it's lagging a little bit it's not like instant"
3218480,3225760," that means the program is doing some work that means the program is doing some work i mean well"
3225760,3233360," why slow people asking why so big it's parsing xml it's parsing xml it's actually doing work"
3233360,3240480," it's not cat it's not just text files you have to parse xml and just compute things around you"
3240480,3247760," you know so that that means it's serious that means it's important you know there was an interesting thing"
3247760,3255520," when only when the computers were only introduced into people's workflow right people had a huge"
3255520,3263120," problem with trusting the computers because they were working too fast right they were working too fast"
3263120,3270240," like they couldn't believe that you can uh do this complicated intellectual work so freaking fast you"
3270240,3277440," it must be making mistakes it must be making mistakes and because of that for some time uh developers on"
3277440,3282720," purpose put like slips in the in the software to make it slow because people didn't trust it enough"
3282720,3290480," right so it's the same you you see it's slow it's doing something right so you can trust the result of"
3290480,3294400," this program right like otherwise like why would it spend so much time"
3294400,3305920," or if i really well i mean i heard about that on a level of rumors maybe it's just like a you know made up"
3305920,3311120," story but this is something that i was told by other uh more experienced developers in the past so"
3311120,3318320," well i mean it sounds plausible enough let's put it this way it sounds plausible enough"
3325760,3333600," well i mean on our discord server somebody recently posted a video about how if you put if you make"
3333600,3341920," your interface slower you get a better ui ux which is basically the same idea isn't it right"
3344480,3348240," do you remember do you remember that somebody posted that on our discord server recently right so"
3348240,3356640," basically make your um your interface uh slower and you will increase the uh the usability because"
3356640,3362800," it's it's too fast i i can't comprehend what's going on or something like that uh yeah it's basically"
3362800,3370960," the same idea isn't it anyways um so but maybe another thing maybe this is because we're"
3370960,3375120," not running in release mode so let's actually try to run but in release"
3375120,3380960," so it's gonna take some time to compile and release because you need to apply some optimization"
3382800,3390080," uh yeah no it's still rather slow so that means it's actually doing some important work"
3390720,3394960," it is actually doing some important work okay anyway"
3394960,3412560," so um what i'm thinking is actually actually slightly faster just a tiny bit faster"
3415360,3421760," the next thing we need to do okay so we can already quite easily start introducing the following"
3421760,3430800," things so std collections uh hash map right so and essentially uh for each individual file"
3430800,3437360," for each individual file you can build a function um let's put it this way uh let's call it index for"
3437360,3443040," now but maybe i'm going to change its name a little bit like later um so the content right we accept"
3443040,3450800," the content which is going to be str but what we want to get we want to get a hash map uh of a single"
3450800,3458560," word or a single term uh and its frequency like how frequently does this entire shit happen right"
3458560,3470800," implement it uh index uh document right uh doc content and i'm going to put on the screen here right so"
3470800,3476320," oh i think yeah there's something like this but i mean to do that that's the trick as well"
3476320,3486560," okay so you index document and then uh we can build a bigger hash map right so we can then build a bigger"
3486560,3496000," hash map uh right so i'm not sure how to call this thing but um you know all documents"
3497520,3504560," which is essentially hash map from the path right so we can call something like path and whatnot from the"
3504560,3511520," path to the document to the frequency of the words within that document right so within that document"
3511520,3520640," and a string use size and this is basically sort of like the final index we're looking for right uh this is the"
3520640,3528480," the final index we're looking to build so you have a hash map map from a path to a file to a frequency of"
3528480,3535680," uh individual terms within that file within that document uh right so something like that and maybe"
3535680,3543840," uh we'll be able to save this entire thing in in the file system i was thinking if the indexing is going to be fast enough"
3543840,3550240," maybe we never have to actually save it because we can just basically re-index everything uh you know"
3550240,3554320," every time we want to search something but looking at the current performance of the application"
3556000,3559200," i think it makes sense to actually save this hash map somewhere"
3559200,3568000," uh right so and this is just a single folder by the way chat chat chat look this is just a single"
3568000,3578160," folder we have several of them uh right so this is one folder uh right we have also gl3 right we have gl2"
3578960,3587440," uh we have uh embedded stuff and stuff like that right and i wonder if this is because of the xml maybe"
3587440,3594080," the xml parser is so slow right because yeah that's what it feels like it feels like xml parcel maybe"
3594080,3602480," with the text file it would do that uh differently so yeah it would be also kind of cool if this the final"
3602480,3609680," system that would build uh you could give it any file and it tries to detect what kind of file it is"
3609680,3617600," maybe by extension for now like txt xml html pdf and depending on the type of the file it would use"
3617600,3622640," different passes for this specific document you know what i mean probably kind of cool"
3624800,3634240," that's pretty powers that's pretty uh so yeah so we need to index document but here's an interesting"
3634240,3644160," thing right so the technique generally is talking about the frequency of the terms but the question"
3644160,3651120," how what is a term right yeah let's take a look at uh one of the existing documents right um"
3651920,3659520," so let me let me grab the compilation uh gl clear uh maybe i'm gonna i'm gonna just grab this thing"
3659520,3669840," i'm gonna just grab this thing and i'm gonna read entire xml file right so we're reading entire xml file"
3669840,3671760," like so"
3671760,3678720," and then i unwrap this entire stuff and then i'm gonna say okay"
3679440,3682480," print the content of this thing like so"
3682480,3690960," boom and uh let's comment out everything else everything that we don't really care about i don't"
3690960,3696480," think we need that stuff well i mean this is literally what i was doing in here okay sure"
3696480,3705360," uh okay and let's just try to run the entire thing should have probably no not used the"
3707040,3715760," the release because it doesn't really matter right so what is the term because you can split everything"
3715760,3724960," by spaces right you can split everything by spaces uh but then you have uh words that contain um commas"
3724960,3731440," they contain commas and what if we have the same word zero somewhere without the comma is it going to be"
3731440,3737520," be considered like a different name or like like this one with a dot or something like that how do we"
3737520,3742960," handle these additional noisy characters that punctuation characters and something like that"
3742960,3749280," how do we handle them uh one of the way we can handle them is basically filtering them out"
3750240,3757200," but this entire thing kind of feels like it feels a little bit familiar"
3757200,3765200," don't you think the problem of figuring out what is individual um individual element of the text"
3765200,3770080," right so what is the individual element of the text is it things that separated by spaces"
3770080,3774800," or within the characters also there is some sort of additional meaning that actually"
3774800,3779920," that actually tells us that this is not uh a single term but actually two of them"
3779920,3787920," you know what i mean right so space is not the only thing it's not the only thing that tells us"
3787920,3793440," uh the differences between the the borders between the terms there's also the terms themselves"
3793440,3798240," right we can treat dot as a different term so this sounds like"
3799840,3806240," tokenization not only tokenization but the actual tokenization that is used in programming languages"
3806240,3814320," it's literally the same problem you know what's funny i never realized that until yesterday until i"
3814320,3820000," got the idea for today's stream i never realized that that this is the same problem that we solve in"
3820000,3824480," programming languages like literally the same problem and you can use this same freaking code"
3827280,3832400," i like and it's kind of weird like how didn't didn't i realize that and it actually makes"
3832400,3838400," everything so easy right so you split it into tokens the same way you uh tokenize a programming language"
3838400,3848800," and a single token uh and a single token becomes the term and since comma and dot are terms that are"
3848800,3856240," encountered in documents so frequently the tf idf is gonna filter it out as noise automatically"
3857280,3861680," you don't even have to think about that this is so cool think about that right so because the"
3861680,3867280," comma and dot are going to be the terms that the occurs very frequently within the documents so the"
3867280,3875520," system the the formula automatically filters it out as a noise that is freaking perfect this is so cool"
3877280,3887920," don't you think chat is that freaking polar okay so uh let me take a look so thank you so much for"
3887920,3894800," uh additional sub from m2g yeah i already addressed that uh do you breathe that dearie beer zero thank"
3894800,3900320," you so much for um oh yeah already you already said that uh things falling apart thank you so much for"
3900320,3904080," the year for twitch prime and stuff like that so you know what i'm gonna do i'm gonna actually select"
3904800,3911120," uh this thing uh this thing and this is going to be an indicator that like where i last acknowledged"
3911120,3919920," people five minutes who needs to ask twitch to implement features when you can use built-in features"
3919920,3925440," of the browser you can just mark okay so this is the last person i acknowledge so we're gonna have more"
3925440,3932240," uh and you know what's funny probably in before uh adding new element to that list will actually clean up"
3932240,3940080," these selections maybe not we'll see you you have to subscribe subscribe so we can see if it actually"
3940080,3946000," removes the selection okay anyway debate"
3946000,3957680," uh so we need to implement the tokenizer we need to implement the thing that given a string will actually"
3957680,3964240," uh you know split uh you know split all of that into tokens but we're gonna get into that before a small"
3964240,3971200," break because i need to make a cup of tea um okay so uh let's write a tokenizer right so literally the"
3971200,3978160," tokenizer that i usually write when i need to tokenize some shit in a programming language right so uh let's go"
3978160,3986000," we're gonna have a structure alexa right and what i like to keep an alexa is the content of the thing that i'm currently"
3986000,3992560," parsing right so maybe it's gonna be uh something like a string uh and then maybe"
3992560,4001680," i'm going to keep uh some sort of a cursor but here's an interesting thing right so i don't really want"
4001680,4007760," the lexa to own the content that is currently parsing right because it's not going to really modify that"
4007760,4015840," content right so it would be totally fine if it just like borrowed it right um so but because i'm storing"
4015840,4021360," the reference inside with structure that means i need to explicitly tell the lifetime of this thing"
4021360,4028800," which i usually don't like to do but let's just go for it right so let's just go for it and see how it goes"
4029360,4037520," uh right so on top of that i kind of want to also support the unicode right so maybe because of that"
4037520,4040800," um i'm going to actually parse uh"
4040800,4049360," characters instead of a string right so specific characters that may require to when we read the"
4049360,4055200," document and we uh sort of parse everything like converting it into characters uh and maybe that's"
4055200,4061840," fine and since this is a slice right so this is a slice slice uh stores two things it stores the"
4061840,4067440," point at the beginning of the memory where data you know starts and the size of the like the amount"
4067440,4072080," of elements that you have in the slice because of that maybe the cursor itself is not really that"
4072080,4077040," important because it's already kind of contained within the content so essentially if we consume a"
4077040,4083840," bunch of characters we can just like reslice the content and move on right so the content itself"
4085200,4092000," you know what i mean right so uh yeah let's go let's implement the rest of the stuff so this is"
4092000,4098800," going to be lexa let's implement the constructor for this entire stuff let's implement the constructor"
4098800,4104720," uh all right so defense thank you so much i'll really acknowledge you uh and brooklyn dev"
4104720,4108080," thank you so much for twitch prime thank you thank you thank you i really appreciate it"
4108080,4114640," uh all righty so how are we going to be constructing all that i suppose we're going to be just accepting the"
4116160,4123440," uh the thing right i'm going to accepting the slice and uh we're going to return self i suppose i think"
4123440,4131280," this is going to be just self and this is just self uh content actually so this is going to be just content"
4131280,4137440," all right so what we're going to do uh i think i'm going to take the content i'm going to take the"
4137440,4145520," content that we have in here so this is the content and let's convert this entire thing to a character"
4145520,4150720," so i think this is how we're going to be doing characters and then uh collect all of that into"
4150720,4157600," into a vector right so this is going to be a vector so we store this entire thing into a vector"
4157600,4162400," so content is going to be the thing that owns uh the character that we read from the file"
4162400,4168480," file right so it owns that and then we're going to create the lexa we're going to create the lexa"
4168480,4173920," that borrows that thing right so that borrows that thing and there you go we got a lexa"
4173920,4181040," and uh let's actually try to print uh the lexa just to see what's going on but we can't really uh"
4181040,4189120," print that right so it has to be a debug so i just want to see what's going on in here so this is the"
4189120,4199280," debug uh and let's go okay so what do we have in here so we have to put comma in here let's go and"
4199280,4205520," yeah so here is the lexa so this is the output so this is the entire content and as you can see these"
4205520,4210240," are the care it's so slow because of emacs but here are the characters we can clearly see that so"
4210800,4218480," yeah it works cool so the next thing we want to do i want to have um a function that takes the next"
4218480,4225360," token right so next token uh we're going to take the mutable self right because as we consume the"
4225360,4231440," tokens we're going to be re-slicing the content and reassigning it back to uh to this field so that"
4231440,4236480," means we have to be like mutable so in here what we're going to return we can literally just return"
4236480,4242800," that slice right so essentially lexa is just a re-slicer that's why it doesn't really own this"
4242800,4247600," character because what it does it takes your slice that you own just looks at it and gives you like"
4247600,4252960," sub slices it doesn't need to own anything so that's why it doesn't need to move inside of it you it can"
4252960,4259920," just like look look from the distance at the data that you give it if that makes any sense right um all"
4259920,4269760," right so and essentially how we i usually develop the um the lexos i trim the white spaces uh white"
4269760,4276080," spaces from the left right so trim the white spaces from the left how we're going to be trimming the white"
4276080,4286000," white spaces from the left well uh so while the content is greater than zero that means we have"
4286000,4296720," something um in the content and if that something is a white space that means we need to get rid of that"
4296720,4302960," character so we keep re get rid of the characters until we uh have no more white spaces right so essentially"
4302960,4307760," what we can do we can take the content right it's a slice we can re-slice it we can say okay take from"
4307760,4314000," the one up until the end so that effectively kind of remove the first character uh right so and then"
4314000,4319760," you basically reassign it back to the to the content as far as now you also have to take the i don't"
4319760,4324880," remember i think the compiler will tell me right so and this is what we're doing here right so this is"
4324880,4330960," how we can trim all of the uh white spaces from the left uh while we while we have something to"
4330960,4337120," trim we're going to be doing that uh all right so we'll get some errors and uh yeah yeah so the type"
4337120,4344480," is character you have to actually take the the pointer to this thing and there we go yeah um so let's"
4344480,4352640," maybe put that to a separate function um trim left in the mutable self and every time you need to trim"
4352640,4360400," the white spaces you just call uh this thing trim left and here's an interesting thing right if after"
4360400,4369440," trimming all of the white spaces um you end up with a thing that is empty well you ran out of the tokens"
4369440,4376960," right so there's nothing to tokenize you may want to return like an empty token or maybe to indicate that"
4376960,4386000," we reach the end we can return option in here uh right and simply return none in here and that will"
4386000,4393200," allow us to turn the lexor into iterator right uh because uh so this is a tool chain"
4393200,4399600," so iterate if you take a look at the iterator interface i'm called them interfaces so they're"
4399600,4405120," usually called traits but this is actually an interface right an iterator is something that has a method next"
4405120,4412320," and next returns optional item if we say that the lexor is an item of slices of characters so we can"
4412320,4419920," actually uh adapt it to the iterator interface right and treat it as an iterator uh so let me actually"
4419920,4426800," try to run this into i think as you can see it compiles and we can say okay implement uh iterator"
4426800,4434240," for lexor right so the compiler will tell us what we have to implement in here uh right so it tells"
4434240,4439840," that i need to specify the item what's going to be the item uh so i think this called uh this is called"
4439840,4445840," associated type right so we're going to say that the associated type in here is the slice right and then"
4445840,4455680," we need to implement the next uh so it accepts uh mutable self and it should return option uh of self"
4455680,4460880," item right so this is how we're going to be doing that and i'm going to just basically proxy that call to"
4460880,4467680," the uh to the uh to the next token right so so i'm going to proxy i'm going to be proxing it there"
4467680,4474560," and that's it so that essentially means that i can treat the lexor as an iterator so that means"
4474560,4479520," i can do something like token in and then i can just print the tokens like so"
4481600,4489280," yeah cool cool i think it's pretty freaking cool mate so uh yeah it's an iterator i think i already"
4489280,4495120," done that before yeah so this is not the first uh lexa that implemented in rust so this is like a very"
4495120,4502080," natural thing to do in rust to turn lexa into an iterator so yeah let's try to compile this and as"
4502080,4509760," as you can see we don't really have anything right we don't really have anything uh so which means"
4509760,4520640," that we probably need to implement some top uh top tier commentary from coding right uh okay so i"
4520640,4527760," suppose the first thing we want to do we want to start uh lexing the words right so and the word starts"
4527760,4535920," with uh alpha right alpha numeric maybe uh it depends on whether we want to treat numbers and words"
4535920,4540640," separately i think we want to treat them kind of separately i think it makes sense yeah for for the"
4540640,4546320," for the purpose of searching does make sense i'm going to tokenize words and numbers similarly to"
4546320,4550400," how we do that in programming languages meaning that if you have"
4551440,4559920," thousandth this is two separate tokens but if you have uh your mom"
4559920,4569120," 69 for 20 this is a single token like so right uh this is how we do that in programming languages but"
4569120,4571040," maybe it's not a particularly great idea"
4571040,4580400," we'll see so you can always change this entire thing but i'm gonna do it like that for now so if"
4581440,4586240," the content is self content so we know the content has at least something that means we can do"
4586240,4598160," something like this if content is um i don't remember alpha alphabetic okay uh is alphabetic right"
4598160,4606960," so what we need to do we need to keep consuming until it's alphabetic right so let's actually have some"
4606960,4618480," some sort of like index right while n is less than self content length and self content n uh by the way i"
4618480,4626800," think i forgot to put something behind my camera behind my camera"
4629600,4640160," while this thing is alpha numeric right we keep incrementing this thing right and now uh as soon"
4640160,4646320," as we reach the end of the content or the end of alphanumeric things uh we need to chop off that"
4646320,4653520," token so we need to take content starting from zero up until n right so n is usually not included"
4654320,4660560," so this is the token that we need to return uh but before we can return that we need to also remove it"
4660560,4666800," from uh from the from the lexer so what we have to do we have to reslice it so we need to do content"
4666800,4674240," uh starting from n up until the end right so and that's the new content that we're reassigning here"
4674240,4681760," as we're reassigning here and then we just return some uh some results right we might as well actually call it"
4681760,4688080," um token um token so this is the token cool"
4688080,4699040," all right so what we have so let me take a look at the errors um so the token oh yeah so we need to"
4699040,4704960," take the reference in here so what's the rest of the stuff this thing has to be mutable of course"
4705760,4715360," because it's rust okay so the first uh thing we got is this name so name gl it's kind of difficult to"
4715360,4723600," read so let me actually change it so as far as i know um so this is the sequence of characters so that"
4723600,4732400," means i can turn it into iterator then collect it into a string right so that's what i can do i can collect it"
4732400,4737680," into a string uh uh never used"
4737680,4747040," yeah there we go so name gl if really is that really what we've got"
4747040,4758400," yeah i see so this is because we when we read xml when we read xml we actually read the chunks of text and we"
4758400,4767280," append them without any uh separation so that means some words from one xml chunk can get blending with"
4767280,4772880," other words from another chunk and that may result with these weird tokens so this is very dangerous"
4772880,4779120," actually this is very dangerous so what we have to do when we are reading from the xml we have to pad"
4779120,4786480," uh this stuff a little bit right so essentially we're going to do content and after that we can push uh"
4786480,4791600," just the space because the space is going to be removed by the tokenizer anyway so we can just like"
4791600,4796000," put it like that i think that's fine so because of that the first token is going to be name yeah as"
4796000,4802720," you can see here is the name and these are two separate tokens which is fine so that's pretty cool um okay"
4804080,4812960," so uh now um we don't really know why it failed it failed because it reached the to do right it"
4812960,4820320," reached this place uh right so let's actually do the following thing invalid token starts with"
4820320,4826960," right and we can do the following thing start equal to self content zero so"
4826960,4835920," we'll know what we're dealing with so it starts with that's pretty weird so how can we take into"
4835920,4844480," account this kind of stuff i think i know if we encounter like weird symbols and maybe we're"
4844480,4850720," going to treat it as a separate token right so here we're going to do the following thing okay the token"
4852240,4861600," is basically this one right then we slice this entire thing from one up until here"
4861600,4866000," and then we just return this entire token so at the end here we don't have to return anything"
4866000,4871600," and that kind of takes into account all of these tokens right so like weird things uh let's see how"
4871600,4877440," bad is it gonna go yeah so this is actually pretty good except for the numbers right the numbers are not"
4877440,4885760," tokenized properly uh and let's try to tokenize them so this is going to be content if this is a numeric"
4885760,4890800," if this is alphabetic so this must be numeric uh we're going to be doing the following thing"
4890800,4894640," uh while it is numeric"
4894640,4906240," uh we're going to be doing like that and oh numbers didn't really work but i suppose those were like a really"
4906240,4915040," separate numbers then um can we have like numbers that have more characters while it is numeric it is"
4915040,4927440," numeric i don't know i guess it's fine so that's cool uh so and again oh here we go so i was looking"
4927440,4937200," for number like like this uh like for uh for stuff uh uh to to do two so yeah and what's interesting is"
4937200,4944480," that you can see like a garbage like this but if this garbage is going to be very frequent in all of"
4944480,4950240," the documents the tf-idf uh formula is going to basically filter that off so we don't really have to"
4950240,4956240," worry about like having this sort of random garbage in in the things right because it's not going to really"
4956240,4963280," affect too much um and if anything maybe in the future we can simply have some sort of like a list"
4963280,4970000," of stop words just like ignore these kind of tokens and whatnot uh for that like this is more than enough"
4970000,4977200," to tokenize things maybe another thing i would like to do is to uh bring all of those things to uppercase"
4977200,4982400," if you know what i mean uh all right so map um so x"
4982400,4992080," uppercase is that what it's called i think uppercase two uppercase let me take a look"
4992080,5000640," uh oh yeah i remember there's something weird about two uppercase it like removes returns an iterator"
5001680,5008720," uh returns an iterator that yields the uppercase mapping of characters as one or more characters so"
5008720,5021200," it actually can have more characters i think there was a way to have like a more classical uh to ascii"
5021200,5028640," uppercase yeah i vaguely remember this kind of stuff some things i remember in rust some things i don't"
5028640,5036960," just like kind of interesting and this one returns the character okay so to asking uppercase and uh"
5036960,5042160," yeah so this is one of the things that i may want to do right and because of that like words with the"
5042160,5047600," different case are going to be considered the same tokens uh and maybe it's going to be like easier to"
5047600,5054080," handle we will see right but if we're going to be doing uppercase we probably have to do that on a"
5054080,5062320," level of a tokenizer right because the tokenizer yields those things and that means that we probably"
5062320,5070320," need to return a string in here but yeah we'll see we'll see we'll see so i have the idea so we can"
5070320,5076560," compress a couple of things in here as you can see we actually repeat this kind of part uh three times"
5077280,5081840," and this part it's only parameterized by n so it basically tells you how many"
5081840,5091200," characters you want to chop off from the content right um chop off from the content"
5091200,5106960," chop uh and we're gonna accept the uh mutable self right and uh use size and in here we're gonna return the slice"
5107920,5113040," all the character right and we can move this entire thing in here"
5113040,5123600," so but since we always return something we don't have to wrap it in sum and now in all these places"
5123600,5131680," we can just do uh i think return some self chop n right so the first thing we do we compute the n"
5132240,5137280," right we compute the n and then we chop the uh like this amount of characters we can do the same thing"
5137280,5140480," in here some uh self chop n"
5140480,5146240," and here we know that we want to chop a single one so we can always do"
5146800,5151360," uh self chop one"
5151360,5159680," okay so that works another interesting thing is that we definitely have a repetition here"
5159680,5170160," but the problem is that we have uh different predicates in here right so we can try to factor that out as"
5170160,5176960," well but uh calling it something like chop while right so we're going to accept self and here we'll"
5176960,5183200," have to accept the predicates uh p and this thing is going to return the slice but the question is"
5183200,5190320," what how we define p so we can try to define the same way as defined in take while right because there's"
5190320,5195840," like a fan fn once fn mute i don't really know the difference between them so i'm going to just follow"
5195840,5201440," whatever condition in this in take while i know that mute that means it can modify its environment or"
5201440,5206720," something like that but i don't remember implications uh right so you you can we can use a fan mute and it"
5206720,5212000," takes it by a reference do we really want to take it by reference we can take it by reference right so"
5212000,5218160," yeah we'll see okay so this thing is going to be a fan mute it's going to take the character by a"
5218160,5225680," reference and it's going to return a boolean right so and in here we can simply just move"
5225680,5234000," this entire thing uh right and uh yeah return self chop n"
5234000,5243600," so instead of like calling this thing we're going to be calling uh predicate all right predicate"
5243600,5247920," by a reference because it's supposed to accept the reference uh and there we go"
5248160,5255200," so now we we can have a customizable uh predicate so essentially here we take until it's numeric so"
5255200,5263760," that means we can chop while um x is numeric and we can remove this entire thing now right"
5263760,5270720," so that comprises it even more right and we can do the same thing in here so this is alphanumeric so"
5270720,5279440," chop while uh x x is alpha numeric we can remove this entire stuff so the entire logic of tokenizer is"
5279440,5287200," now this and we have two very powerful things that are reusable right so we can chop while uh a character"
5287200,5293360," satisfies certain predicate or we can chop like exact amount of things uh right so and all of that is just"
5293360,5299360," to factor it out right so now it's super convenient if we want to chop like specific prefixes we can"
5299360,5303120," quite easily do that right so we can introduce that functionality but i don't feel like we need that"
5303120,5309120," functionality right now uh so whatever we have here is already enough right it's it's more than enough"
5310800,5317680," okay okay so let's actually go through this entire stuff uh p not found this is because it has to be uh"
5317680,5328160," generic uh predicate uh help consider it has to be mutable so it wants to be mutable in here sure"
5328160,5337280," cool so let's say that this is the uh tokenizer right so this is the tokenizer that we're dealing with"
5337840,5345600," let's try to build the frequency table right so for that specific document um uh so"
5345600,5356480," term frequency let's call it tf right so this is going to be hash map we accept the string right so"
5356480,5361920," since we're uh you know bringing it to uppercase and then we're going to do the size um"
5365920,5367680," so let's create the this thing"
5367680,5376160," and of course this thing has to be mutable so the first thing we we want to do we probably want to tf"
5376160,5381920," uh this is because tf is a troll face emoji yeah"
5381920,5395440," we want to check if the term if the token is already in the um in the table in the tf table right so let's"
5395440,5405360," state this entire stuff and let's call it uh term right this is a term uh tf get mutable because we're"
5405360,5412480," going to be modifying it if it exists we're going to just increment it by one um right so token term"
5414320,5423760," if let some count count uh count plus one otherwise otherwise otherwise we're going to be probably"
5423760,5436080," just inserting um um this one is interesting right yeah i think we're going to be like moving it inside"
5436080,5443440," right i think it's totally fine so since the uh string was created in here and at the end of this loop it's going to be"
5443440,5448800," either destroyed or removed inside of the hash map so that's totally fine so this one is going to be"
5448800,5459200," one uh all right so now let's go ahead and maybe bring those things right so hash map does it have like"
5459200,5466800," something like items or maybe you can just iterate it right so you can do something like uh term uh and"
5466800,5477280," frequency all right so it's a frequency right so it's a term frequency this is a term frequency uh in uh tf"
5477280,5485120," what's actually going to call t f in tf right so we're iterating this entire thing and then we can say okay"
5485920,5496400," so this is how this specific term how often it um you know occurs in the document specifically okay so"
5496400,5505360," let me see so frequency binary assignment is mutable i have to dereference it first um maybe i don't need that"
5505360,5519840," stuff anymore exit is not cache uh semicolon what else okay so these are the terms this is how frequently"
5519840,5525760," they basically occur but it would be nice to actually kind of sort them by how frequent they"
5525760,5533040," are if you know what i mean right so it would be kind of nice to do that we can try to basically"
5533040,5543040," collect all of that into a vector right and sort that vector um you know by by the frequency right"
5543040,5549840," by the frequency but here we're iterating by reference right so we're not really moving out"
5549840,5557840," the elements um from the hash table right so we can actually kind of do that i think i didn't think"
5557840,5563840," it's uh it's that bad right so let's call it stats and there we go here we have a vector of stats"
5563840,5574000," then we can just sort it by uh sort by oh you can sort by key that's actually pretty cool right so how"
5574000,5579680," do you provide the key uh you provide through the function oh basically you take the element and"
5579680,5584960," you extract a key out of the element telling but by by what you have to compare things okay that's"
5584960,5592720," cool uh sort by uh so we accept in here and we have a term and frequency uh we don't care about the"
5592720,5599040," term we only care about the frequency and uh since it's a reference i just do it like that uh right so"
5599040,5607200," and then after we sorted all of that i probably want to reverse this entire thing uh can i reverse"
5607200,5613680," okay so you can reverse the slice and if the slice i suppose it has to be mutable uh yeah okay so that"
5613680,5622960," means i can do stats uh reverse cool and now we can do something interesting i suppose we can take the"
5622960,5629520," stats but we can also take like top 10. let's take top 10 right so we're not interested in all of the"
5630320,5636720," uh in all of the tokens we're only interested in top 10. uh so sort by takes two arguments um"
5636720,5640160," sort by so by the key"
5640160,5653920," uh this has to be mutable sure i don't mind that okay look at that these are top 10 tokens within the"
5653920,5656240," document isn't that cool"
5656240,5666160," so these are top 10 which actually kind of makes sense with dots it does make sense right so dots are"
5666160,5666880," very frequent"
5672400,5681920," and also um vertex is also very frequent um so let's actually take 100. so a lot of words like"
5681920,5690640," happen there like uniquely must errors so they happen like one time you know what i want to do i actually"
5690640,5696880," want to iterate through all of the documents within the directory and do this process on them right so"
5696880,5706960," so essentially i can do uh println uh and say file path and it can pad this thing with like uh four spaces"
5706960,5714160," so we can just separate the name from the from the from the filing uh file path is not called file path"
5714720,5720320," uh okay so i should have actually like this so this is the file path"
5720320,5730320," okay so as you can see here is the file path and here is the top 10 tokens"
5730320,5738320," we can now put that in a loop right so here uh we have a directory right we have a directory we read the"
5738320,5746720," directory and here we have a file path file path right so we can take this entire stuff and move it inside"
5746720,5752880," of that loop right just move it inside of that loop and iterate through all of the files and for each"
5752880,5763360," file compute the compute the stuff right so uh there we go it's gonna work so fs is not available and"
5763360,5773760," this is because i removed it uh yeah file path this one is a little bit uh weird but we can easily work"
5773760,5783280," around that by doing that file path uh so has it move occurs okay so the file path was moved this is"
5783280,5788880," because we want to actually take a pointer in here and there we go so we're going through all the documents"
5788880,5794480," and we're analyzing all of them uh right and we're seeing how frequently certain words occur"
5794480,5801600," uh it's actually kind of cool gl get active uniform block uniform is actually one of the frequently used"
5801600,5809520," uniform and block uniform and block so we pick the random sample parameter texture gl so we know that"
5809520,5814960," this thing is probably related to texture we can already see what these things are related to so this"
5814960,5821600," thing is related to texture this thing is related to frame buffers this thing is related to samplers well"
5821600,5829760," yeah uniforms what's funny is that get active uniform name the most act like frequent thing here is"
5829760,5837360," program but maybe down there there's also uniform and whatnot um so yeah this is cool isn't it cool"
5837920,5842160," it's actually pretty cool buffer sub data of buffer yeah length"
5842160,5847600," damn it's so cool"
5847600,5858400," uh right so for instance we have this thing as like 10 but we can configure that top n right and we can say"
5858400,5862240," okay top n is going to be 20 let's actually do 20 here"
5864640,5871520," um so now we can see more terms uh we can see more terms"
5871520,5889760," that's pretty cool so now once we have this frequency so maybe we could actually do type uh tf"
5890560,5902400," hash map uh string to use size right so this is a tf uh tf and we can simply just replace this thing with tf"
5902400,5911600," um because now i want to have the document index right so"
5917680,5926080," tf i don't know how to call that right so i want to have a name from a for a thing that maps a path"
5926080,5935200," right certain path uh maybe even path buffer to tf for that specific document right so if this"
5935200,5945440," is the term frequency let's call term uh term freak uh term freak so a mapping from a document to a term"
5945440,5948000," frequency for that document how would you call it that thing"
5948000,5958080," i have no idea that's kind of difficult um index"
5962560,5965200," maybe index uh maybe index uh global index"
5965200,5971360," term freak index yeah boy"
5971360,5978880," term frame index okay term freak"
5983280,5996800," right and in here we can have uh tf index right and essentially now as we iterate through all of the"
5996800,6005280," files right as we iterate through all of the files we can add uh this thing to the global index right so we"
6005280,6016960," can do it uh tf index insert uh file path and file path as far as i know is a path buffer so um we can"
6016960,6024480," just move it inside of the tf index right we move it in inside and we just move tf inside of it as well"
6024480,6032960," so we're not going to print anything right we're going to just try to build the entire thing and we can try to"
6032960,6043520," iterate through all that stuff um okay so um path path tf in tf index"
6044960,6060400," uh we can do println um has count unique tokens that's a pretty good thing and we can do tf"
6060400,6066480," length so the amount of uh entries with the term frequency is the amount of unique tokens you have in"
6066480,6072960," this thing and uh those we're going to demonstrate that we managed to parse all of the documents um what's"
6072960,6079600," funny is that it's going to take some time we already um demonstrated that it's rather slow so maybe"
6079600,6087760," one of the things we have to do in here uh is print the progress right indexing"
6090640,6101520," file path"
6101520,6107440," so it also would be nice to put triple dot in here uh okay"
6110480,6121120," uh tf len count what else do we have we have a path buffer uh path buffer is located to where std path"
6121120,6130320," path buffer so the difference between path and path buffer is similar to difference between str and string"
6130320,6139440," if i'm not mistaken right so path buffer owns the data that represents the path and path is just a like a view on"
6139440,6147120," right because of that data right because of that uh t uh tf index in this specific thing uh we own it"
6147120,6156080," right because it can be like self-containing there um right so let me see uh tf index is not mutable but"
6156080,6164320," it probably has to be mutable um okay so and it's taking some time to index all that right so we're indexing things"
6164880,6166880," we're indexing things"
6166880,6170880," it's pretty cool"
6170880,6173920," all right"
6173920,6183840," and it didn't print okay so at the end after it finished indexing it also printed"
6183840,6189440," the amount of unique tokens within each individual thing so this is where it finished indexing and this is where we"
6189840,6193040," set okay so this is basically the state of this entire site"
6193040,6200880," um it would be kind of cool to just dump the hash map somewhere it would be kind of cool to just dump it"
6200880,6204000," somewhere maybe in the form of a json or whatnot"
6204000,6209680," because reparsing all of that is kind of a pain in the ass"
6210800,6214240," every time it would be nice to just dump it uh right"
6214240,6222640," it would be nice to just dump it uh right so let me see i have no idea i didn't think about that uh"
6222640,6230880," okay rust uh dump hash map as json i swear to god if it's gonna suggest me to assume yeah"
6230880,6233040," okay"
6233040,6242960," ton of dependencies on top of dependencies on top of dependencies framework of framework"
6242960,6246400," serializer of serializer of framework of serializer of frameworks"
6246400,6251120," pain literally pain"
6251120,6260400," i freaking know that the restitions are gonna debate me into that madness i knew that they're gonna do that"
6260400,6267280," i knew that they're gonna do that i'm not gonna playing not gonna be playing by their games"
6267280,6272800," fuck that"
6272800,6281280," but zozin don't you want one terabyte dependency folder"
6281280,6285680," it's kind of tempting not gonna lie it's kind of tempting"
6285680,6290320," anyway so maybe we can store that shite in um"
6290320,6293360," i don't know in the custom format or something"
6297280,6301440," um so um rust"
6301440,6305120," generate json"
6305120,6313360," though i have an idea okay so here is the json right so sergey json whatever the fuck it is"
6313360,6314640," let's take a look at the dependencies"
6314640,6319040," so development dependencies"
6324400,6332000," row values so it is like the lower level stuff right so it is the low level stuff"
6332000,6338320," oh yeah but i mean it's it's depth dependencies but the actual dependencies here are"
6338320,6342240," uh on on this thing which is okay"
6351920,6355200," all right let's try the search json i think this is gonna be"
6355200,6359920," imagine having a separate package for i to a for a function that converts"
6359920,6364880," integer to a string or whatever the it is i assume that this is from from c"
6364880,6370240," let's see i'm gonna i'm gonna regret that"
6371840,6373440," of course you need to update the crates here"
6373440,6381840," okay so we added that so so apparently i already had that uh somewhere"
6381840,6384000," okay let me remove that"
6384000,6387440," and let's try to build this thing"
6387440,6394720," oh yeah so it's it's not downloading until you try to build that"
6394720,6412560," okay so it wasn't that slow so maybe i'm gonna i'm gonna be okay with that uh so let's open the"
6412560,6418160," documentation so i want to build documentation for 30 and 30 do i even pronounce that correctly"
6418800,6423280," do i don't do i even care if i pronounce that correctly i don't okay uh"
6423280,6427600," so checking server json"
6429520,6431120," imagine building documentation"
6431120,6449760," json json json json okay can i do something like two string two st two string okay that's cool"
6451360,6458320," uh but if i want to build this stuff out of the what if i want to see okay so i can try to serialize that"
6458320,6462400," serializing a structure um"
6462400,6469920," hash map any type that implements third the serialize a trade can be serialized this way"
6469920,6473280," maybe"
6476480,6480560," so that means i can just like use this stuff right"
6480560,6492320," i can just put this stuff in here right then uh i can simply take this so this is also result"
6492320,6499120," tf index"
6502240,6511280," to string but then i want to save that short um does it have something like save directly to a file"
6511280,6514240," save directly to a file"
6514240,6521520," so let's take a look at the list of available thing"
6521520,6526400," uh 30 json"
6526400,6528160," to"
6532240,6541360," right"
6541360,6541680," aha"
6541680,6546560," you provide the writer"
6546560,6549520," and you provide the civilizable value"
6549520,6557520," so that means we can do the following thing we can open the file we can create"
6559840,6565200," um let's call it something like index path right so this is the index path uh index"
6565200,6577680," index json uh grant lm saving uh maybe yeah saving index path"
6577680,6585120," okay so we create the file of course we create the file"
6586880,6587600," index file"
6587600,6593840," and then we want to do serd json uh"
6593840,6596320," tool writer"
6596320,6599920," index file"
6599920,6606320," so when the writer is just like this so i suppose it's you want to move it inside yeah i guess that"
6606320,6610000," makes sense you just move it inside and then we provide the tf index"
6610560,6615600," right and this thing returns the result but the result is what a result is"
6615600,6625280," uh this thing okay expect uh serd works fine because uh the main returns a different kind of"
6625280,6630320," error i'm not in the mood of uh actually adapting errors around and stuff like that so i can just do it"
6630320,6635040," like this maybe you can just like a map error and whatnot and the actual error could be a string"
6635760,6644080," uh i couldn't do that sorry uh so use of undeclared crate but do i really need that is it really that"
6644080,6650960," important anything okay so it was not important so okay that's cool oh i mean i i built the documentation"
6650960,6658400," instead of the program so maybe it is important okay so it's not failing or anything let's try to run"
6658400,6666480," so okay so we indexing thing we're indexing thing uh and then we're going to be saving the json hopefully"
6666480,6674720," um so and definitely every time i want to do search i don't want to do this process over and over again"
6674720,6680880," that's for sure that's why i need to be able to uh to save that to the json and we've got a json which is"
6682720,6689440," i mean 640 kilobytes should be enough for everyone but we can't even fit index into that so let's open"
6689440,6697280," the index json i can't open it in emacs emacs actually dies because okay there we go so can you see this"
6697280,6705200," but this is the index uh built out of the out of this stuff which is kind of cool isn't i think it's"
6705200,6710720," pretty freaking cool bitrate is yeah bitrate is actually pretty bad but one of the things we can do"
6710720,6718240," actually we can uh do jq index json index pretty json"
6718240,6725440," uh i don't remember do can you just do cut"
6725440,6733840," yeah what did it say it says something some i forgot how to use jq"
6736080,6738640," um"
6738640,6746400," i suppose you just do that uh jq pretty print i don't remember"
6746400,6752720," uh how do you how do you do that pretty okay pretty print"
6752720,6761280," dot oh yeah you you cut it and what i mean it's not an answer"
6762080,6768400," yeah jesus christ imagine the interface so unintuitive that you have to google this kind of stuff"
6768400,6774000," uh okay so now i can do index pretty json"
6774000,6779520," and it's one megabyte but i mean it's worth it there we go so here's the index"
6779520,6786800," so this is for the first one this is for the another one uh and so on and so forth okay that's pretty cool"
6789120,6799520," uh so we have 63 000 almost 64 000 lines of code okay so that's the index uh now we need to be able"
6799520,6804960," to read this entire right so because we want to you know take the index and just do something with it"
6804960,6813360," right so let me see how we're gonna be doing all that so i'm gonna command out sort of command out the"
6813360,6818160," entry point right so this is gonna be a fun name um results"
6818160,6830160," um so the question is insert do we have from reader yeah we do have from reader so this is sort"
6830160,6841040," of like symmetrical thing uh so that means i can do a left file path let's call it index file path which is"
6841040,6846400," index json right uh open uh index path"
6846400,6859280," index file like so um and when i do from it literally returns the result in here okay so that's fine"
6860160,6872960," uh search json from uh and i have to specify the deserialized owned i suppose i have to like say"
6872960,6880640," the actual type so tf index i have to say that it's a uh term freck index right so that's what i have to"
6880640,6885680," say and here i'm going to wrap in so hoping that the compiler is going to infer the type for this thing"
6885680,6896560," correctly and also i'm moving the reader there so uh index file so and after that after that"
6896560,6903840," um i'm going to be doing the following thing i want to actually do something like reading um"
6906720,6912400," index path index file right because maybe it's going to take some time we'll see how fast it is"
6912400,6919600," and then let's just iterate all right so um maybe not we can print something like very short"
6919600,6932080," uh index path contains account files right so an account is basically tf index length"
6932640,6938960," have a go so in here we'll probably want to uh to expect because it's a different kind of error um what"
6938960,6949920," do we want to um serve does not fail right okay so let's do the card build just to ensure that everything's"
6949920,6961440," fine um to do so at the end we have to say okay and it builds believe it or not so now if we do run"
6962640,6969760," the card build um no file or directory for what precisely oh this is because i'm not in the right"
6969760,6979440," place okay so that's faster than what we had look at that that's that's faster than re-indexing everything"
6979440,6984480," it's it's still kind of slow and maybe this is because of the uh not like debug build so let's"
6984480,6986880," actually do this one oh it's gonna rebuild everything for this one"
6986880,6997440," so uh beanlab thank you so much for uh the twitch prime thank you thank you thank you with the message"
6997440,7007360," hello hello to youtube building a2a i2a that's a very big framework that you need to build"
7008240,7014720," uh okay so now if i just try it around still kind of slow but anyway um"
7014720,7019360," so yeah that's pretty cool"
7019360,7025760," uh so we already have all of the like building blocks to build some sort of a system"
7025760,7035280," so the next the actual thing that we need to do in here is to you know get the prompt from the user"
7035280,7043600," get the prompt from the user um and tokenize it and then compute the uh ranking for all the documents"
7043600,7049280," so here's an idea so how are we going to do the search how we're going to do the search uh we take"
7049280,7054800," the prompt from the user we tokenize it the same way we tokenize all the documents right it's quite"
7054800,7061600," important to have the same tokens and stuff like that then for um each token right for each document"
7062240,7068960," we compute the tfidf of that token then tfidf of another token and another another and we have a"
7068960,7074800," bunch of tfidfs for the prompt for that specific document we sum them up and this is the ranking of"
7074800,7080160," that document for that specific prompt we repeat that for each individual document and we have rankings"
7080160,7085600," for each individual document for that specific prompt that you provided and then we just sort the"
7085600,7092960," documents uh according to that ranking and we give it like top 10 top n right uh and then you pick the"
7092960,7096960," the document that you're interested in so that's that's the idea of how we're going to be querying"
7096960,7103600," all that stuff right so it would be kind of nice to maybe have some sort of sub commands in here right so"
7103600,7110400," because you need to be able to do two things right you want to be able to index a folder right you want to"
7110400,7115600," index the folder and then once you created the index file you want to be able to given the index file"
7115600,7122800," search with a certain prompt so you have two sub commands like index and search uh right so and this"
7122800,7129680," is probably something that i want to do but unfortunately unfortunately i'm registering for two hours"
7130640,7137200," so i don't really want to go over two hours so i think i'm gonna stop the development here and we're"
7137200,7145280," gonna continue doing this thing next time uh all right does anyone have any questions before i go"
7145280,7152400," yeah i'm sorry like if i go over two hours i get really burned out and uh it's not gonna go well"
7152400,7158320," right so one of the things i'm gonna i can give you the source code okay so uh right before i go i can"
7158320,7167680," just give you the source code and uh yes yes yes i'm not gonna include docs.jl i'm not gonna include"
7167680,7174400," docs.jl you're supposed to uh you know clone it yourself uh right so this is that and no no"
7174400,7185760," ready set go do we have a redmi degenerate with you uh right so how did i call the project i forgot how"
7185760,7196320," i'm serious i i called the project serious okay um"
7196320,7209120," serious so the idea is that uh this tool should work universally on any folder with xml's"
7209760,7219120," that's kind of the idea right so um and maybe in the future right as i already said we're going to"
7219120,7225840," support several uh you know several formats and we also need to implement so it actually traverses"
7225840,7237440," recursively so that that would be cool okay search uh search engine uh local search engine uh"
7239120,7247760," yeah local search engine that's that's how i'm gonna call it uh nothing much to say so what's the"
7247760,7255440," license uh license uh license it's gonna be on the mit and the mit"
7259600,7272160," um um add origin uh i also wanted to include the license so let me quickly amend that so now"
7272160,7276480," license is available and i'm going to push that right into the red one"
7285120,7292480," so let's refresh so the source code for this thing must be available in here i'm going to copy paste it"
7292480,7298640," in the chat and for people who may be watching on youtube it's going to be available in here"
7298640,7306160," source code yeah this is not so good this is this it's not even source code uh all right"
7307280,7313360," there's no power so maybe off screen i'm going to clean it up a little bit so maybe introduce some"
7313360,7319760," sub commands and whatnot uh and then on the next stream we're going to try to actually implement the"
7319760,7322560," search um"
7328240,7333680," so mr tuding i know it's early in development to tell but do you plan to carry this project on"
7333680,7338480," uh i'll see i'll try to use it locally and see if it has a potential for me"
7338480,7341520," uh and if it does i'm gonna carry on with that"
7342400,7347200," uh so yeah i'm i don't know yet i don't know it let's see we'll see"
7347200,7356800," all righty all righty uh i slow thank you so much for for the sub with the message thanks"
7356800,7366080," zozy for not being irresponsible and writing c in 2023 yeah um all right so that's it for today"
7366080,7371760," boys and girls thanks everyone who's watching me right now really appreciate that i have a good one"
7371760,7377360," and i see you all on the next azuzin a session"
