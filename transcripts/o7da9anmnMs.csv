start,end,text
400,5760," looks like we're live hello everyone and welcome to yet another azu is in session"
5760,12160," azu is in session let's make a little bit of an announcement and officially start at this stream"
12160,19440," hello hello welcome welcome welcome how are you guys doing so uh let's do the usual uh red circle"
19440,28000," live on twitch and what are we doing today on twitch.tv website today we are implementing"
28640,35040," back propagation propagation uh in c that's what we're doing today i'm going to give the link to"
35040,40240," twitch.tv/totting the place where we're doing all that twitch.tv/totting and i'm going to ping"
40240,45600," everyone who's interested in being pinged there we go the stream has officially started the stream"
45600,53040," has officially started so so for the past several streams we were developing uh an nh which is a"
53040,58480," simple header-only library for neural networks and we're doing that purely for educational purposes"
58640,63760," right so we're trying to explore the paradigm of machine learning and the paradigm of machine"
63760,69680," learning is essentially paradigm where you're developing a model that models a certain process"
69680,76000," right something and you're not coding the model directly you're not developing the model directly"
76000,82960," you are essentially describing how the model has to behave and the training process or the learning"
82960,89520," process uh is supposed to configure the model so it behaves as close to your description as possible"
89520,94400," so that's basically the paradigm we are working with it's a very new paradigm for me i never worked with"
94400,98800," that paradigm before but it's extremely fascinating and that's what we're doing today so and for"
98800,104080," educational purposes i'm doing everything from scratch i'm using c not touching any of the you know"
104080,109360," tensorflow or pi torch or whatever the fuck they are until i understand how this entire thing works"
110080,115920," right and that's essentially what we're doing today so we're gonna give the link to the nn.h uh all"
115920,121040," right to the chat right so let me quickly copy paste this entire thing so it's in the chat and for"
121040,126720," people who's watching on youtube it's going to be in the description as usual so uh let's take a look"
126720,133600," at uh what we have in here so we already implemented sort of like a hello world of uh neural networks we"
133600,139520," implemented xor neural network it's a very simple neural network that consists of like two input"
141040,148880," two input neurons two hidden layer neurons and one output neuron and it's supposed to act like a xor gate"
148880,153760," right so i i can even draw how it's supposed to look like so i think it's it's a little bit better to"
153760,160960," understand if you just visually draw this entire thing so the entire neural networks network looks"
160960,170240," roughly like this uh so you have two input neurons uh right then you have two hidden neurons and one"
170240,174400," output neuron and since it's a fully connected uh neural network that means all of these things are"
174400,181600," connected uh everything is connected right and essentially this is sort of like the first bit that"
181600,187360," you put in here right and this is the second bit and this is the output bit and it's supposed to act"
187360,195120," like a xor gate right so the way we uh you know sort of train this entire neural network is that we"
195120,202800," initialize all the parameter of the neural network uh with random numbers right so each uh sort of like"
202800,209200," neuron each neuron has its own bias and each connection has its own weight uh and stuff like that"
209200,214640," right and then uh the learning process basically just configures all these weights and biases until"
214640,220400," this entire thing acts like a uh xor gate right so that's basically what it does we can even take a look"
220400,225120," at this entire stuff so we have a little bit of a training data in here so we even have like a training data"
225120,230560," for order right so it's actually easy easy to swap different training data data if you want to train"
230560,235680," different uh different gates and stuff like that so let's actually build this entire thing"
235680,240720," uh right so i'm going to do xor xor.c and i'm also going to link with the math library"
240720,246080," just in case as you can see it built and uh i want to run this entire thing and as you can see"
246080,250080," so what we have in here it's printing the cost function while it's training everything"
250080,256000," so it's doing i don't know how many epochs it's doing it's it's one hundred one hundred thousand"
256000,263760," epochs so it's actually pretty slow in the uh in emacs because emacs is not a very fast terminal right"
263760,270880," it's not a very fast terminal so it's actually open like a real terminal right so uh so we can do that"
270880,278480," a little bit faster uh right unless you just use or uh and it goes through one hundred thousand epochs pretty"
278480,284640," easily right and these are basically final weights and biases of the neural networks of this specific"
284640,291040," neural network and this is its behavior so this is what we're basically validating so zero zero is very"
291040,298800," close to zero right so it's very close to zero zero one is one one zero is one but one one is zero right"
298800,306080," so as you can see it works and again we can actually swap it out we can swap out the data if we want or"
307280,313520," it's going to train or so let's actually just build this entire thing without you know running it because"
313520,321040," we want to run it in a separate terminal so and uh i think i can do a half of the iterations and as you"
321040,327360," can see it acts like or zero zero is zero right so maybe i can even make it a little bigger zero one is"
327360,334880," one one zero is one and one one is one right so and you can sort of model this simple sort of gate so the way we"
335680,346000," uh optimize the um the cost function we're doing a very stupid hack right we're doing a very stupid hack"
346000,353760," uh which uh is called finite differences i'm not sure how common this hack is uh from what i saw on the"
353760,360560," internet and from what i could understand from like talking to chat gpt it is very uncommon but i like this"
360560,368160," trick i absolutely like this trick because it is so simple it is extremely simple so it just for"
368160,374080," educational purposes i think it's extremely like invaluable right because it just allows you not to"
374080,381360," talk about back propagation and just focus on what a gradient descent actually is even though it is very"
381360,388640," slow you know what i mean right sometimes like um so there there's a problem i think we discussed that"
388640,395360," recently is that um one of the cool things in math one of the valuable things in math is its rigor"
395360,404480," math it tends to be very rigorous but at the same time uh rigor is actually quite often bad for education"
405040,410560," right because it makes a lot of things obscure for people who are not familiar with the concepts they"
410560,417120," are trying to learn right so and it's kind of similar with uh i think performance in programming"
417120,424560," right in performance in programming because uh people are trying to like you know force other people to"
424560,428800," write as fast as possible to like write the most performing algorithms"
429440,436400," without understanding like why like where to move i made an analogy in the previous stream is that right"
436400,443280," demanding a person who's learning neural networks to write propagation back propagation from day one"
443280,448160," is like asking them to run as fun as fast as possible without telling them direction"
448160,455120," right essentially you come to them and say you have to run you go to the jump to the car and drive but"
455120,459040," but where do you drive like i don't know it doesn't matter just drive as fast as possible but but you"
459040,465520," don't know the direction right so that's the analogy i uh i like to make so back propagation is rather"
465520,471280," confusing right for people who are never working with this kind of stuff and uh finite differences is"
471280,476240," actually super easy to understand right for educational purpose but it's slow right it allows us to figure"
476240,482000," out the direction in which we can move and uh once we figure out the direction we can try to move"
482000,488800," faster right so let's actually uh do a recap of uh how uh finite differences actually works"
488800,496880," so essentially we have a cost function right and the cost function is effectively a function of all of"
496880,503120," the parameters of your uh of your model right or neural network in that specific case so here i'm going"
503120,510400," to denote uh for all of the parameters as w right so but usually you have like thousands and maybe millions"
510400,514880," and billions of parameters right in your neural networks in this specific case how many parameters"
514880,523680," do we have we have at least six uh and then uh four so ten uh we have 12 parameters i think i think that's"
523680,531280," how many parameters we have in here so if i'm mistaken yeah so this is it's actually less i think it's oh it's"
531280,538160," nine yeah i'm gonna get i'm sorry yeah i think it's nine anyway so essentially and the cost function"
538160,544960," essentially tells you tells you how well your model performs right so uh the higher it is the higher it"
544960,551440," is the worse model performs so essentially what you want to do you want to drive your cost function uh"
551440,558320," towards zero right so the smaller it is the better for you this is how you optimize right so and essentially"
558320,563680," what you want to do is you want to find the local minimum that function what that means let's imagine"
563680,571280," that our cost function right so if we plot our cost function right so the x is going to be the parameter"
571280,577520," and y is going to be the actual value of the cost function right so and it's something like this"
577520,583760," right we initialize our model uh with a random parameter so in that random parameter is somewhere here"
583760,590320," right so something like this uh right so this is the initial value and essentially you want to keep"
590320,597120," modifying your parameter uh to drive it to towards here right towards the local minimum and that's going"
597120,603600," to be sort of your trained model right so initially it's just basically w0 right and this is like w"
603600,610160," and after like n iterations you actually figured out that you have to move here so the question is uh how do you"
610160,617360," figure out where to move at all like how do you even do that so the way we do that is we pick some sort of"
617360,624160," epsilon right so we pick some sort of epsilon a fixed small epsilon and essentially what we do we add"
624160,632800," that epsilon right to our uh you know to our parameter right and we compute a cost function somewhere here"
634240,640480," right the cost function somewhere here so we take a cost function of this entire thing okay cool so and"
640480,645920," we want to see what's the difference between the modified cost function and uh the previous one so we're"
645920,654160," going to take the difference in here uh right so and basically it tells us the this value all right"
654800,661120," uh but we want to know uh you know the the ratio between uh between the difference of the value of"
661120,668640," the function and uh the uh the value of the parameter so we divide this entire thing by this epsilon right"
668640,675920," and that is basically the velocity of the function that tells where the function wants to grow right it"
675920,681600," roughly wants to grow in this direction and essentially what we're doing uh once we got this entire thing so"
682960,691760," we can we can we can call it it's sort of like a different scene we just take and add this entire"
691760,698800," thing to our parameter and this becomes w1 right and we keep repeating that thing we keep repeating that"
698800,706480," thing uh right and we just pick uh some sort of like a an optimal epsilon manually uh we sort of like"
706480,712160," it's a magical value right so we we're trying like one epsilon so it didn't really work so we made it"
712160,718400," smaller we made it bigger and we picked the one that actually drives uh us to towards the local uh local"
718400,724480," minimum as fast as possible but but that creates like a little bit of a problem right so what if um"
724480,731440," we pick too big of an epsilon and basically we're going to be jumping uh back and forth around the minimum"
731440,739440," but we never actually uh reach the minimum right and if we pick the epsilon too small we may actually never"
739440,747520," reach the local minimum fast enough and furthermore the closer you are to the minimum the um you know"
747520,753360," the more changes you have to have so the sensitivity to the parameter actually changes over time some of"
753360,760480," the parameter um so the cost function is usually more sensitive more to one parameters but less sensitive"
760480,766720," to other parameters and basically this epsilon assumes uh essentially uh the equal sensitivity"
766720,773200," of all of the parameters so it's very much not optimal and it's um you know very imprecise as well"
773200,778320," right it's sort of a hack right it's sort of a hack and because of that people usually don't"
779600,786400," really uh don't really use this kind of approach at all right they usually don't uh use this kind of"
786400,792480," approach at all so what people usually use instead they usually use a gradient descent"
792480,799520," uh a gradient descent so let's actually try to uh to explore the gradient descent i wonder if we can"
800080,805200," switch to something more convenient uh then uh you know than my painting maybe we should switch to"
805200,812240," something like uh later right so uh i used to do like a mathematical explanation using later so i think"
812240,818400," uh the time has come to get back to that right so i think i've got a couple of uh subs so i think i"
818400,824720," would like to acknowledge them uh azora kimura thank you so much for gifting one community sub it was four"
824720,830640," days ago but i still want to acknowledge it because you know twitch shows me that uh 28 grace uh slater"
830640,836160," gray slater thank you so much for twitch prime uh ice leo thank you so much for twitch prime with them"
836160,842080," with a message yo more machine learning more machine learning indeed that's right i'm gonna do like"
842080,848000," even a couple of more streams right so i have a couple of ideas for this specific series"
848800,852800," low and see thank you so much for twitch prime thank you thank you thank you and welcome everyone"
852800,860960," to an epic artificial intelligence stream and we got another sub while i was actually acknowledging other"
860960,865360," subs uh damned me thank you so much for twitch prime subscription thank you thank you thank you"
865360,871600," uh alrighty so let's create the latest document where we're gonna do uh our exploration right so we're"
871600,878240," gonna call it grad dot tech right so i'm gonna try to remember how to use the later right"
878400,883760," because i haven't practiced for quite some time so i think you do document class right so we do"
883760,889200," document class and you say that the class you do is article and then you say something like"
889200,894800," uh this is where you start the document and let's do a section and then that's in that specific section"
894800,903280," specific section we're gonna say gradient this is very fun uh descent uh how do you spell descent"
903280,911360," descent yeah i think i need to google that up because i don't speak english uh right"
912720,921680," did i spell that correctly yeah i did okay i actually know english how about that how about that"
921680,932880," so uh how do we uh actually um you know so i want to rewrite this entire formula but in later right so"
932880,939440," for to do that i think align environment is not available in later uh right away so we have to do pdf"
939440,946960," later uh grad tech so i think it's going to complain uh yeah so there is no such environment"
946960,954880," okay so i think this environment is located in uh a package amy uh a not avs but ams math"
954880,961680," right so and this is where uh we're going to have this kind of stuff right so let's do pdf later it"
961680,967600," still doesn't have a line uh for a paragraph ended oh this is because it wants to have something okay"
967600,971120," so let's actually start defining something right so we're going to have the difference in the cost"
971120,977680," function equal to uh you know the cost function of the current parameter right so maybe we can call"
977680,985760," this current parameter as wi plus some sort of epsilon uh right some sort of epsilon minus the uh this"
985760,992400," thing without the epsilon right and we're going to divide this entire thing by the epsilon right so"
992400,998080," that's basically the idea in here uh right is it going to actually allow us to compile this entire"
998080,1006080," thing and it seems to be working it seems to be twerking so let's do mu pd mu pdf right there we go"
1006080,1009840," so to not flash bang you i think i'm going to invert the course and this is basically what"
1010320,1016240," uh i wrote in my paint right so i i think it's clear right so it's it's big enough so i think i can"
1016240,1024960," even split uh this entire thing sort of like this uh okay so and essentially doesn't this formula remind"
1024960,1032960," you something right doesn't this formula remind you something interesting right doesn't it look something"
1032960,1033520," familiar"
1033520,1046960," so essentially right uh this tells you uh yeah people people who know this kind of thing they instantly"
1046960,1053280," said it's it's a derivative right so it's it's very close to derivative here's an interesting thing so the"
1053280,1061360," value of uh dc it's sort of precision as the actual direction of the function depends on how small"
1061360,1072320," epsilon is right how small it is so and what if we make it infinitely small not equal to zero but"
1072320,1079200," infinitely small right we basically i'm gonna try to do a limit right so but i don't remember how to use"
1079200,1094400," this limit in later uh later limit uh bound okay oh that's actually pretty pretty good so we can do"
1094400,1101600," do they have to do underscore i didn't remember no i don't have to do it on this crop uh okay so"
1101600,1112000," so we can say uh epsilon right epsilon two uh zero right so maybe it will probably also make sense to"
1112000,1117440," do something like i right because it's sort of like an iteration uh but maybe uh i'm not sure if it makes"
1117440,1122720," sense to actually have this size so let's remove it to make it a little bit less noisy and let's try to"
1122720,1129760," recompile this entire thing and uh what am i am i an idiot oh it has to be with a slash because it's a"
1129760,1138400," command all right so it's a command yeah there we go so and essentially this looks like a definition of a"
1138400,1145920," derivative believe it or not if you just google up uh derivative derivative derivative um right"
1145920,1154640," derivative and we go to wikipedia that's literally the definition of derivative that's literally what a"
1154640,1166320," derivative is uh right so doesn't it look like what we just wrote right so here we're talking about some"
1166320,1171920," sort of abstract function and instead of epsilon they use h and but they still drive this h towards"
1171920,1178960," zero right so and basically what this thing does it tells you the sort of like a velocity of the function"
1178960,1185200," uh in one specific point in one specific point w so that's basically the derivative right so we can"
1185200,1192080," even write it like this right and if you use an actual derivative right if you use an actual derivative"
1192960,1197840," so people tell me that the underscore is missing yeah it's much better if you use an actual derivative"
1197840,1205040," you are moving towards local minimum precisely because that's like precise thing uh but here is the"
1205040,1216640," interesting stuff right so here is an interesting stuff um let's actually uh go back to the original"
1216640,1223280," simple example right to the original simple example where we had the model with a single parameter w and"
1223280,1228560," we were trying to uh find that parameter right so do you guys remember like if we take a look at the uh"
1228560,1233920," stuff from the first stream right so we have ml notes i think i'm going to give the link to ml notes"
1233920,1237920," uh in uh in the chat and also in the description so you can find the stuff in here"
1240640,1251040," there we go and uh so let's go to the description and here so ml notes from the first stream there we go"
1251040,1260480," so let's actually go there certain ml notes and we have a model in here called twice right and so"
1260480,1266480," basically we're trying to train a model that predicts this data where the first column is an input and the"
1266480,1272640," second column is an output and you can clearly see that this model is supposed to just double the"
1272640,1277040," number right it's supposed to double the number it's supposed to multiply by two but we don't know that"
1277040,1284800," so what we did we just assumed that model has uh you know sort of like a general form uh of uh"
1284800,1293200," essentially uh y equal w x and w is the parameter of the model right a single parameter of the model"
1293200,1299360," uh i suppose in this specific example i also added a bias to sort of like a fully simulate a neuron uh"
1299360,1306240," why not right so the actual model in here is basically uh w equal x multiplied by um y equal"
1306240,1312800," x multiplied by w plus b right so to actually do the gradient descent right so the gradient descent is"
1312800,1317120," basically using an actual derivative instead of finite differences right so that's basically what"
1317120,1322000," people usually mean by gradient descent it's just like you take a derivative at that specific point and"
1322000,1325920," you move in an opposite direction right because derivative tells you in which direction the"
1325920,1331200," function increases so you have to go in the opposite direction uh right so to actually use the actual"
1331200,1338160," gradient descent we have to um take a derivative of this function and here is an issue question how the"
1338160,1343920," f can we do that right this is there is like a for loop in here like how are we supposed to do that"
1343920,1350640," right there's also division in here and stuff like that uh right so to take a derivative of this entire"
1350640,1356640," function we need to write it sort of like a in a mathematical notation just to at least see"
1356640,1362000," how this entire thing is going to look like right so let's actually quickly do that let's try to write"
1362000,1367920," uh this entire model in a mathematical notation right so we can also take a look at how the model"
1367920,1374480," works if anyone is interested so we can uh just compile this entire thing uh right just a second"
1374560,1384800," let's do it twice there we go so we compile it and we can do twice and yeah uh so essentially cost was"
1384800,1390240," starting at here right so this is where the cost was starting and it was driving down until it i think"
1390240,1397280," became basically yeah very close uh right it became very close and we found sort of these two parameters"
1397280,1402320," right so the the weight and the bias uh and this essentially if you plug them into the formula it"
1402320,1410080," will multiply by two right so it will multiply by two if we get rid of the bias uh i think w will"
1410080,1414720," literally become two right i think w will literally become two maybe we should get rid of the bias"
1414720,1420080," uh for for the sake of simplicity right so let's actually get rid of the bias i think i think it's gonna be"
1420080,1428640," better right uh so that will require uh go into the compilation errors right so but that should be fast"
1428640,1436720," relatively right so here we don't have to provide the bias anymore uh so none of that stuff is needed"
1436720,1442800," like this entire thing is not needed uh this entire thing is not needed because it's a bias"
1445440,1448000," so this is a compiler assisted refactoring essentially"
1448000,1458640," new line okay so now if i just try to run and as you can see uh the parameter like literally became"
1458640,1464880," two right so we started with the parameter equal to four the cost was huge and then we draw the cause"
1464880,1469440," the cost down and it became uh this it was not super precise but that that's not really the point"
1469440,1474720," uh anyway so let's try to write it in a sort of mathematical form because we can't really"
1474720,1481600," take a derivative of this entire thing if it's not in a mathematical form right so uh right"
1481600,1490320," and uh so let's create maybe some sort of a subsection right subsection uh right not sub subsection"
1490320,1496080," but just subsection so what i'm gonna put here i'm gonna put like a twice in here so this is gonna be"
1496080,1502480," twice uh so let's go ahead and try to write the cost function let's say that the cost function has only"
1502480,1512960," one parameter w and essentially what we're doing we are uh plugging some sort of a like we have n uh we"
1512960,1519360," have n samples in here right so if we take a look at the code in here so this is the coast uh we have"
1519360,1525680," n samples and we are iterating n samples right so this is going to be n"
1525680,1535600," and in here down there train column count uh we divide it by n so essentially what we're doing for"
1535600,1544720," each individual sample we are actually uh finding the square of the difference of the actual value of the"
1544720,1552800," model and expected one right so let's actually denote the inputs uh as x i right so because we have like"
1552800,1558880," several uh several samples in the training that and the output as uh yi right so since it's going to be"
1558880,1565040," some we can write it like this so this is a sum and we're iterating starting from one up until uh n"
1565040,1572480," right so we have like n samples in here and what we do we take the current sample i right the current"
1572480,1579280," sample i multiplied by w we don't have a any uh any bias in here i remove the bias and we subtract"
1579280,1587200," expected uh expected value and we rise this entire thing to uh to the power two right so uh so this is"
1587200,1593520," essentially we're computing the cost function which is the sum of the squares of the differences like"
1593520,1599440," uh right so if the actual value deviates from the expected value too much the square is going to"
1599440,1604640," magnify that difference it is going to magnify the difference and the cost function is going to"
1604640,1612800," uh like basically rise very very quickly right and after that uh i need to divide all of that stuff"
1612800,1618800," by the amount of samples because it's a um it's an average right so we're doing an average in here so"
1618800,1624480," divide it by n so this is how you can mathematically write this entire function right so although it's"
1624480,1631040," procedural you can like easily write it mathematically uh and let's actually try to go ahead and uh see how"
1631040,1636160," it looks like it looks like by the way right so i didn't want to put a star in here right because"
1636160,1642320," in light you're not supposed to do that uh right there we go and to be fair this kind of fraction"
1642320,1648560," it's just like makes it look kind of kind of meh uh i think i could have done something like one"
1648560,1659040," over n uh like so all right so let me quickly do that uh huh there we go uh yeah that looks like that"
1659040,1664000," that looks much better that looks much better right look at that so this is basically our cost function"
1664000,1671920," right essentially essentially our cost function so what we need to do we just need to find this"
1671920,1680640," derivative like how can we find this derivative uh so let's like literally take this entire thing"
1680640,1684560," and put it like this right so i'm gonna put parentheses around this entire thing and we're"
1684560,1689920," gonna do that step by step right since we're using later it's going to be much easier to do that and"
1689920,1695360," later than uh in with the pen and paper or in my paint right because what i will be able to do"
1695360,1701360," i'll be able to copy paste entire chunks of the expressions and move them around and show you mechanically"
1701360,1706320," how you're supposed to do all of that right so this is something that is kind of difficult to convey"
1706320,1712640," when you're like writing uh like handwriting because you cannot rearrange things like by just taking them"
1712640,1716640," and moving them to a different place here in the text editor i can do that and i think it's a huge"
1716640,1723040," advantage when you're using later so quite often when i do mathematical doodles i just use this kind of"
1723040,1728480," thing to rearrange around use knock knock is not finishing unfortunately right so i'm going to finish"
1728480,1736800," it one day maybe uh all right so this looks like shise because they are not aligned by equals right i"
1736800,1743680," would like them to be aligned by equals look at that look how nicely they are aligned and kind of these"
1743680,1752400," parentheses look lame i would like them to be as large as the expressions they enclose so to do that i think there is"
1752400,1761280," this left and right thingy uh look at that doesn't this look serious doesn't this look serious right"
1761280,1767680," so like you look at this formula and you're thinking holy they're doing something important"
1767680,1773600," holy he probably has a phd in like in nuclear physics or something like holy look at"
1773600,1777920," these brackets they're so huge holy he's a genius in fact i'm"
1777920,1784240," just i don't even know what the fuck i'm doing right so in reality i don't really know what the"
1784240,1789840," fuck i'm doing and in fact i'm going to google a lot how to do this kind of shit because i don't"
1789840,1795760," fucking remember okay so here's an interesting thing how do you take a derivative of this entire"
1795760,1802240," shit all right so here we have some sort of a constant and it's quite important to remember we're"
1802240,1809360," doing derivative relative to only this variable right only to this variable so we have two"
1809360,1817440," expressions we have a constant multiplied by this expression which is basically a function of x right"
1817440,1825040," so this entire thing this entire sum is a function of x and we multiply it by some sort of a constant so"
1825040,1831200," what a derivative of such expression would look like right so we can take a look at the derivative cheat sheet"
1831200,1836080," right so we're going to cheat a little bit uh derivative uh cheat sheet"
1836080,1847520," all right so let's actually grab the first thing uh please don't tell me professor that i'm cheating on"
1847520,1855680," the stream okay so do we have anything useful and this looks like oh there we go this looks like"
1855680,1858960," what we need look at that so you have"
1858960,1866000," is that a sufficient quality like i don't know like it would be better to"
1866000,1874000," to maybe have yeah this is shit but i think it's it's kind of close right so you have an expression"
1874000,1881040," right so constant a function of x right and basically you can put the constant outside of that specific"
1881040,1885840," derivative right so and essentially what it allows us to do right what it allows us to do"
1885840,1893040," is just move this entire thing like this"
1893040,1902880," look at that so i suppose uh it would be even better to have an equals in here look at that yeah so because"
1902880,1908400," we're sort of like chaining this entire stuff like around so essentially the this particular derivative"
1908400,1914560," like it's not going to do anything to this specific cost okay so now we have a derivative of a sum"
1914560,1921120," right so and effectively what kind of sum do we even have in here right so what kind of sum do we even"
1921120,1930160," have in here uh we can think about this sum as essentially um so let me let me try to do something like this x"
1936480,1955520," yeah so we can say that this is x zero y zero plus uh x one y one plus and uh how do you do like uh"
1955520,1961760," h dots or something like that i think this is how you you effectively do that right so basically the rest"
1961760,1966240," but i'm not quite sure uh yeah there we go so that's how we do that so effectively you have this"
1966240,1974000," so and uh a derivative of a sum right if you have a derivative of a sum is basically a sum of"
1974000,1980000," derivatives right so this is what it says in the cheat sheet right so it's totally okay to cheat right so"
1980000,1984880," they tell you that you're not supposed to cheat on an exam but this is not the exam this is a real life"
1984880,1994240," you're supposed to cheat in real life and in real life everyone cheats and if you don't you have"
1994240,2001280," disadvantage that's right so that's as simple as that believe it or not as simple as that"
2001280,2008960," so it's not like you know it's unfair right you're not supposed to cheat like everyone cheats"
2009520,2016320," literally if you don't do that you're putting yourself in a huge disadvantage right so um it is"
2016320,2020800," what it is and it isn't what it isn't so effectively if you have a derivative of a sum"
2020800,2026800," it's a sum of derivative so which means that effectively i can move this entire parenthesis even"
2026800,2032560," further inside of this entire thing right so i can just say okay just put this entire left"
2033840,2039840," uh like inside in here right so and what i effectively do like i just basically told you"
2039840,2045040," that uh we can move this um derivative like further and further inside of this entire stuff"
2045040,2055120," um so i don't really like how we repeat the the sum a lot right so we keep repeating some quite a lot"
2055120,2061120," not gonna lie but maybe that's fine for the specific case uh so maybe we can come up with"
2061120,2066800," uh some sort of like a command that can allow us to reduce all that anyway so here we have a square"
2066800,2074000," of a function of x right a square of a function of x so what you're supposed to do with this kind of"
2074000,2083120," thing so here uh we are getting into a territory of a chain rule right um right so where is the chain room"
2083120,2091120," i can't really see uh chain rule and other examples yeah there we go so this is the most important"
2091120,2097840," formula uh that we can have in here right so that we're going to be keep using throughout the string"
2097840,2102720," it's a chain rule it's essentially when you're trying to take derivative of two nested functions"
2102720,2110320," and do we even have like nested functions specifically in here right do we have any nested functions in here"
2110320,2117520," uh well the first function that you can see is the function that takes a square of something right so"
2117520,2123600," we can say that the uh the very first function is the one that takes the square of the thing right and"
2123600,2129440," it's an outer function right it's an outer function and the inner function is uh this thing that"
2129440,2138480," multiplies the x by uh the weight and uh subtracts the the w right so in this case we can say so x multiplied"
2138480,2144160," by w minus y right so this is basically two functions like this is basically two functions and essentially what"
2144160,2153520," we have to do uh we have to uh take uh the outer function like the entire expression of the outer"
2153520,2162080," function and uh basically move it outside and take a derivative of it considering this entire thing as"
2162080,2168640," the single variable so and derivative of square is basically two multiplied by this thing right so this"
2168640,2175360," is the outer function we took a derivative over outer function right then we have to take a derivative of"
2175360,2183280," an inner function like so uh right so and let's actually recompile this entire thing let's recompile this"
2183280,2190240," entire thing and uh it didn't work something is missing uh yet again and this is because we have too many"
2190240,2198560," repeating uh repeating things so the line is 21 um i don't see what's wrong in here so it's supposed"
2198640,2203600," to be oh yeah it's because it's a double right in here i think that's what it's complaining about"
2203600,2210080," so let's remove this entire thing okay so and now if i take a look at this entire stuff there we go"
2210080,2215600," so as you can see what we did in here is essentially we took the outer function and took a derivative of"
2215600,2220480," it and it's just basically to that specific expression and then we take the inner function and"
2220480,2224720," we take a derivative forward we didn't compute it yet but we're going to compute it right and essentially"
2224720,2232080," this entire idea is uh if you have two like several nested functions you just like peel off the layers"
2232080,2236480," of the function so we compute the derivative of the outer layer and you sort of peel it off and then you"
2236480,2241280," compute the derivative of the other layer and you peel it off and you just like multiply them together"
2241280,2246160," so this is how we make mechanically do that and you can do that with as many functions as possible"
2246160,2249920," uh as you as you want actually right so that's basically what it is that's how you"
2250640,2256240," take a derivative for this entire stuff right and this is precisely the reason why i didn't want to"
2256240,2262640," show this entire on the first stream because that will scare you away because it's not very pedagogical"
2262640,2268960," pedagogical or how do you say that in english right so because the point of like optimizing the cost function"
2268960,2278560," is not about that per se right it's about just driving the cost function towards zero right but to drive it"
2278560,2284160," more precisely and more efficiently you need to take its derivative and derivative is just like"
2284160,2291680," the velocity of the function and you have to go in the opposite direction right so yeah all right so i"
2291680,2298320," think we've got some uh subs and some things some beats i would even say uh azori akimori thank you so"
2298320,2304080," much for on hand beats with the message there once was a man named toying who sat down to do some coding"
2304080,2310080," he steeped the tea and started the stream for another season of recreational programming uh thank"
2310080,2314560," you thank you so much for working a bit is that supposed to be some sort of a poem i'm too stupid"
2314560,2323680," i'm sorry my brain preoccupied with trying to explain uh this stuff right so and um i might be missing"
2323680,2330400," something i really apologize for that uh oh it's a limerick oh yeah you're supposed to read it as a limerick so"
2330400,2335360," i'm really sorry because because my brain is really preoccupied with like how i'm going to continue"
2335360,2341200," about that because it's a kind of a you know um interesting topic right there is a lot of details"
2341200,2345520," it's not particularly difficult it's just like there is a lot of details and you need to get uh get them"
2345520,2353920," right uh okay so how can we um get the derivative of this specific expression right so this thing uh here"
2353920,2358480," is actually a constant so that means it doesn't really matter so we we can like straight up like"
2358480,2364400," remove this and that thing uh right because it's going to be equal to zero anyway so uh nobody really"
2364400,2370160," cares right so nobody really cares and we need to take a derivative of this entire thing right and when"
2370160,2376080," you have a derivative of a constant and uh a variable right of a constant of a and a variable"
2376080,2384080," uh the constant is basically uh the the variable basically becomes one and you're only left with a"
2384080,2390080," constant right so the variable in this specific case is w because it's a parameter of the cost function"
2390080,2396080," well it's somewhere here but yeah you get the point so essentially the result of this entire stuff"
2396080,2407360," is going to be uh x1 right is going to be x1 and we just computed the derivative of the cost function"
2407360,2415680," right so i would like to maybe even reiterate a little bit further right so we can basically put"
2415680,2424240," them side by side right so this is the original function right which is a sum and this one is a cost function"
2425840,2432880," so this one is the cost function and we're going to put it like this and let's just actually compare"
2432880,2439600," them uh compare them together so here they are here they are so this is how we compute the cost so this"
2439600,2447760," is the first thing and this is the velocity like how where exactly that cost is moving right so that's"
2447760,2455280," basically what it is so what we need to do now to create a better learning algorithm is instead of coding"
2455280,2462720," this and instead of coding a finite difference let's try to code this and basically keep subtracting that"
2462720,2476080," value from the current parameter right um so that's essentially what we can try to do let's go ahead"
2476080,2482960," and do that so let's literally try to code that i accidentally closed it uh right so let me open it one"
2482960,2490560," more time you know what i think i need to move the entire pdf document to towards like ml notes i think"
2490560,2494000," i'm going to actually move it in here because that's where i'm going to be doing all this stuff"
2494640,2509040," uh right so let's go ahead and like literally code the this entire thing twice dot c so here we have a cost"
2509040,2514960," right so here we have a cost and let's actually introduce uh g cost right so for the gradient or maybe"
2514960,2521760," maybe d cost right uh flow w uh right and what we're gonna have in here we're gonna have a result so let"
2521760,2528080," me maybe uh split the screen into did i already close it accidentally i think i already no no i didn't"
2528080,2537920," done immediately i'm sorry um right so there we go so here it is this can be zero so the actual n is in"
2537920,2546240," here is the train count we are iterating through all of the examples uh n plus plus i right and what"
2546240,2553680," we're adding in here uh right we're adding two multiplied by the current x so we need to get the current x"
2554640,2562160," right so maybe we can even steal this entire stuff from here right so x is a train i uh zero and the"
2562160,2571440," expected y is one so here we do x multiplied by w minus y multiplied by x again right so because this"
2571440,2578640," is the derivative then after that i just divide this entire thing by n and there we go we got the uh"
2578640,2585760," derivative of the cost function right derivative of the cost function so uh what's gonna be the next"
2585760,2592720," thing so here uh right so here we're just like um taking the finite difference in here right by epsilon"
2592720,2600480," but we don't have to do that anymore so we can do instead we can do uh d cost w there we go and if i"
2600480,2605840," try to compile this into i think i think it's going to be kind of interesting right so the uh compiler is"
2605840,2610640," going to complain that uh we don't really use cost function anymore right so which is"
2610640,2615120," kind of interesting right so because we were completely like you know competing the difference"
2615120,2620800," from this entire stuff and another parameter that we don't use anymore is an epsilon right so we"
2620800,2627680," effectively don't need an epsilon anymore right and let's try to actually step only once right so let's"
2627680,2632560," actually try to step only once maybe some way here i actually want to print like the full thing uh maybe"
2632560,2640000," even zero right so let's actually do zero and i'm gonna do twice right so the cost is 83 it's pretty"
2640000,2646960," big the actual value is w uh but it's supposed to be two right what if i now step once so it's using"
2646960,2653680," like absolutely random value let's actually fixate the value to something stable right so let's say 69"
2653680,2660160," uh right there we go uh so we have of course that and no matter how many times i run this and i think"
2660160,2667040," it's going to be the same and now let's try to do one iteration of that uh of that specific thing uh"
2667040,2674560," right so one iteration looking good uh we can i think even move a bit faster or maybe not right but"
2674560,2679920," as you can see it actually drives the cost uh closer to zero right it actually drives the cost to equal to"
2679920,2686880," zero so if we try to do like 10 iteration uh right it drives pretty uh pretty fast uh we can even take it"
2686880,2693760," two i think it's gonna go even faster right there we go uh right maybe even like one and yeah"
2693760,2704000," if you take an epsilon of one uh it goes towards like zero really really fast so it only took around"
2704000,2711360," i think yeah six iterations to actually go towards zero and became equal to two instantly"
2712080,2719440," uh right so it goes there really really fast so let's say uh we're gonna have six of them"
2719440,2726720," uh right and it's like two uh yeah it's a little bit too fast and i wonder can we compare all of that"
2726720,2732560," with the finite differences this will be kind of interesting so if we do zero uh right so here is"
2732560,2740880," the original coast and this is going to give flow dw so this is the finite differences uh and in here"
2740880,2748720," i'll skip and if there we go so and we're going to put the epsilon somewhere here all right"
2748720,2757280," there we go so now we can flip-flop between this entire implementation so this is the essentially um"
2758880,2765360," gradient descent right so we're actually computing the like the actual uh you know derivative and"
2765360,2771760," here we're gonna use the finite differences right so and finite difference is actually as fast uh you"
2771760,2778160," know as the rest of the stuff but it kind of yeah it still didn't reach like the two precisely i would"
2778160,2786000," even say so if we like make 10 uh right it still doesn't really fully reach because the epsilon is probably"
2786000,2792480," too big right the epsilon is too big so if we kind of make like 50 right it still couldn't reach it like"
2792480,2801040," as soon as the uh you know step like as soon as the function becomes too like sensitive to the parameter"
2801040,2807920," uh epsilon is failing to adapt right so because we got too close to the actual minimum and epsilon is too"
2807920,2814000," big it just like cannot like jump over it right because because it becomes too big the advantage of the"
2814000,2820400," actual derivative of using actual derivative is that uh as closer if it gets closer to the local minimum"
2820400,2826480," it becomes smaller and smaller and smaller and it can get very very precise right it can get very very"
2826480,2839600," precise um so and uh in case of the uh you know actual gradient right it can just like straight up jump"
2839600,2846880," here and just stay here for as long as possible uh it's just a fancy route finding method yeah exactly"
2846880,2853600," isn't like a route finding basically a gradient descent but it's a it's a gradient descent with"
2853600,2858560," like a second derivative for some reason i don't really understand the justification of this entire step"
2858560,2866720," but yeah uh i don't really understand the justification but anyway so and uh as far as i know uh computationally"
2866720,2876560," uh computing d cost is actually faster than doing finite differences right because um you have to compute"
2876560,2883040," the cost twice in here right you have to compute the cost twice and just subtract it here you basically"
2883040,2887920," computing a different function but only once so because of that you're kind of iterating through"
2887920,2895360," the data uh like uh only once so maybe that also uh you know contributes to like better performance but"
2895360,2901920," i'm not like a mathematician i'm not the ai person i'm just like talking out of my ass but from my testing"
2901920,2906160," uh gradient descent actually greatly improves the performance it's just like works faster"
2906960,2914240," right the generally uh works faster so uh yeah and that's basically what it is believe it or not so"
2914240,2922320," this is basically how you're supposed to uh you know optimize the cost function you need to take the gradient"
2922320,2929920," the derivative of that function and gradient is basically the the derivative right so uh right so"
2929920,2934400," do you guys know what is a gradient and what's the difference between gradient and derivative and stuff like"
2934400,2943280," there uh right so essentially um when you take a derivative it tells you the velocity of the function"
2943280,2950960," right uh so where it moves but it makes sense when your function has one uh variable what if your function"
2950960,2956800," has several variables right like two variables for instance how we take a derivative so what is the"
2956800,2962560," velocity where things are growing where it's moving or something like that gradient is basically a"
2962560,2969680," derivative for multi-variable uh multi-variable function but instead of returning one number it"
2969680,2978240," returns a vector that points towards the direction the uh the function is moving right so for instance"
2978240,2985040," you can even google up what is a gradient right so it's sort of like a vector field uh you know function"
2986160,2994800," function gradient gradient gradient is a vector gradient is a vector indeed so you can google it up right so"
2994800,2999440," if you have a two-dimensional function essentially you have some sort of a field right so because where"
2999440,3006480," the variables are x and y and uh the value of the function is like the third dimension uh right and"
3006480,3010880," yeah so the gradient basically computes the derivative for this and it just creates this sort of like"
3010880,3016400," gradient like a vector field so we have these two functions uh with like a local minimum and local"
3016400,3022000," maximum in here uh right and because of that this specific field moves like sort of towards this"
3022000,3029200," local minimum and uh because of that it moves like though i think yeah so this is supposed to be minimum"
3029200,3036880," right so because the gradient actually uh grows towards the the bigger value right and this uh this is the"
3036880,3042960," the minimum and this is the maximum right it grows towards the maximum so yeah there's like two peaks"
3042960,3050000," in here so it's a basically a function that uh returns you a vector right so the derivative returns you like"
3050000,3056560," a one value but in this case it returns a vector right uh and since uh our model can have"
3056560,3064880," many parameters gradient is going to be vector with the amount of elements as the amount of parameters of the model"
3064880,3071840," and you will have to subtract that gradient from your model and your model can have one trillion parameters"
3071840,3081040," right so imagine a vector with one trillion elements right so that's basically how you probably train"
3081040,3087520," gpt4 right gpt4 has one trillion parameters so what they do they compute a gradient which is a vector"
3087520,3092880," of you know one trillion parameters one trillion element vector and they subtract that from the whole model"
3094880,3101920," so yeah essentially so and on the high level this is pretty straightforward right so if you know"
3101920,3107760," derivatives if you know partial derivative and gradients and stuff like that you can just do that but it"
3107760,3116400," it's getting kind of complicated when the cost function grows in complexity right so when the cost function"
3116400,3122720," grows in complexity it becomes really kind of complicated uh right so and this is basically where back"
3122720,3129840," propagation sort of comes into play right so this is where it comes into play because it basically allows"
3129840,3137200," you it's a technique of computing the gradients and derivatives of a very complicated nested functions"
3137200,3139840," and this is basically the cost function of a neural network"
3143520,3150720," so that's basically what it is uh right so but when let's not actually jump too much into the complexity"
3150720,3159120," of like computing the gradient of an actual neural network so let's um start with simpler neural networks"
3159120,3163920," right let's start with the neural networks with a single neuron you can say that this is basically um a"
3163920,3171840," neural network with a single neuron but the neuron also has a bias and an activation function right so uh right"
3171840,3178240," so let's actually put something let's go to the grad text and let's introduce subsection subsection"
3178240,3188800," one neuron model right so if we take a look at the uh right so of this thing so the actual value"
3188800,3196400," right is basically the input multiplied by a weight of the connection plus the bias of the neuron and some"
3196400,3201920," sort of activation function uh which in our case we're using sigmoids right so we're using sigma"
3201920,3208960," uh so that's basically how like the model of a single neuron right the model of a single brain cell"
3208960,3213920," so let's actually try to do pdf grad right so and let's take a look at this entire thing so this is how"
3213920,3219200," we do that so we're not supposed to have uh this star in here i don't think it makes any sense"
3219840,3225200," right so this is a simple thing in here and a sigmoid right so sigmoid is a function that"
3225200,3231760," basically maps values from minus infinity to plus infinity to uh two values from the range from zero"
3231760,3237200," to one right it takes the minus infinity plus infinity and squashes it into zero and one so that's"
3237200,3244640," what it does so uh we can even define right the sigmoid in here just in case just to have it like in"
3244640,3250400," front of us when we're competing things right sigma of x is if i'm not mistaken i think it's something"
3250400,3260960," like one over one plus e to the power of minus x i purely like memorize this thing uh right i think"
3260960,3266480," that's how it looks like if i'm not mistaken uh yeah this is this is basically how it's defined it's not"
3266480,3274720," really uh that interesting for us right so it's not really that interesting uh right so and what's"
3274720,3280720," interesting for us is its derivative right its derivative actually literally equal to itself"
3280720,3287280," but not really multiplied by the sort of like inverted value of itself right so that's basically the"
3287280,3292160," derivative of this entire thing and it's kind of useful to have in front of us right because we will"
3292160,3298800," need this kind of thing when we compete in derivative and stuff like that all right so okay so this is a"
3298800,3306720," one neuron model right so we can even try to to draw it using like ticks or something like that would be"
3306720,3313760," kind of interesting right so uh let's do we probably have to do like a use package ticks right so let's put"
3313760,3325760," ticks um right and i think it's something like ticks uh picture right so there we go yeah it works it"
3325760,3332080," doesn't complain which is nice if i remember correctly the ticks has this notion of like nodes right so let's"
3332080,3342160," put a node um like at zero zero and you put like a text in here and it will literally put the text in here"
3342160,3347920," i didn't yeah so i didn't think it it works like that oh you forgot the semicolon yeah you're supposed"
3347920,3355360," to also put semicolon in here uh right so where is my compilation error uh compilation buffer there we go"
3355360,3364080," hello but it's not centered right so we have to center this entire thing oh my god now the completion"
3364080,3371040," poisoning now the completion point okay so there we go so this is hello and uh i actually want to display it"
3371040,3378640," as a single neuron right so i think you have to do something like shape circle uh i think it will uh it"
3378640,3388160," will work like it was something like i don't know draw and you something like color or whatever like"
3388160,3396000," it's just like i'm going pure out of the intuition uh yeah okay it's so freaking stupid anyway uh so"
3397600,3404240," um okay and uh here we have like two things right so basically the the the weight and bias right so"
3404240,3411040," this is the weight and bias and this is the sort of the entire thing um right it also has activation"
3411040,3417680," function right so we can probably put uh this stuff in here as well so we can put the sigma right but"
3417680,3424720," this can probably going to be too too big uh yeah but it's fine anyway so we may want to place some"
3424720,3431680," parameters right we want to place some parameters at like minus one uh right and in here we can say"
3431680,3440000," this is sort of like x right and then at plus one we want to put uh y right something like this"
3442080,3450720," uh so maybe i want to put spaces in here uh huh there we go but i honestly don't really want to draw"
3450720,3460720," anything around this stuff right so yeah so let's put them apart a little bit so maybe you can define"
3461280,3466000," some sort of some sort of a parameter in here right where we're going to have just one and i'm going"
3466000,3476240," to put emacs uh it's a it's more of a multi cursor problem rather than emacs so now i can do something"
3476240,3486400," like two yeah there we go so could they actually have uh right so let's put happen here yeah there we go"
3486400,3490000," so now what i want to do i want to connect these things and i think it's sort of like a path"
3490000,3497760," um right you also have to specify the names for these things right so this is basically uh x"
3497760,3508960," this is a neuron and this is a y right we want to do a path from x edge y uh and i think you specify"
3508960,3516480," like a direction or like what's an error or whatnot uh right if i'm not mistaken well yeah it kind of"
3516480,3524320," worked but it has to go towards the end yeah there we go so that's basically it and then we're moving"
3524320,3533840," from n to y and we've got a single representation of a single neuron how about that isn't that poggers"
3533840,3540960," this is neuron doesn't look cool i think it looks pretty freaking cool that's amazing so that's what"
3540960,3548560," we're trying to model in this specific case that's very cool so this is how we compute these parameters"
3548560,3553520," and this is the definition of sigma and all of that is going to be very uh very useful right it's a one"
3553520,3561920," neuron model so let's go ahead and create like a sub sub section right sub subsection for one neuron"
3561920,3566880," model and we're going to call it a cost right so what's going to be the cost uh right so let's do align"
3566880,3576080," so cost function so here we actually accept two parameters right so w and b and as we make our model"
3576080,3579920," more and more complicated right as we make it more and more complicated we're going to have just more"
3580400,3586800," uh parameters in here so i think we should basically drop uh passing parameters in here because like"
3586800,3592240," it's just like it's not useful right so we have too many parameters in my opinion uh anyway so we're"
3592240,3601680," gonna have this uh fraction one over n which i find kind of annoying to like you know type out all the"
3601680,3606480," time because we're gonna be repeating it anyway and it just like takes too much space if you know what i mean"
3607680,3616720," right so so where is the cost here this right so it's just that this is how much space it takes just"
3616720,3623360," to draw this specific thing and it's just like not particularly useful uh okay so what we want to do"
3623360,3632320," all right we want to take the activation of that neuron for one single sample of the training data i"
3632320,3637840," and we want to subtract what is expected right what is expected and want to write that to the power of"
3637840,3643520," two right so that's what we want to do in here and there we go so we've got this entire thing so the"
3643520,3648960," next thing we need to do we actually need to start computing the derivative but in this case we need to"
3648960,3656080," modify not a single parameter w right not a single parameter double but also b and this is where partial"
3656080,3662000," derivatives come into play all right and partial derivatives are actually very simple right so"
3662000,3667680," essentially you say okay i'm computing derivative with respect to w and what that means that means i"
3667680,3673600," treat w as a variable and even though b is also a variable dysfunction i just treat it as a constant"
3673600,3680960," right and i compute uh basically this entire thing sort of like with respect to this entire stuff so i think"
3680960,3688480," it's done like partial uh partial and then you have to put a fraction in here the notation of this of this"
3688480,3695040," stuff is actually very annoying uh so the difference of c with respect to the difference of w right so and"
3695040,3703360," then if we put this parameter in here so something like this right so we're competing something like this"
3703360,3712000," right and i find this particular notation kind of annoying so partial derivative notation right because"
3712000,3717520," it's kind of like noisy like it's just like too much noise but you kind of need to have a separate"
3717520,3723760," notation from the prime because uh you want to specify that it's not just derivative it's derivative"
3723760,3728880," specifically like with respect to this variable because you can have a derivative with respect to"
3728880,3732720," different variables and stuff like that so it's it's kind of important so partial derivative"
3732720,3737760," is there any better notation that than this right something more like yeah i would like to use"
3737760,3742560," something like this but i'd like to have like a one single small thing that you can move around"
3742560,3748960," and i kind of like this approach right partial with a subscript of the variable that you're doing"
3748960,3756560," i find this like so the fraction style notations are kind of like too noisy they're really good for"
3756560,3762160," job security in my opinion if you work as a mathematician or as a computer scientist you can just like slap"
3762160,3767840," them around your entire paper and it just looks impressive and everyone holy this person is"
3767840,3774720," irreplaceable because nobody except them can read that holy so yeah this is very useful for"
3774720,3781520," job security in my opinion but for explanation it's not particularly useful because it's too noisy"
3781520,3788720," right so yeah and because of that i kind of like this one right because it's a little bit more compact it's"
3788720,3793520," still a bit noisy but it's a bit more compact so instead of doing that what we're going to say"
3793520,3797360," right so i'm going to just remove it like that"
3797360,3804640," i'm going to say partial but with a subscript of the current variable that we're working with"
3804640,3813120," right so that's already better but in in the later itself this is too big because we used to just move a"
3813120,3819680," single prime around right remember how we were just moving it around and like moving this like a huge"
3819680,3824960," thing around is not particularly pleasant as well we can define like a separate sort of command uh let's"
3824960,3830160," call it partial derivative like pd and it's going to have one single parameter and we're going to just"
3830160,3837120," basically construct uh partial underscore that single parameter and now we can replace this entire thing"
3837120,3845200," with pd uh w uh and that is a little bit more compact there we go so now we can just move this small little"
3845200,3852160," thing around uh and uh again right so to take a derivative of this entire stuff right to take a derivative of this entire stuff"
3852160,3862880," uh we could place it somewhere here we could say pd w right and then left underscore and then right underscore"
3862880,3870320," something like this and look how impressive that looks just look at how impressive this entire"
3870320,3877040," shit looks like holy uh right but we know that we can move it basically instead inside of the sum because"
3877040,3883040," this is a constant this is a sum so we can just straight up take this entire stuff right this entire"
3883040,3889680," stuff and move it uh inside of this thing and that's basically what we have to actually compute in here"
3889680,3897680," right so and another thing that i already mentioned i don't really like how huge this entire thing is"
3897680,3902800," right so we keep repeating all of that but it doesn't really convey too much information i'm going to be"
3902800,3908560," copying it everywhere so i think i'm going to define some other function let's call it so it's what is"
3908560,3914400," it it's basically an average sum right so it's an average sum and it has two parameters the iterator and"
3914400,3920640," the the amount of iteration right so that's basically what it is and the uh amount of iteration goes here"
3920640,3928160," so it's basically n and the iterator goes here so now i can basically replace this entire shit with uh"
3928800,3941200," yeah a well avg sum why you keep asking me this okay uh i n there we go so that's actually a little"
3941200,3948720," bit more compact uh right so at least now this entire stuff is justified there we go so that's cool that's"
3948720,3957120," very very cool okay so uh we're taking derivative of a square so we have to apply the chain rule the"
3957120,3962640," the same we apply uh the same way we apply chain rules in here right so essentially uh here we have"
3962640,3970400," the outer function which is a square so we have to basically take that entire square and move it outside"
3970400,3976640," of this entire derivative right move it outside of this entire derivative take derivative of this entire thing"
3976640,3981600," and then we have to take the derivative of the inner thing right we're taking the derivative of the"
3981600,3990240," inner thing something like this uh there we go so that's basically what we have now right uh right but"
3990240,3998800," yeah i forgot to actually move this partial derivative towards the the left thing so now it goes like this"
3998800,4004640," so we sort of like uh peeled off one layer of uh derivative and stuff like that so basically by the way"
4004640,4009120," yeah it starts with zero it's supposed to actually start with one uh thank you so much for telling me"
4009120,4015680," right because we're mathematicians and mathematicians start all of that shit from one uh there we go so"
4015680,4023040," you see like there is another annoying like huge thing in here if you can notice right it's like"
4023040,4031920," repetition of this expression of sigma x i w plus b it's it's repeated three times and it adds a lot of mental"
4031920,4036560," overload like it's just kind of difficult to read this entire thing because it's it looks like there's a"
4036560,4042080," lot of stuff in here going on right but it's actually it not that much stuff is going on"
4042080,4050560," really not that much stuff so what if we factor out that specific expression into a separate variable"
4050560,4054560," just like we're doing programming right even though we're doing math we're still programmers we're still"
4054560,4060160," engineers so it would be kind of nice to factor it out to something else okay so this thing is like"
4060160,4066320," related to the ith sample right so let's actually say that it's basically ith active like activation"
4066320,4073840," for the ith sample or something like that and then we can take this entire expression right and literally"
4073840,4075440," replace it with ai"
4075440,4084560," and boom look how much nicer it looks like it's suddenly became readable you just like factor it out"
4084560,4090720," and it's just like oh yeah that makes sense now right i think like factoring out shit like that into"
4090720,4095760," separate variables it's just like so much valuable because oh i can now see shit because before like i"
4095760,4105760," couldn't see shit in this mist uh right but now i can um w i and uh b i no no we don't have to put i in here"
4106320,4115120," because i is the ith sample right it's a ith sample we have only one w and we have only one b"
4115120,4122320," and they are not related to the samples at all right so that's why there is no subscript for w and b"
4122320,4128880," does that make sense because i denotes the current sample right so we're writing from uh one to n and we"
4128880,4134800," have n sample and for each sample we just like uh you know add this sort of like a square right"
4134800,4142560," if that makes any sense anyway so in here right in here uh yi is a constant which is not related to w"
4142560,4148080," at all so that means this entire thing uh really not needed right so it's a constant so we can just"
4148080,4153360," basically remove this entire stuff so the the only thing that we need to compute in here is basically"
4153360,4161120," uh just this right so it's basically just this so to be fair i think i'm moving too fast like i usually"
4161120,4168000," have to have like like to have some sort of steps in here so i think uh i need to put this stuff in here"
4168000,4181040," right uh-huh and here let's actually have pd w uh left all right"
4183520,4187680," yeah there we go well i mean probably we want to move pd"
4187680,4198240," inside of here so it makes more sense yeah okay so the first step was this and then we uh like applied"
4198240,4204160," chain rule to the to the outer function and we got that and i think maybe we could remove the parentheses"
4204160,4211200," now so the only thing we need to do in here is compute the derivative of ai uh with respect to the"
4211200,4218080," the variable w right with respect to the variable w so let's actually put it near in here i think like"
4218080,4222800," it would be nice to keep things that are related to each other close to each other so we have c"
4222800,4229520," and then derivative of c and then we need derivative of ai so let's put the definition of derivative of ai"
4229520,4238960," after the ai right so i think that would make sense so pd w uh ai and what it should be equal to right so that"
4238960,4247840," means basically what we have to do is just like take pd w uh like this and i can probably even move this"
4247840,4254720," into that stuff like so right so that's basically what we can do in here there we go so how we can do that"
4254720,4260800," so as you can see here we have uh two functions right here we have two functions so like the outer"
4260800,4267120," function is the sigmoid and the inner function is this linear function right so and again since it's"
4267120,4273600," an acid function we can apply the chain rule right so we take this thing move it out and take the"
4273600,4280640," derivative of it but what is the derivative of a sigmoid right so uh the derivative of a sigmoid is the"
4280640,4288400," sigmoid multiplied by one minus sigmoid right so and that means it's super easy so here is the sigmoid and we just"
4288400,4296560," multiply it by one minus sigmoid uh and we're kind of having a similar situation again i thought we"
4296560,4302160," sort of like tucked some bullshit behind the variable and it explodes inside of that variable anyway"
4303760,4312400," so i think we need to factor out more things in here what if we take a thing inside of the sigma"
4312400,4319760," right inside of the sigma activation and factor that out as well right factor that out as well"
4319760,4327280," uh right so let's say that um something like value i value i is inside of this thing right so let's put it"
4327280,4332320," in here so this is a value in here so this is a value i and now we're going to just take like all of this"
4332320,4339440," shit and replace it with value i and a boom and suddenly we can read this shit okay so that"
4339440,4344640," makes sense suddenly holy fuck and of course here we have to actually take the derivative of the inner"
4344640,4354000," of the inner thing in here right so uh there we go okay so and yeah parentheses are probably not needed"
4355200,4361600," so parentheses are probably not needed so now we need to take the derivative of vi with respect of w"
4361600,4368880," right and similarly as we did with ai let's actually move like define it like below vi right let's define"
4368880,4377680," it below the i so uh pd so what's cool about this entire process is that you can do it purely mechanically"
4378400,4387200," you can do it purely mechanically uh right so which is kind of nice though oh yeah some people say that"
4387200,4394320," maybe we should not defined this entire thing yeah that makes sense actually so this is entire like"
4394320,4400800," another idea that somebody suggested uh right so let's actually go back yeah maybe we didn't have to even"
4400800,4411360," do that yeah that's nice so essentially since we know what it is uh we can replace it with ai and it"
4411360,4418640," kind of works out as well uh right and the internal thing is going to be basically yeah that's even better"
4418640,4424880," thank you so much uh right because that actually does not like we wouldn't have to define that stuff"
4424880,4431680," anymore yeah thank you uh and now we need to find the derivative with respect to like uh this entire"
4431680,4439280," thing so the the variable is w so b becomes zero w becomes one so we're only left with uh x"
4440880,4445440," uh x i right so we're only left with x i"
4445440,4454160," so there i go and uh well i mean yeah so we have to remove the derivative"
4454160,4464400," there we go so the derivative of activation i is this value right and so that means we can finally"
4464400,4469520," substitute this with this value right so we can literally take it like in here"
4469520,4476960," and just put it in here uh right but we want to actually put it in a different place"
4476960,4489120," there we go cool so basically we found the first partial derivative of this cost function and it's"
4489120,4498640," equal to that and it's basically equal to that but since uh we have two parameters we have two parameters"
4498640,4504400," we have to do the same thing the same thing but with respect to b"
4504400,4515680," with respect to b luckily it's actually really easy right we basically can copy paste this entire thing"
4515680,4521440," up until here right and the only difference is going to be is just instead of w we have we're going to"
4521440,4528160," have a b in here right and let's take a look at how uh would you compute this entire thing with respect"
4528160,4534240," to b right how would you can speak the computer center i think with respect to b let's see so uh"
4534240,4542720," essentially you can differentiate like differentiate as usual up until here and here instead of w you're"
4542720,4547760," you're going to have b as a variable which means uh this entire thing will become zero and this entire"
4547760,4554400," thing will become one so essentially uh the derivative of this thing is going to be just this so this"
4554400,4561120," parameter just just the sigmoid without any you know coefficient in here so it's going to be just that"
4561120,4571440," which means that uh the partial derivative of c with respect of b is just that without any x or anything like that"
4571440,4578240," and uh there we go so we just computed uh both of the partial derivatives and this is the things that"
4578240,4583840," you have to subtract from both of your parameters of your neural network that consist of a single neuron"
4583840,4591200," we can probably um maybe even simplify all of that and remove all the intermediate steps in here because"
4591200,4596240," i don't think they are particularly useful uh in this specific case but maybe we can we can keep them"
4596880,4603600," uh right so and essentially yeah so now you have this thing that you can subtract from"
4603600,4608000," from the weight of a single neuron and this thing that you can subtract from a single bias"
4608000,4613040," of a single neuron uh right and that's how we compute the gradient of this entire thing so you literally"
4613040,4620000," have to just like go and compute that mechanically all right and once you've got that uh you can keep"
4620000,4625600," subtracting it from the parameters and it will just go there uh by the way if you are going to be just"
4625600,4632400," using one of the frameworks right uh right if you're going to be using one of the frameworks they are"
4632400,4638720," already have this implemented uh like for you you don't really need to understand how to do all that as"
4638720,4644000," far as i can understand you just like uh tell you like give your model to tensorflow and it will just"
4644000,4650160," automatically do the gradient descent on it right so here we're trying to just like go deeper into"
4650160,4657680," how to sort of like do your own gradient descent right and this is only a single neuron by the way"
4657680,4665200," right as soon as you add another layer of the neuron to your entire model it gets combinatorially"
4665200,4671280," more complicated right because now you you don't have like a single sigma in here you have two nested sigmas"
4672320,4677440," we have two nested sigmas and it can become kind of like difficult to compute the derivative or such a"
4677440,4682960," like a complex expression and this is where back propagation comes in right this is where the back"
4682960,4691440," propagation actually comes in which makes it like mentally easier to sort of like think about uh you"
4691440,4699920," know computer the gradient of this entire thing right okay so and as far as i know the model that corresponds"
4699920,4705600," to a single neuron that we have in here uh is essentially the gates right so we have this"
4705600,4713440," uh little thing that tries to model the or and and and and gates uh and we can try to implement d"
4713440,4720240," cost for this thing as well right so essentially what we what we did in here so here we actually have two"
4720240,4729840," weights surprisingly uh right but it's kind of easy to add uh like more weights in here i think"
4729920,4746800," uh uh so our model is actually a little bit more complicated so it uh actually has two inputs right so"
4746800,4754320," that means it has two weights but this is relatively easy to modify this entire thing to uh support uh two"
4754320,4761120," weights as well right so it's going to be just a simple simple change uh so but we're gonna do that"
4761120,4766400," after a small break right so first i want to make a small break uh i want to make a cup of tea and after"
4766400,4774640," the break we're gonna just try to apply this approach to this specific model right okay all right so let's"
4774640,4782000," continue right so let's continue let's continue let's continue so we actually have two inputs right so we"
4782000,4788960," actually have two inputs so we need to add uh these two inputs to our model right so let's go ahead and"
4788960,4795120," freaking uh do that so this is a gradient descent this is a twice model one neural model uh let's update"
4795120,4803120," maybe the the title one new model with two inputs right so now we have two inputs in here so to be fair"
4803120,4812000," by the way uh the weight is kind of related to uh to the connection rather than the neuron itself i put it"
4812000,4818560," inside of the neuron so maybe we should actually put it uh somewhere on the connection and i think it should"
4818560,4826400," be possible to do it like this so we do node um i don't quite remember but yeah so we're gonna do node"
4826400,4833840," and uh this is gonna be w right so that's how we're gonna do that uh let me try to do pdf gray grad tech"
4833840,4841040," and it kind of worked right so first of all it has to be mathematical w right so that's uh that's the"
4841040,4847200," requirement and it kind of has to be i think above the the edge right so it's going to be above"
4847840,4852720," if i remember correctly ticks uh yeah there we go so and since we're going to have two inputs in here"
4852720,4861120," right so that's specific uh that specific weight has to be like one right so this is the first weight"
4861120,4872320," um right so this is that uh epic epic epic epic so we have x but i think uh i need to now call it"
4872320,4879120," differently okay maybe not uh so i need to locate this entire stuff like on a different sort of plane"
4879120,4885600," maybe at an angle right what if i make it like higher right so because i want to have two inputs"
4885600,4893360," that go towards a single neuron uh yeah there we go that's that's a good one uh right and maybe now uh"
4893360,4901200," i would say that the output is actually z right so the output is actually z not x because i want to use y"
4901200,4907280," for the second for the second input right so this is going to be the second input so this is y but it's"
4907280,4917280," actually minus one uh all right so this is that and now we can try to connect right so the path uh like"
4917280,4929520," this y uh edge all right so this is node above above uh w2 so this is the second one towards the"
4929520,4936320," towards the neuron uh right and there we go we've got this thing isn't it poggers i think that's pretty"
4936320,4943280," freaking poggers so this is the actual model that we have in here isn't that cool uh okay so that also"
4943280,4952640," means that uh it has to be something like w1 uh plus y w2 and then the b so this is the actual model in"
4952640,4959840," here uh right there we go so this is the activation so let's update the rest of the stuff in here right"
4959840,4969280," let's update the rest of the stuff so x y uh w1 plus y w2 right so that's the actual thing in here"
4970080,4981520," um all right so i can probably put this stuff in here uh and then this stuff in here and just remove"
4981520,4991360," this entire thing so just like additional parameters right additional parameters uh and what's interesting"
4991360,4997520," is that the result of uh you know this derivative with respect to w is going to stay the same right"
4997520,5004480," because well i mean almost right since uh now we have two weights we have to kind of specify"
5004480,5009920," that when we're referring to partial derivative with respect to w it's actually the first one"
5009920,5016640," that we mean right so that's very important point in here right but the result is still going to say"
5016640,5022960," the same because if the variable is w1 so the constant that we're going to have in here is xi anyway"
5022960,5029440," and by the way for since we have like several samples it has to be actually something like this"
5029440,5037200," right so it has to respect the current sample uh since this sort of like repeats the pattern"
5037200,5043200," it should be super easy to compute the derivative with respect to w2 right so we can just like copy"
5043200,5050720," paste this entire thing uh right so let's put it somewhere here with respect to w2 and it's basically"
5050720,5058160," the same as this right it's basically the same as this but instead of x we're going to have yi"
5058160,5062080," right so we're going to have yi uh there we go"
5062080,5073040," uh all right so yeah so the first one is just sigmoid multiplied by xi uh the second one is just yi and b is"
5073200,5080080," just like nothing and uh because of that it is also super easy to compute the whole cost function"
5080080,5086160," with respect to two right with respect to two and instead of x we're going to put y in here so as you"
5086160,5092880," can see it starts to like actually sort of like display some sort of a pattern right so it's super"
5092880,5096800," because of this pattern it's super easy to just like add parameters in here right we don't have to"
5096800,5103200," recompute like um you know find the derivative over and over again we can just see these patterns and"
5103200,5106000," we can just add things in here uh right"
5106000,5121200," so uh let's go ahead and uh just do that"
5125200,5131520," uh so now we need to actually compute the uh you know the uh the gradient of the cost"
5131520,5137040," right so and the gradient unlike the derivative right so if you have a function of several variables"
5137040,5143200," uh that gradient has to return three values right so for each of this variable we have to return like"
5143200,5147120," a new value so that's what we have to do so the way we're going to do that we're going to have a"
5147120,5153840," function let's call g cost right so which is a gradient and we're going to just accept this entire"
5153840,5162080," stuff in here and uh the output effectively the output is going to be uh the the partial derivatives"
5162080,5166720," that we're going to have in here so we're going to actually return them as the pointers right so we're"
5166720,5172640," going to return them as a pointer let's denote them as d uh right so let's uh denote them as d"
5172640,5177120," so it would be kind of nice i want to kind of remove these intermediate steps"
5177120,5182160," right so kind of remove this intermediate steps but i'm not sure i think they're kind of useful just to see"
5182160,5188800," what exactly going on here so i'm not going to remove them uh okay so each individual partial"
5188800,5197360," derivative in here is basically a sum right so let's initialize let's initialize this thing like this so"
5197360,5202240," the first one is going to be zero uh the second one is going to be zero and the third one is going to be"
5202240,5209920," that so we're starting to iterate through all the samples right so uh zero less than n n"
5209920,5215360," is the amount of samples that we have in here right plus plus i"
5215360,5226720," so then we have things like x i uh x i y i and uh i think"
5228560,5236560," yeah i think i did a fucky walkie in here i changed the meaning of yi right why i used to mean expected"
5236560,5243200," value now yi means the second input right so it means the second input and the expected value in here"
5243200,5250080," is actually uh has to be called z right so okay we had to actually refactor here a little bit more than"
5250080,5256880," expected right so we have to refactor a little bit more so and essentially here uh not really not that much"
5256880,5267440," okay so that's actually pretty cool uh actually not that much so we can just query replace uh yi with zi"
5267440,5275760," except a single place which is in here yeah that's that's good so now this is what we basically have"
5275760,5282720," in here because the as i already said z is the output so the expected output should be like z in here"
5282720,5289600," uh but everything else is basically the same so the reason why it was so easy is because we factored out"
5289600,5294880," a lot of things so factoring out like like complex and repetitive things is actually very useful it's"
5294880,5300480," just like makes it a little bit easier to you know to modify if we just need to add a new parameter in"
5300480,5306640," here it's just like with the code right so math is basically programming without computers basically"
5309040,5314800," so yeah it's just like the computer is the human who's doing the math if you know what i mean so"
5314800,5319040," when they are verified that they think is correct they're basically evaluating and as far as i know"
5319040,5326080," there is like some sort of like a theorem that demonstrates uh you know similarity between proving"
5326080,5332880," a theorem and executing a code like something horvards something equivalents or something like that i'm i"
5332880,5338880," don't know anything about that because i'm not a math person uh right so that's basically what it is but"
5338880,5348480," anyway uh right so what we have to have in our code right is uh zi as well right and all of that is in the"
5348480,5355600," training data right so the train i zero so this is the first thing here uh and i'm probably gonna just"
5355600,5361440," copy paste this entire thing so this is going to be one and this is going to be uh two right there we go"
5361440,5371200," so and what we need to do we need to compute the first um activation i suppose basically right so"
5371200,5378160," we need to compute this ai all right so this is going to be ai and uh this is going to be sigmoid right"
5378160,5389440," so x i multiplied by w1 plus yi multiplied by w2 plus b so that's the current activation that's the current"
5389440,5401440," activation uh and uh then what we're doing we're doing w1 add um this thing so we do 2 multiplied by ai"
5401440,5410400," minus zi multiplied by ai again multiplied by 1 minus ai multiplied by xi so we're just like uh you know"
5410400,5415040," once we figured out the formula we just need to retype that formula into the code and that's going"
5415040,5424480," to be basically it right so for dw2 it's yi for b for b it's nothing so because b is just like one"
5424480,5429280," single thing without any coefficient so that means it's going to be one so and that's the entire thing"
5429280,5435440," and what's funny is that this thing is like repeating itself right this thing is repeating"
5435440,5445760," we can maybe replace it with something like d i all right so di equal well i mean that's uh that's"
5445760,5454800," going to be basically that all right so it's even simpler than that isn't it i think that's pretty cool"
5455440,5460000," so you may think that it's something complicated but when you write it down like in code it's just"
5460000,5465040," like yeah it's a simple thing it's only like you have to do a little bit of a math and reasoning to"
5465040,5470000," like justify all of that but at the end of the day like in terms of a code this entire formula step is"
5470000,5475920," actually super simple uh right but this is just a sum we need to do an average sum so that means we need"
5475920,5482000," to take all of these things right we need to take all of these things and uh divide them by n right"
5482000,5488640," so that's basically what we need to do uh right divide them by n and that's the basically the thing"
5488640,5494320," that computes the function yeah thank you so much uh well i mean the compiler would probably would tell"
5494320,5500480," me but i mean that's it's good enough uh right so that's pretty straightforward"
5500480,5511840," um okay so let's see so how the model works right so it's modeling uh something right so it's"
5511840,5519280," modeling something um we are basically have like a global variable train and we're just setting either"
5519280,5525520," xor or something else so we know that xor is not modelable by a single neuron so let's put and in"
5525520,5533520," here right let's put in here so as of right now uh as you can see we're just using the finite differences"
5533520,5539600," right so here is the finite differences and let's see how well all of that stuff performs right so let's"
5539600,5546080," say we're going to have like 100 iterations uh for this entire thing and this is going to be just the gates"
5547200,5553920," right this is going to be just the gates and yeah so this has to be dereferenced because we pass them"
5553920,5563040," by a point that makes sense okay so cost yeah cost didn't really well it cost goes down so here it is"
5563040,5569120," i would like to maybe move the cost as the first parameter in here in the log i think that would have"
5569120,5574960," been kind of useful right because it being the last parameter doesn't really make much sense"
5575680,5582560," right because i want to see this thing first right i just want to see this entire thing first i think"
5582560,5596240," it would make much more sense uh let's say go oh and there we go so in the cost it does go down"
5596240,5604080," right but let's now compare how quickly it's going to go if we're going to use the finite differences"
5604080,5611520," right so i'm going to do if zero so here we just define that which is fine uh maybe i can do the"
5611520,5621040," following thing i can do dw1 dw2 db and here we do not really do that right so we just like uh create them"
5621920,5629840," and if and here what we're going to do is just g cost we provide w1 w2 b and we get back dw1"
5629840,5639040," uh it has to be a pointer dw2 and db so that's what we get back and maybe furthermore we have to repeat"
5639040,5644800," this process in here so we can actually move it outside in here right so here we're just like differently"
5644800,5650240," computing all that right different computing all that um so maybe we could even move this entire"
5650240,5657120," stuff to a separate function right so something with a similar signature as g cost but actually d cost"
5657120,5662720," all right so it's going to be even more sort of like nice looking stuff"
5663680,5670080," uh so but this one is probably going to accept an epsilon that would have made sense"
5670080,5676080," so where are we going to accept the epsilon i suppose it could be something like first"
5676080,5680400," so this is the epsilon then the parameters and the dw in here"
5680400,5688080," so but here is also cost initial cost which we probably want to compute in here"
5689120,5697200," without any modifications without any special modifications right"
5697200,5711600," so and in here we can just do d cost uh epsilon um w1 w2 b d w1 d w2 db and just remove all of"
5711600,5716560," that so and we can flip flop between them so as you can see they kind of from the point of view of an"
5716560,5723040," interface they kind of similar approaches right so um one more time let's switch to this thing just to"
5723040,5728560," check that this entire stuff compiles and uh yeah that's how it works the code goes down but it still"
5728560,5734640," couldn't find the end i think it was the end what we're currently trying we're training the end so it"
5734640,5745040," didn't even come closer to the end so if we switch to an actual gradient that we computed ourselves with our"
5745040,5756400," bare hands will it go better faster stronger um not really right uh but it's actually i don't know so"
5756400,5762080," we're moving with the same rate right so let's actually maybe do a thousand of these things uh after"
5762080,5770160," a thousand it went relatively well i think right it went relatively well um right if we switch now to uh"
5770720,5778560," finite costs uh it's it's actually with a similar performance actually surprisingly right maybe there"
5778560,5786000," is no even point to do all of that uh but as soon as you start to uh do more complicated models uh doing"
5786000,5791600," finite differences is going to be very imprecise and very computation inefficient because you like need to"
5791600,5799040," compute the whole function but actually this is kind of interesting maybe uh maybe like finite differences"
5799040,5802640," uh are actually very good approximation if you think about it"
5802640,5806640," until you need something more precise"
5806640,5820000," um all right thank you so much uh so let me take a look at the name uh last at the last at the vampire"
5820000,5825680," thank you so much for for twitch crimes thank you thank you thank you uh maybe it is i/o bound by writing to the"
5825680,5830640," custom maybe uh we can actually check it out so we can compare the actual time performance right"
5830640,5839200," maybe that improves it right maybe that improves it uh yep yep yep so let's go in here and i'm gonna do the"
5839200,5847760," gates uh right so and the time it takes is actually 110 seconds roughly well i mean it's pretty fast right it's"
5847760,5855840," pretty fast so if we switch to uh well i mean isn't that finite differences right it is basically finite"
5855840,5860240," differences uh let's just recompile all of that"
5860240,5870160," uh right and this is finite differences relatively fast uh if we switch to the gradient to the actual gradient"
5870720,5877680," uh uh it's actually slower surprisingly it's pretty much the same it's pretty much the same um right"
5877680,5882960," it is pretty much the same uh so but maybe if we"
5884800,5896080," get even more epochs right it's gonna be faster let's just like one around one second and the cost is like"
5896080,5902560," zero zero five right the cost is zero zero five uh finite differences"
5905920,5912960," uh yeah it's around i'm actually surprised that the performance is basically the same"
5912960,5918080," the performance is basically the same between the uh you know the gradient"
5918080,5923920," and finite differences at least on such simple models at least on such simple model"
5927440,5938880," and this is very interesting i think"
5938880,5950560," it actually depends sometimes it's faster sometimes it's slower it's gonna hurt uh all right so uh okay"
5950560,5962400," that's cool uh but how can you compute uh the gradient if you have two neurons and not only two neurons but"
5962400,5968800," the ones that are also nested because this is actually very interesting right uh this is actually"
5968800,5974880," very interesting so let's actually go here right so we're gonna have uh so one neuron model right"
5976000,5982240," and here we're gonna have two neurons model with one input right so let's actually not"
5982240,5989520," make everything too complicated let's actually keep it at one input right so let's actually try to draw"
5989520,5996320," that specific model that i mean uh right with ticks and stuff so that's something that probably won't have"
5996320,6001440," so we're gonna copy paste the current model"
6005440,6012880," and there we go so i i wanted to actually the pdf later grab tech"
6012880,6022320," uh and refresh uh there we go so this is just like one single neuron uh and in here let's get rid of y"
6022320,6029760," all right let's get rid of y and this is going to be just like that and x let's keep x at zero right so"
6031200,6036320," this is the first thing this is the first thing uh and now"
6036320,6044080," what i'm going to do i'm going to create this second neuron right so we're going to rename this first"
6044080,6051040," neuron to neuron one so there's three references to this name we're going to introduce neuron two we're"
6051040,6058400," going to place that neuron at i suppose at d right because we already have this sort of like a distance in"
6058400,6064880," here which is kind of convenient in my opinion right so it is kind of convenient uh but it's too"
6064880,6070240," yeah so it doesn't take into account the padding right it doesn't really take into account the padding"
6070240,6078480," and by the way by the way from neuron one to we have to move to neuron two right so let's do neuron two"
6078480,6086160," like so uh so that we'll put it in here and z let's actually put z at twice of the d"
6086160,6091280," twice of the d uh there we go and we're going to move the z"
6091280,6103440," path we're going to have an additional path from n2 edge towards the z and in fact i would like to rename"
6103440,6108240," the z to y because we don't have several inputs in here so this is going to be just y and this one"
6108240,6116560," is going to be just y uh yep there we go so that's what we're having here so we're gonna have two weights"
6116560,6124160," in here right we're gonna have two weights in here uh so this is the first weight and then on the edge"
6124160,6131520," between n1 and n2 we're gonna have another one which is the second weight right which is the second one"
6131520,6138160," there we go so this is that so the first weight second weight and in terms of uh"
6138160,6147280," biases also we're gonna have here uh the first bias and the second bias um yeah there we go"
6147280,6152880," so that's basically a different model though between two neurons i would actually like to have a"
6152880,6159760," little bit more distance if you know what i mean um right maybe that's fine yeah maybe that's fine"
6159760,6164800," we're gonna keep it like that so here is an interesting thing um so there is a little bit of a confusion"
6164800,6173440," there is a little bit of a confusion between a subscript in this model and a subscript in this model"
6173440,6179680," right and it's this kind of confusion i think is kind of important because in this model subscript of w means"
6180240,6190640," the input the weight of the input right but here subscript means the layer right it's a weight of"
6190640,6197040," first layer and the weight of second layer here it's a weight of first input and the weight of second"
6197040,6202080," input and i think this kind of distinction is actually kind of important right and because of"
6202080,6207760," that i kind of want to denote this kind of stuff slightly differently right what if instead of using"
6207760,6212800," subscripts we're going to use superscript when we denote the uh the specific layer we're going"
6212800,6217600," to use a superscript i kind of got this idea from through low and brown from his neural network series"
6217600,6222320," uh right and i think it's kind of useful right so essentially we're going to denote it like that"
6222320,6230720," so as you can see right so it's a little bit more like bigger and verbose but i think it conveys conveys"
6230720,6239840," the can conveys conveys conveys the point so basically superscript is the layer and subscript is one of the"
6239840,6247120," inputs or one of the samples right because uh here we use subscript to indicate like one of the samples or"
6247120,6253600," something like that so i think yeah i would like to basically refactor those things to be like that and"
6253600,6262560," i put parentheses around the superscript did you note that uh this is not um um sort of the power right so"
6262560,6269440," it's not the power of things i think it's also quite important because it may lead to confusion right it"
6269440,6275600," may lead to confusion yeah yeah so i need to actually add three blue and brown to the references right so"
6275600,6283920," three b one b and n playlist uh playlist and it's going to be done so i'm going to put it in in here"
6283920,6290240," as well so thank you thank you for telling me that uh right so this one has to be one right so this is"
6290240,6299280," going to be one this is going to be two uh oh my god so we need to have a like a bigger difference in here"
6299840,6308960," so okay what if i just increase the d right so where is the d uh i don't see where it's defined"
6308960,6318160," okay so let's say it's going to be maybe two uh yep i guess it's fine so the only thing i get"
6318160,6325680," about if whether it's readable or not and this is where it becomes kind of interesting right because"
6325680,6337600," essentially right essentially y is equal to right and first you have to take x and you have to apply"
6337600,6344160," the uh the weight of the first uh layer right so applying the weight of the first layer and add"
6344160,6351440," the bias of the first layer and then you apply the sigmoid to activate uh it's actually sigma in here"
6352400,6358000," then to that so you activated the first thing you activated the first thing to that you need to"
6358000,6369040," multiply w uh of the second layer and then the bias of the second layer and activate that again"
6369040,6377120," that's what you need to do you need to activate that again and that is the model of this entire thing"
6378240,6386560," we just added another layer and now we need to compute a derivative of that and that looks"
6386560,6392080," actually kind of daunting but there is a trick that makes it a little bit easier actually"
6392080,6403360," oh so we got another sub thank you so much for um aluminium aluminium aluminium aluminium"
6404160,6410560," sorry i'm an idiot thank you so much for twitchpan subscription uh right so essentially what we can"
6410560,6418960," do right what we can do we can factor out uh some of the layers right so remember how we in in the"
6418960,6424240," previous models we actually took the entire sigma and we factor it out into like separate activations"
6424240,6430560," what if we do the same in here right what if we say that this thing is basically an activation at the"
6431920,6437360," at the layer one it's a layer one activation uh right so it's a layer one activation and it's just like"
6437360,6445040," this uh suddenly it becomes kind of more readable right and it just like chains really really well if"
6445040,6453040," you know what i mean right it chains really well and you can think of i why as actually activation of the"
6453040,6464960," of the second layer right so you see something interesting is going on in here right something"
6464960,6471040," interesting is going on in here and here i can probably even uh actually replace y with uh you know"
6471040,6478240," activation of the second layer just to you know be consistent with all that uh right so and for the"
6478240,6485280," sake of consistency we can also rename x as an activation of zero slayer and it's sort of like"
6485280,6490240," a special case but let's not do that right so yeah yeah so some people already say that and x is"
6490240,6495600," activation of the zero slayer and the brain actually automatically tries to like find the patterns and"
6495600,6501840," generalize this into recursive formula doesn't it right so this is actually very interesting right"
6501840,6507200," as soon as you just like start uh factoring it out you can see instantly part okay this is recursive but"
6507200,6513680," let's not jump ahead of ourselves right so uh let's not jump ahead of ourselves anyway so"
6513680,6521360," essentially uh let's try to compute the cost right so let's try to compute the cost even though this"
6521360,6529360," thing uh is like looks daunting right but if you approach it like very slowly and methodically it actually"
6529360,6536800," like breaks down into like simpler and smaller problems quite easily uh right so let's go ahead and just"
6536800,6543120," create sub subsection right not mark but just subsection and this is the cost uh right and let's"
6543120,6554640," start with the cost and here is an interesting thing um so computing the cost for first w2 and b2"
6554640,6563680," is easier than w1 and b1 right because in case of w2 so we need to take a derivative with respect to this"
6563680,6569280," specific variable so that means this entire thing becomes a constant so we can just like throw it"
6569280,6576000," off and we don't have to go in like deeper so uh computing this kind of like outer variable first"
6576000,6584080," is a little bit easier so we're gonna have a cost right so and the cost is essentially just like average"
6584080,6594160," some right so we have several uh samples in here uh from i to n right for from z from 1 to m and essentially"
6594160,6602480," what we do we take the outer activation the yeah so the outer activation but activation in our specific"
6602480,6611600," case also has a sample attached to it right a sample attached to it so uh because of that i want to actually"
6611600,6620080," define this thing twice so i also want to have like a uh subscript in here right so because this is an"
6620080,6628960," activation with respect to ith sample it's activation for the ith sample so this is quite important so"
6628960,6636480," we have two nested loops in here basically right in the cost function uh right so if we have something like"
6636480,6649840," cost uh cost uh like cost function we are first iterating samples uh right this is the outer function"
6649840,6656160," we're first iterating samples and for each sample we are iterating all of the layers"
6656160,6663520," so that's basically what we do and that's why i kind of pass two things in here so the subscript indicates the"
6663520,6670000," the current sample outer loop and the superscript indicates the the layer inner loop but we haven't"
6670000,6674320," defined the inner loop yet right so we just like do it like that but i think it's quite important"
6674320,6681520," because we basically pass that i to to x in here right so we pass that i to x and we pass it everywhere"
6681520,6687920," so w doesn't have a subscript right because it doesn't depend on the sample right and b also doesn't"
6687920,6694880," depend on the sample i'm gonna get right so so here this has nothing to do with sample the samples right"
6694880,6702560," it has nothing to do with samples it's just like a demonstration but to be fair all right we want to"
6702560,6711440," denote yi as an expected value of activation of the ith activation so that's basically what we want to do in"
6711440,6719520," here uh right so let me actually go try to recompile this intense step so uh"
6719520,6728240," yeah so this has to be where am i i don't see i don't see in this mist i forgot to put a new line in"
6728240,6735600," here that's why that's why we can't see in this mist let me go so here we basically take the difference"
6735600,6743280," between an actual activation for the x i and the expected value so and that's the difference between"
6743280,6750720," the activation of the second layer and y y is sort of an expected value and i'm really sure like in this"
6750720,6757120," specific example like in here where we define sort of model maybe i want to keep this thing as y just in"
6757120,6763200," case right i think it makes a little bit more sense so i'm going to go back to y all right"
6763200,6770800," yeah so i think that makes a little bit more sense but again we're trying to be sort of rigorous or"
6770800,6777120," something like that uh okay so this is basically the uh the cost function right so this is the"
6777120,6783600," cost function nothing particular special so as i already said it's a little bit easier to compute"
6783600,6788640," it's a little bit easier to compute the partial derivatives for the outer variables in here uh"
6788640,6793360," so the the weight of the outer function and the bias of the outer function so let's quickly do that"
6793360,6798800," let's first do that so this is going to be the weight all right so this is the weight"
6798800,6805600," for the c so we don't really know what it is yet and then the b there we go"
6807680,6815520," okay cool so what are they going to be equal to what are they going to be equal to um we can kind of"
6815520,6825680," predict by effectively taking this entire sum just putting it in here and saying that this is going to"
6825680,6838960," be that so let's go uh you know step by step in here right let's go step by step so this is a chain rule"
6838960,6847200," we take the derivative of the outer function like so so this is going to be just two this is going to be"
6847200,6854320," just two and then we take the derivative of the inner function which i can't really see uh yeah so the inner"
6854320,6860960," function doesn't include the this thing so then we can remove the parentheses this is just a constant"
6860960,6866400," this is just a constant and now we just have this entire stuff uh and there we go i suppose we are"
6866400,6875200," ready yeah there we go so there's a lot of like uh superscripts and subscripts and stuff like that but"
6875200,6881840," it is what it is and it isn't what it isn't it's kind of difficult to uh you know make it less job security"
6881840,6889280," if you know what i mean so now we need to find the partial derivative for the weight of the second layer"
6889280,6895840," for the activation of the second layer right uh for the activation of the second layer so here is the"
6895840,6903840," definition of the activation of the second layer so let's try to put this entire thing in here right so how is it going to look like"
6903840,6909840," okay that's the real question that's the real question so we can just put this stuff in here"
6909840,6916720," it's kind of similar to what we were already doing before it is kind of similar uh right so"
6916720,6925440," this is sigmoid right and that means uh that the derivative of sigmoid is sigmoid multiplied by"
6925440,6932480," one minus sigmoid so that means we can just take this entire thing and multiply it by one minus this and just take the"
6932480,6940560," the derivative of the inner function like so uh right there we go so and then when we're taking"
6940560,6946400," derivative of this expression with respect to the weight of the second layer so the weight of the"
6946400,6952960," second layer is in here so the actual value is the activation of the previous layer right so what we"
6952960,6960800," end up with in here is the activation of the previous layer uh right so we can put this stuff like this"
6960800,6968880," and there we go so this is the derivative of that so it's the activation of the current layer this thing"
6968880,6974800," the sigmoid thing and then multiply it by activation of the previous layer you see how things start to"
6974800,6979920," chain with each other which is how the things start to chain with each other it's actually kind of cool"
6979920,6987120," uh right so there's another thing that we can quickly compute it's the uh you know partial derivative"
6987120,6991520," for the bias it's basically without anything because bias is just like a single thing in here so it's going"
6991520,6997760," to basically compute to one so and we already have like everything in here so we have both of these things"
6997760,7004000," right we have both of these things so that means in here we can quite easily just replace this entire"
7004000,7013520," partial derivative uh with this thing all right so with this entire thing and basically call it a day"
7013520,7018640," well i mean for for that partial derivative at least so and this is what we ended up with"
7018640,7024880," right this is what we ended up with so it's just like the usual thing plus multiplied by activation of"
7024880,7031280," the previous layer right activation of the previous layer we can even probably get rid of all of the"
7031280,7035840," intermediate stuff because i don't think it's particularly useful for that specific case or maybe"
7035840,7041840," it is useful i don't know uh right so let's keep it let's actually keep it"
7041840,7052480," okay so for relative to b it is super easy to compute because it's literally this right it's literally this"
7054240,7056960," but we just don't have this thing in here"
7056960,7068000," did it crash okay it just it's it's literally crashed uh okay we're doing the pdf one more time"
7068000,7077840," okay so uh the first derivative the first partial derivative for the outer variable for the"
7077840,7083600," weight of the second layer is just this and for the bias of the second layer just this it's kind of similar"
7083600,7089760," to uh whatever we had with the neuron uh with the single neuron right so it's just basically that but"
7089760,7096240," instead of the previous activation there it's just like uh straight up y uh well it's it's a wrong"
7096240,7099360," it's a wrong thing so there's nets"
7099360,7108720," well yeah y is a like x or y they are the inputs right so they're basically previous activations yeah so"
7108720,7116640," they're basically previous activations so but now we need to compute shit like you know uh grad attacks"
7116640,7124960," weight for the inner like a partial derivative for the inner weight so an inner weight is located like"
7124960,7128080," deep inside like very very deep inside so i'm going to show you"
7129440,7139760," um so let's recompile this into i think right so if you take a look so here is the activation of the"
7139760,7145360," second layer then you go inside activation of the first layer and in the activation of the first layer"
7145360,7153280," only then you see w1 right so it's kind of difficult to go there it kind of becomes"
7154560,7162800," very tricky to go inside because you have like one layer in there right um so there is an interesting"
7162800,7168560," idea that it's kind of difficult for me to express but the idea is that you basically"
7169600,7177280," have not not just a single cost function but separate cost functions you have a separate cost"
7177280,7185280," function for each an individual layer and for separate specialized cost functions for each layer"
7185280,7192080," you can only compute the variables that are nearly accessible like like in this case for for this cost"
7192080,7197520," function it's like easy to to compute this variable and this variable but it's difficult to go inside"
7198000,7202800," right so we can just basically introduce the following notion okay so the cost function"
7202800,7208400," for the layer uh two right so this is the cost function for the layer two and this is partial"
7208400,7217600," derivatives for the layer two uh right okay so what does it give us what exactly what are we trying"
7217600,7223680," to achieve with that uh right so if for this specific cost function for the second layer"
7225680,7233200," right we can compute uh partial derivatives for weight and bias can we compute partial derivatives for the"
7233200,7241040," activation of the previous layer so essentially we computed the partial derivative for weight for bias"
7241040,7247120," and we can't go inside of the activation but what if we treat activation of this thing as a variable of"
7247120,7257600," that cost function and just like differentiated what would we get we get basically an error a difference"
7257600,7264960," right so the expected value was that but we expect it to be different by that much we're computing an error"
7264960,7273360," that we can then use to compute the cost function for the inner layer and then another difference and for"
7273360,7278240," another layer and so on and so forth and it is where the idea of back propagation comes in"
7278240,7285680," does it make sense so you yeah somebody says in the chat so we have series of cost functions so we have"
7285680,7291360," a cost function for the first layer and then you compute like what are actually expected activations"
7291360,7297840," for the for the previous layer okay based on those differences we can see another like layer of cost"
7297840,7304880," functions and so on and so forth until we go uh to the input and as we go we basically complete partial"
7304880,7310560," derivative for for all of the weights and biases and we know how to update them so and like we're"
7310560,7317040," basically propagating this entire thing backwards does that make sense it's the thing with this entire"
7317040,7325360," stuff is that it's very tricky to explain uh right because there's a lot of like layers of complexity in here"
7325360,7332080," it's not particularly uh difficult concept it's just like a lot of things that you need to keep track of"
7332080,7339040," and the only way you can actually get a grasp of that is by literally doing it yourself right so essentially"
7339040,7345840," get that idea of okay so here's the cost function for that complicated model and just like literally"
7345840,7354560," go ahead and try to take partial derivatives for that big cost function right and as you do that you"
7354560,7360560," kind of learn you kind of discover this recover recursive structure structure you basically need to do it"
7360560,7365760," yourself to actually uh you know comprehend that uh and if you don't want to do that you can just like"
7365760,7372000," use one of the frameworks or something like that but uh here i really want you to understand this backpropagation and"
7372000,7378560," it's kind of it's kind of beautiful in a way right anyway so let's actually try to go ahead and uh"
7378560,7386000," right and it's totally fine it's if it's not clear right it's totally fine if it's not clear like what"
7386000,7391760," exactly is going on in here because it's it's genuinely complicated you know uh topic right so it's totally"
7391760,7400560," fun i i tried to understand this thing for like three days straight for three days straight i was just like"
7400560,7407120," basically writing rewriting and rewriting this specific derivative until i actually like found the"
7407120,7412880," structure that i can comprehend and it just like conceptualized uh but essentially yeah let's try to"
7412880,7422240," go ahead and treat this entire like previous layer uh the previous layer as the variable so that means"
7422880,7429600," we're going to have uh we're going to have uh the cost function for the second layer but the partial"
7429600,7438640," derivative is going to be activation for the ith sample but for the previous layer right so that's what"
7438640,7446880," we're doing for the previous layer so that means that means that we need to have in here you need to have in here"
7448080,7457840," you know a partial derivative for this specific thing but for a i of the previous layer right so"
7457840,7463440," how sensitive the activation of this activation is to the activation of the previous layer"
7463440,7470960," uh all right and this is rather interesting so here we supposed to uh have the coefficient the"
7470960,7478560," the corresponding coefficient so if the a i is the variable so that means the coefficient is going to"
7478560,7484640," be the weight so here we have uh basically the weight of the second layer so this is the weight"
7484640,7494320," of the second layer so and let's actually go ahead and uh just do that there we go so as you can see like uh"
7494960,7500880," weight was not really a coefficient it was the previous activation but now it is a coefficient like that"
7500880,7506320," so that means that that specific partial derivative is going to have"
7506320,7517280," this thing with the weight of the current layer as the coefficient"
7519040,7528400," there we go and here is an interesting thing uh here is the interesting thing now we need to"
7528400,7535840," define so this is the um you know the cost function for the second layer so this is a cost function for"
7535840,7540240," the second layer what's going to be the cost function for the for the first uh for the first layer"
7541920,7547680," right so this one is rather interesting actually what's going to be that so we need to have an"
7547680,7551040," average sum right so this is definitely going to be an average sum so that's for sure"
7551040,7562160," uh right and we have the activation of that first layer but what's going to be the expected value"
7562160,7568720," right we we have expected value for the second last layer but what's the expected value for the"
7568720,7574240," inner layer for the activations of the inner layer right so here is the uh thing so we know the"
7574240,7582400," expected value for this thing but what's the expected value in here what is it what is it i don't know"
7582400,7589440," we don't know the expected value of that thing but we know the difference between the expected"
7589440,7594480," value and an actual value and what's the difference between an expected value and an actual one"
7596240,7602880," it's this partial derivative actually we just computed that difference we just have it in here"
7602880,7609040," so here we can probably do something like minus uh expected i right let's put it this way right so"
7609040,7616000," let's say expected i so when something went horribly wrong so let me let me see so it's a"
7616000,7624880," just uh this thing right it's just this thing uh there we go okay so expected i and let's just"
7624880,7632560," like define this expected i what the hell is that expected i it's just basically uh essentially"
7632560,7642240," uh the current activation right so the current activation uh minus right minus the current"
7642240,7648480," activation again minus the difference that we just computed so and the reason why i'm doing it like"
7648480,7653280," that of course obviously if you just like open the parentheses you're gonna end up with that specific"
7653280,7659200," difference but i want to keep the general structure of this cost function similar to the cost function"
7659200,7664960," of the second layer so we can see the similarities we can see the similarities so i think it's it's kind"
7664960,7673520," of useful to actually think about it like that right i do think so um right so well i mean why is it yeah"
7674080,7681600," so it has to be actually the first activation so this is the first activation and so on and so forth"
7681600,7692000," all right so this is what we have okay so we have the cost function for the inner layer right and so"
7692000,7702320," since it works with the inner layer it is super easy to reach all of these variables like weight of the first layer and the bias of the first layer"
7702320,7708240," right so and let's just literally go ahead and do that right so let's partial derivative of weight"
7708240,7715200," of the first uh weight of the first layer right so this is going to be the first thing that we'll need to compute"
7715200,7721760," might as well like just literally pre predefined them so the bias of the first layer and that is it because"
7721760,7726960," we don't really have any more previous activations right so that's basically the only things that"
7726960,7733760," we need to compute in here right that's the only things we need to compute in here all right and"
7733760,7742560," let's go ahead and try to tackle uh this entire thing so it's going to be sort of like this"
7746160,7747600," so i'm going to copy paste this stuff"
7747600,7754160," uh-huh so this is the right and this is the left"
7754160,7760640," so i just want to see this huge huge formula right not really huge formula but the parenthesis i want"
7760640,7768480," to see the huge parenthesis and uh let's just go inside like i want to go step by step so we can see"
7769120,7773200," how this entire thing is going to work out right hopefully"
7773200,7783600," so this goes inside and we need to do the usual thing with the chain rule right so we'll have to"
7783600,7791440," take this entire thing right take this entire thing and put it outside right put it outside and"
7791440,7798720," simply compute it like that uh right and then we need to take the derivative of the inner thing so"
7798720,7805520," this difference this thing is a constant so we can just straight up remove this entire thing right so"
7805520,7810720," we can straight up remove this entire thing uh right and this thing becomes one of the things that"
7810720,7817200," we just need to compute at some point uh somewhere up there right so it's basically the partial derivative"
7817200,7824880," for the first way of the first activation layer uh right so and here is an interesting thing in here"
7824880,7828880," right so we can simplify this expression because we know what"
7828880,7833840," ei is equal to ei is equal to this specific expression right"
7833840,7842320," uh ei is equal to this i'm i'm an idiot actually i should have not put this stuff in here"
7843040,7850240," right so it should have been just like that so ei was supposed to be just like the current activation"
7850240,7856560," minus this thing and the reason why i supposed to do it like that is because when i'm basically"
7856560,7865440," shoving it into the eye down there like down there it's going to play out very nicely look at that"
7865440,7869680," right so you have something like this all right so we have something like this"
7870320,7876320," you open a parenthesis so this becomes plus so let's remove one of the parentheses"
7876320,7884160," so two numbers we subtract from each other they become zero right so the effect will become zero"
7884160,7892320," and what you end up with in here right is the derivative that we computed in the previous step"
7893200,7902160," right so here's the thing uh for the cost function of the first layer this is the difference"
7902160,7908720," this is the difference but for the cost function of the first layer the difference is the derivative"
7908720,7915680," that we computed on the previous layer for that specific thing right so and this is how they"
7915680,7920640," chain together this is how they chain together so but the only thing we need to have in here is just like"
7920640,7927760," uh the uh the weight derivative for for the first layer right the weight derivative and it's going"
7927760,7933680," to be kind of similar to the previous layer here it's just like it's going to be one layer lower right"
7933680,7939840," so we i'm going to just copy paste it in here and while a layer lower lower what do we have uh we just"
7939840,7947680," have this previous activation in here this previous activation in here and here activation of the layer zero"
7948480,7955600," and the activation of the layer zero is x i as we already said so it already kind of generalizes kind of"
7955600,7961440," kind of nicely right so it already generalizes kind of nicely so for that for that first layer"
7961440,7966560," uh all right it's going to be it's supposed to be like a previous layer but previous like zero's layer is x i"
7966560,7972320," and uh for the bias bias is just one we're just not going to have anything in here"
7972880,7977360," we're just not going to have anything in here and there you go so that's pretty pretty straightforward"
7977360,7988000," so that means here uh this is going to be just x1 right so this is going to be x x x i uh and this thing"
7989680,7993760," is going to be is going to be just that"
7993760,8006560," okay and essentially we computed partial derivatives for all of the four variables"
8006560,8015120," right uh actually we computed five partial derivatives so we computed partial derivatives for the first layer"
8015680,8019680," then an intermediate partial derivative an intermediate partial derivative"
8019680,8025680," uh intermediate partial derivative so we can can compute the rest of the partial derivative for the"
8025680,8031360," inner layer and if we had three layers we would have computed another intermediate partial derivative"
8031360,8037440," to go even further and further and further and further so we can try to"
8038320,8043600," somehow generalize this entire thing right so we can try to generalize this entire thing"
8043600,8048400," to arbitrary amount of uh what clears um"
8048400,8055680," so but we're not going to have several inputs yet right we're not going to have several inputs yet"
8055680,8069760," so uh several uh arbitrary or arbitrary neurons model with one input one inputs one inputs so uh here"
8069760,8075840," all right i'm going to do align and what i want to do i want to actually grab all of these definitions"
8075840,8082720," right but i want to uh sort of get read intermediate steps because they kind of get"
8082720,8089200," in a way right and it don't really help right so we're going to just basically remove all of these"
8089200,8094880," intermediate steps so we can clearly see all of the necessary patterns that we then need to generalize in"
8094880,8103760," view uh okay so here we have some sort of a forwarding obviously we can generalize this entire forwarding"
8103760,8115040," right so if we um we can even split this entire thing into two sections right so the first section"
8115040,8122880," here we're starting from the uh like the um the input layer towards the uh output layer so essentially"
8122880,8128960," here we define forwarding so we're forwarding things uh you know forward uh and then here when"
8128960,8133440," we're competing the cost function we're competing things backwards right so we're competing things"
8133440,8140960," backwards um so in here there's something weird i just copy-pasted i think i copy-pasted too much so"
8140960,8146960," you're not supposed to have anything in here so did i yeah yeah i forgot to remove some of this stuff so"
8146960,8152320," it doesn't make sense so uh what i want to do i want to split this entire stuff right so here"
8152880,8163040," we're going to have end a line and here begin a line so in here we're going to have a sub subsection"
8163040,8173600," uh feed forward feed forward uh this is the feed forward and sub subsection back"
8176160,8188000," okay so let me take a look at how it will went right so this is a feed forward right so we basically"
8188000,8193920," define activation starting from the input towards the output and back propagation we are computing the"
8193920,8199680," partial derivative starting from the output layer towards the input layer you're going to share the pdf yes"
8199680,8206640," of course i'm going to show it at the end of the stream right so yeah here we're defining activations"
8206640,8212000," so essentially how can we generalize this entire stuff right so i can just take this entire thing"
8212000,8224080," and i can basically replace right so if i is the um iterator over samples let's for the layers say we're"
8224080,8229120," going to have l right i think it makes sense l is the current layer so let's assume that for this specific"
8229120,8236240," section the current layer uh two so that means we're going to replace it with l right so this is basically"
8236240,8249600," l so that's what we have in here and layer one is the previous layer l minus one there we go so and"
8250560,8258080," then we need to have some sort of a definition right uh for like a special case of some sort we can say"
8258080,8269840," that let's assume that a i zero is"
8271920,8282480," x i and that's basically our recursive definition of fit forwarding right so the activation of the l"
8282480,8290720," layer for the ice sample is uh activation of the previous layer multiplied by the weight of the"
8290720,8297120," current layer plus bias of the current layer uh pushed into the sigmoid apply with a sigmoid apply"
8297840,8304480," right and then you go backwards and for that layer it's the same until you hit a zero slayer which is"
8304480,8309840," xi and this is very much recursive definition and for each of these things this is how you compute"
8309840,8316480," their partial derivatives right as you can see you can basically derive this uh you know generic recursive"
8316480,8323040," definition through whatever specific cases we had in here let's actually try to do the same uh the same"
8323040,8330160," thing for the back propagation right so we have this thing and let's try to uh also define it recursively"
8330160,8337200," right but this one is going to be a little bit more difficult uh right a little bit more difficult"
8337200,8344560," because that difference that difference is basically this right so it would be kind of nice"
8345440,8351600," it would be kind of nice to like replace that difference with this sort of like right away"
8351600,8358320," right sort of right away so we don't have to have that uh-huh"
8358320,8366160," okay so that's pretty cool and here is the thing so you can see"
8367600,8373520," that to compute the cost function of the current layer well let's assume the current layer is one"
8373520,8378960," you need the derivative from the uh not really previous layer but from the next layer"
8379520,8386640," right so it's derivative from the next layer to generalize uh this entire sort of recursive approach"
8386640,8395680," we can say that this difference is actually derivative from the layer c3 from the layers three"
8395680,8403200," we don't have the third layer we don't have the third layer but we sort of imagine that we do and that"
8403200,8410160," error difference is the derivative from that imaginary third layer and imaginary i'm not using the word"
8410160,8414880," imaginary here in terms of like complex numbers or anything like that i use it in the sense of what"
8414880,8420400," we're imagining things we're just hallucinating things so we have a last output layer we're just"
8420400,8426160," hallucinating that there is another layer there and that difference is basically that that's basically"
8426160,8435360," what it is right so that um you know this uh derivative gives us the difference for the previous layer"
8435360,8439920," and we already have the difference from the next layer that we computed sort of manually that makes any"
8439920,8448000," sense so we can kind of say that um all right uh let's assume that we're going to have m layers"
8448000,8460400," right so we probably want to assume something like that uh let's uh assume that we have m layers"
8460400,8467920," right so we have m layers i don't know why i put the semicolon here but yeah so and essentially"
8467920,8473520," when we are talking about that specific difference right so that specific difference"
8475360,8483440," we usually have the following thing so this is the current layer and the cost is from the next layer"
8483440,8492960," so we're gonna literally replace l in this specific case with m so this is sort of like a difference"
8492960,8505120," from from this thing so let this let's denote this specific difference this specific difference"
8505360,8524400," uh right as this right or maybe even yeah like this let's denote it like that uh all right which will"
8524400,8532560," allow us to do pretty interesting shits so in here i'm query replacing one with the current layer so this is the current layer"
8532560,8549840," um boom boom so then uh yeah two is the layer plus one yeah the layer plus one and now here what we have"
8549840,8558000," so the current layer is actually two now for this specific case uh and one"
8559200,8568320," one minus one i hope i hope i didn't make any mistakes yeah so that's roughly how we can generalize this entire thing"
8568320,8575280," right so here we have for the previous layer and for this thing we sort of continue like that and i think"
8575280,8586800," that's basically the whole formula for backpropagation right so and the idea is the following right so you have some sort of data right"
8586800,8593040," so you're computing all of the activations and then all of the necessary like uh differences and stuff like that"
8593040,8601040," and then you're going backwards starting from the last layer and just like applying that so you compute these uh two things"
8601040,8604240," like the the variables of the current thing and here is"
8604240,8607200," oh god damn it so i still have to put"
8608160,8614720," yeah i forgot about some some stuff in here right so this one uh has to be l"
8614720,8620880," so some of the indices here is a little bit up because i was not to uh"
8620880,8626400," how to say that right i just like missed some of them"
8626400,8631200," but it could be easily fixed in the future but yeah so that's basically the recursive formula"
8632560,8639120," which looks kind of scary it does in fact look kind of scary but this is because we're chasing too"
8639120,8646800," many indices in here we're chasing too many indices so we're chasing first the sample indices and then the"
8646800,8657200," layer indices right so but that's not even enough that's not even enough because here we have only one single"
8657200,8664000," input and single output for each individual neuron right single input a single output"
8664000,8669600," what if we had more complicated neural network with multiple inputs and multiple outputs"
8669600,8675280," that will become even more interesting i think so i don't really know why i closed it uh"
8675280,8678560," right because i wanted to draw like i didn't really have to close this intense stuff"
8680000,8690160," so uh right so essentially to actually perform this entire idea algorithmically to uh to perform this"
8690160,8696960," entire idea algorithmically um let's write some sort of like very like simple neural network so we're"
8696960,8704160," gonna have something like this maybe four uh things in here right there may be two things in here"
8704160,8709280," right so let's fully connect this entire thing let's fully connect this entire thing"
8710000,8717520," uh so this is a fully connected stuff today i think i forgot some of the connections in here but it"
8717520,8722400," doesn't really matter so they're they don't really have that much meaning right so they sort of like"
8722400,8730960," demonstrate the full connections this things right so to do the backpropagation so you're going to have"
8731680,8741120," um four nested loops you're going to have four nested loops so the first outer loop is the loop from samples"
8741120,8743120," right for each sample"
8743120,8752080," sample from the training date so this is the first loop right for each sample of the training data you're"
8752080,8757840," going to be basically fast forwarding that sample until you get the output in here and then you're going to"
8757840,8766720," be basically going backwards propagating that those differences in the activation layers so now you have"
8766720,8778240," another inner loop which is for each layer for layer for samples for layers and again we had only one input"
8778240,8786800," and one output so that means essentially what we did we just followed that one path right in our mathematical"
8786800,8794400," expressions that one path and then followed back the same path but here we have to actually follow back"
8794400,8801840," this path and then this path then uh like this path then another path all the possible paths"
8801840,8811200," so so that means we're going to have another for looping here for uh current current activations current"
8813200,8820160," acts for current acts so if your current layer is here you would have you would have another inner loop"
8820160,8826640," that iterates through each activation in here through each activation in here right then uh when you're"
8826640,8830480," done with that iteration for that specific layer you're going to go here and then you would have to"
8830480,8835600," iterate each individual activation in here right and then each individual activation in here"
8836480,8844000," and for each activation once you are iterating each activation you would have to check each activation"
8844000,8851280," of the previous layer right so essentially for each activation in here you need to check each of these"
8851280,8856800," things then checking here you need to check all these things so it's going to be another nested loop for"
8858080,8870800," previous activations right and by doing that you need to keep propagating the differences and partial derivatives"
8872480,8881200," yeah and that's basically my friends a back propagation algorithm that nobody gives a about"
8881200,8888480," and that's precisely is the reason nobody gives a about because it's just look already implemented"
8888480,8896400," so but we can try to implement it ourselves right so we know the math we know how it mechanically has to go"
8898480,8904800," uh we can just go ahead and try to code this if you know what i mean we can just go ahead and try to code"
8904800,8912800," that uh okay guys so uh since we're going to be doing that on an actual neural network we'll already have"
8912800,8920800," a neural network implemented in uh in our framework right so uh let's go and do that so we have a finite"
8920800,8926240," different here so that's basically the function that implements that finite difference thing uh it just"
8926240,8932400," computes the current cost it just adjust things and stuff like that uh what i want to do i want to"
8932400,8939680," introduce another one and i'm going to call it something like back prop right so the idea of this"
8939680,8945680," interface is actually rather simple so here you provide the neural network and another neural network"
8945680,8951680," which is a gradient for this specific neural network and then you provide the epsilon basically the step for"
8951680,8957600," the finite difference and then the the batch the this uh the training data so this is the input training"
8957600,8961920," data this is the the corresponding output and it just like computes the difference between the cost"
8961920,8967920," function and puts all of these differences in the gradient right with a certain epsilon of course right"
8967920,8972720," i want to implement the same kind of idea for the interface but uh we're not going to provide the"
8972720,8978720," epsilon because this is going to be like a gradient right this is going to be a gradient and here we're going to have an input and output"
8978720,8985440," so and let's just go ahead and implement that so uh essentially what i want to do is uh implement back"
8985440,8991920," back propagation for that specific neural network and put the gradient into a separate neural network i think it's kind of like"
8991920,9007760," um i think it's kind of redundant i'm pretty sure you can update the weights and biases in place but i still kind of want to have like uh like the the gradient separately so i can just do one step"
9007760,9014160," one step and literally print the differences well i suppose i can do that if i just copy the the previous"
9014160,9017920," state of the neural network or something i don't know i kind of like this maybe in the future i'm going to"
9017920,9024000," change this interface who knows uh all right so this is the finite difference and let's just go ahead and try to"
9024000,9028000," to implement this entire stuff as i already said we're going to have like a four nested loop"
9028000,9034400," and the outer loop is the all the samples and all the samples is just like uh these things right so this"
9034400,9041440," is a matrix each row of this matrix is the input uh and each row of this matrix is the output which means"
9041440,9048160," that uh their the amount of their rows should be equal right so they should be the same essentially"
9048160,9053760," and we can basically assign the amount of rows in here to n so it's like a little bit easier mentally"
9053760,9059680," for us to just do it like that and we know that i is the current sample or something like that right"
9060400,9070720," so that's basically what it is so uh for each sample uh we basically first thing we need to do we need to"
9070720,9078080," forward we need to effectively forward uh the current sample right so the current sample is actually the"
9078080,9084960," the current row so we can take the row the ith row of ti and that's the input and we can try to copy"
9084960,9094400," copy that specific row into ti um into the input of the neural network so and then input"
9094400,9101120," and then and there we go so that kind of assumes that kind of assumes that these things are going to"
9101120,9105920," be equal does matt copy check for the sizes yeah i think it checks so i don't really have to check that"
9105920,9111520," myself so we take the uh the ith row and we put it into the input of the neural network and then we"
9111520,9119680," forward this entire neural network so and then in the output of this neural network uh we're going to"
9119680,9129280," have probably uh something hopefully uh equal to the ith row of to right ti and to stands for training"
9129280,9134400," input and training output right so we take the row of training input we put it into the input of the"
9134400,9139120," neural network we forward that that will just like activate everything and just put this data through"
9139120,9146320," the neural network right and then we have an output and we just need to see how much this output is close"
9146320,9154960," to whatever we expect them here essentially uh right and here is an interesting thing right so here is an"
9154960,9163120," interesting thing um this is how we already did the the forward pass right so this is the forward pass it's done"
9163120,9168080," by the call of this function right so it's it's already done for us we already have all of the activations"
9168080,9175040," uh right derivatives what's cool about sigmoid is that we don't really need to have the um partial"
9175040,9181360," derivatives computed because we can always compute them based on these activations right we can always"
9181360,9188640," compute them based on this activation so we don't really need that uh all right so now we need to get that"
9188640,9195920," difference from that imaginary uh from that imaginary layer from that imaginary output layer and we know"
9195920,9203120," that the difference is basically uh differences from between this thing and what's expected right"
9203120,9212240," so uh essentially we want to iterate uh the columns of the output stuff right so we want to iterate the"
9212240,9219680," columns of the output stuff though we do not really check right we do not really check that the amount"
9219680,9225200," of columns of the output and amount of columns of the this thing is the same so this is probably something"
9225200,9233360," that we want to check i'm going to do and then assert uh right so the output and the output amount of"
9234160,9240720," yeah amount of columns should be equal to the amount of columns in here so this is quite important i think"
9240720,9250880," to check uh uh uh-huh so and in here we're going to be just like iterating uh zero calls right"
9250880,9261040," so this has to be j i'm sorry uh okay so we take the output"
9263440,9269440," all right so we take the output and we have to so it only has one row so the row is going to be zero but"
9269440,9276480," the column is going to be j right and uh we need to subtract what is actually expected in here so it's"
9276480,9284560," going to be met at uh to oh the row is i because it's kind of simple and the column is j and so this"
9284560,9292080," is the difference and the question is where are we going to store that difference right where are we going to"
9292080,9298800," store that difference this is actually kind of interesting right so because in an n"
9298800,9307360," we already have the activations right so if we take a look at an n right so we store"
9307360,9313040," the weights and biases and intermediate activations and intermediate activations"
9314000,9320080," and we have more like a count plus one activations than the layers specifically because we need to"
9320080,9328960," store the actual output uh right so and we can't really use that intermediate memory"
9329680,9337040," for the activations but we have a duplicate of the activations uh in the gradient itself"
9337040,9346880," in the gradient itself right so essentially uh i use a separate neural network i use a separate neural"
9346880,9352160," network to store the gradient because i want to be able to print it and see it but because i make a"
9352160,9359840," clone of the neural network it also creates waste of memory in terms of this uh like activation stuff"
9359840,9367600," which we can put in use what if in an n in the activation arrays and activation matrices we store"
9367600,9376080," the actual activations after this forward but in a gradient we store these differences these partial"
9376080,9384080," partial derivatives of those activations right so we kind of naturally have a place to store these intermediate partial derivatives"
9384080,9388800," which we can do right and essentially we can just like go ahead and store that"
9388800,9399920," in the uh output in the output of j uh zero j and there we go so and this is where we store all that"
9400400,9408800," cool so we pre-computed uh we pre-computed the uh partial derivatives of activations from that"
9408800,9415680," like uh in forward imaginary layer from the next imaginary layer that doesn't exist"
9415680,9420960," so now"
9423440,9432240," now we need to start iterating the layers backwards right so let's do size t l and we're gonna start"
9432240,9439520," uh like how many layers do we have like we have n uh count right so and it's gonna be while l is greater"
9439520,9448480," than zero minus minus l right so minus minus l so there is actually a little bit more um for loops in here"
9448480,9455120," as i said there will be like first samples and then layers but there is another one for the amount of"
9455120,9461120," outputs in here just to prepare this like you know first derivatives of the activation so that's something"
9461120,9469920," that is going to be needed in the future uh okay so we need to start so we're starting iterating the"
9469920,9478000," layers right we're starting iterating layers and for each layer we need to start iterating the activations"
9478000,9485280," of the current layer so how can we even do that how can we even do that so how many activations of the"
9485280,9493040," current layer do we even have so if we take a look the activations of the layer l and that's the correct"
9493040,9500160," layer right so it's a row so that means rows in this particular case is equal to zero so there's no point"
9500160,9507200," in looking at that so that means here uh this is what we need to iterate so we're iterating the current"
9507520,9512880," uh sort of uh sort of activations right so let's actually denote them with j because i is already"
9512880,9520000," taken for the samples uh right so we can say something like i current sample"
9520000,9533120," um l current layer so then j current activation current activation and we also need the previous"
9533120,9536960," activation right previous activation right previous activation let's denote that with k right so this"
9536960,9544000," is going to be like a four nested loop i l j k right so something like that uh so we're going to iterate"
9544000,9553520," from zero uh j less than that uh plus plus j right and i think we can already compute something on that"
9553520,9565920," specific layer right so we can compute the biases because they don't have anything in here so bias is"
9565920,9579520," only for the current uh for the current neuron right so you don't have if in case of weights right in case of"
9579520,9588240," weights you have several weights per neuron and that's why for a single current neuron you need to iterate"
9588240,9594240," each previous one because by iterating each previous one you're basically iterating each of the weights"
9594240,9601200," for each neuron you only have one bias so for computing the partial derivative for the bias you don't need to"
9601200,9606880," to iterate through all of the uh previous activations so because of that i can actually put this stuff in"
9606880,9614160," here so uh we're gonna start the derivative in the gradient itself so we're gonna say okay so this is g"
9614160,9622480," activation so the current activation is already taken so uh actually yeah this can be l minus one right"
9624880,9636320," this is the l minus one and inside of this thing met at right so the uh the activations are the row so that"
9636320,9642640," means the row is zero but this is j right and how we are computing all that we're computing all that by"
9642640,9650080," multiplying it by two and essentially yeah taking the difference from from the previous one so we can say"
9650080,9657440," it's something like something like something like d a did i do a lucky walking here yes i did a lucky walking"
9657440,9662640," so this is a completely incorrect stuff"
9662640,9670080," so let's uh yeah i can see how it is incorrect"
9673200,9682240," so because yeah i completely forgot the rest of the derivatives that we need to put in here"
9682240,9685200," so this is just that"
9685200,9688480," oh i mean"
9688480,9691840," this thing"
9691840,9694480," that"
9696720,9701520," yeah yeah so let's say computer it's going to be in ml notes"
9701520,9713120," yeah there we go this is what we have to have so for b we only need the activations of the current"
9713120,9721440," layer right the activations of the current layer and let's take a look if we can have them so this is"
9721440,9727600," going to be a uh a and an n"
9727600,9736080," as l as l right so this is going to be basically that uh and"
9736080,9745360," mapped at row so this is a row zero and we take the j so this is a"
9746160,9754160," uh-huh so we just take that and the actual difference d a all right i'm going to denote"
9754160,9760320," this scary thing with just d a because that's what it is we're actually taking it from from the gradient"
9760320,9765360," which allows us to compute this kind of stuff quite easily it's going to be d a"
9766960,9774880," multiplied by a one minus a that's it actually for that specific thing"
9774880,9779120," for that specific thing"
9779120,9788880," so okay so we managed to compute all of the biases but this is where it becomes complicated now we need to"
9788880,9795120," compute the partial derivatives for the weights and for the previous activations so we can propagate"
9795120,9800480," them further right so we can propagate them further and that means that we need to start"
9800480,9805760," iterating through all of the previous activations right all of the previous activations as l"
9805760,9810560," l minus one actually right so calls"
9810560,9820880," plus plus a g something like that so all of the previous activations and how are we going to be"
9821840,9826720," constructing all of the that now we're going to be constructing all of that so we need to compute"
9826720,9834160," sort of like the current matrix right if that makes any sense uh so this is biased by the way"
9834160,9844160," so we were computing biases plus plus k thing but nobody told me that i had to actually put biases in"
9844160,9849520," here instead of activations of course everybody noticed the easiest thing plus plus k not the hard thing"
9850320,9858000," all right so yeah but primarily because i fucked up this thing in here right so the thing about the"
9858000,9863680," back propagation stuff is just like it's very difficult to explain uh i think the best explanation"
9863680,9870000," online is by three blue and brown that i'm gonna put in the description uh literally for people in the"
9870000,9875680," twitch chat you can just like go to youtube and just do three blue and brown neural networks and you'll"
9875680,9882560," instantly find that playlist uh and there is one of the episodes where he explains back propagation he"
9882560,9892880," actually dedicated two videos two separate videos to back propagation right so imagine like how complicated"
9892880,9899920," that topic is that three blue and brown the master of explanation of mathematical things uh dedicated two"
9899920,9906240," separate videos for that where in the first video he basically laid down the high level intuition and in"
9906240,9913520," the second video he went into mathematical details and even then he didn't went into as much details as i"
9913520,9920880," did in here so in his second video where he supposedly went into mathematical details actually he didn't even"
9920880,9924960," demonstrate it how you write that in a code because i suppose you're supposed to do it would come up with that"
9924960,9930240," much with yourself but there's still some feeling like missing there's a lot of things that you still"
9930240,9938640," need to play yourself um all right and it's not very difficult topic per se oh thank you so much for"
9938640,9946480," for uh posting the the video to this thing right it's not difficult topic it's complicated so this is a"
9946480,9953360," fundamental difference it's not difficult it's just complicated because there is a like too many layers of"
9953360,9961520," complexity for human brain to keep track of right so it is genuinely difficult for a human brain to um"
9961520,9967200," to comprehend all of that because again you have four nested loops and it's not like you can just"
9967200,9974000," abstract them away you need to think about them simultaneously you need to think simultaneously about"
9974000,9983040," all four layers of nested loops to get the idea across it's hard man it's freaking hard and i'm still not sure"
9983040,9989040," sure that is this thing is going to work like i kind of know how to code that but i'm already a little"
9989040,9997120," bit lost right i'm already a little bit lost i'm not sure in which order i have to pass j and k as the"
9997120,10002800," coordinates for the weight matrix i i just don't know my brain cannot visualize that because i'm already"
10002800,10010560," keeping too much to like keep track of too many things already it is not difficult per se it is complicated"
10010560,10018320," it's like it's the complexity that kind of like uh makes it like this but again if you're don't want"
10018320,10026560," to get into like details too much and understand stuff you probably don't really have to uh right you"
10026560,10031920," probably don't even have to because all of the neural net network frameworks just do all of that for you"
10031920,10036800," like why why would you need to understand that uh but i personally kind of want to understand that i was"
10036800,10043280," always this kind of guy that it's kind of difficult for me to use something like it's difficult for me to"
10043280,10050480," use black boxes right it's difficult for me to use black boxes because i like to go into inside of the black"
10050480,10058240," boxes and like really rearrange them uh right so it's it's kind of difficult to explain why like going"
10058240,10070240," inside of the black boxes is so powerful so imagine that you have two black boxes and you need to kind of"
10070240,10075760," solve a very specific problem that is kind of difficult to solve when you're thinking about"
10075760,10081440," them on this level it's kind of like none of the black boxes sort of fit together right they kind of"
10081440,10086640," like you put like inputs and outputs together and it's never kind of fit and you have to put like"
10086640,10094320," additional hacks more black boxes to actually fit it across but quite often if you look inside"
10094320,10103600," of the black boxes right if you look inside of the black boxes whatever you combine as inside"
10103600,10110080," of two black boxes might be the right solution that you want it's like it's um i know that i'm just hand"
10110080,10116800," waving but that's how it feels for me when i go inside of these black boxes right because quite often"
10116800,10124480," you can't really solve the problem on this level it's kind of like difficult but if you break it down"
10124480,10130880," if you go a little bit deeper you can rearrange some of the stuff from from insides of these black boxes"
10130880,10134240," and get exactly what you want you know what i mean"
10137040,10141600," so you know what i mean it's kind of like difficult to explain and i can't really provide like exact"
10141600,10150160," example uh but it this kind of happens to me way too often when i was like like learning how things"
10150160,10159280," work inside all right so because like when you see only like high level black boxes you don't really see"
10159280,10166080," a lot of things that could help you to solve the problem efficiently right"
10166080,10173920," if that makes any sense that's why i kind of like like to look inside uh and see what parts of"
10173920,10179440," the inside of the black boxes could be rearranged from the inside of different black boxes to get a new"
10179440,10186880," black box that didn't exist before you can't create a new black box that didn't exist before if you don't"
10186880,10191360," look inside of other black boxes and see how their insides can be combined together"
10191360,10194080," um"
10194080,10200720," many refer to this as thinking from the first principle i think i heard that phrase from"
10200720,10204480," jonathan blow quite a lot is that what he means when he says"
10204480,10209920," first principle because he keeps saying that thing and i'm not quite sure what exactly he means by that"
10210560,10219200," right it's probably that so um but so what's interesting like i used to program on a very high"
10219200,10225360," level right so i used to work as a java enterprise developer but the more i start digging in inside of"
10225360,10232560," these black boxes and realizing how you can rearrange their insights and get more interesting emergent and"
10232560,10238720," synergetic behaviors the more it became addictive to me if that makes any sense"
10238720,10247040," it is kind of addictive how you disassemble these black boxes and you just unlock something that was"
10247040,10252640," not visible before it's extremely addictive that you can't stop you keep like digging down and down and"
10252640,10258320," down what else what else what else i can discover in there it's just like fucking insane yeah it's just"
10258320,10262720," like keep digging down and down and discover more and more you rearrange things and it's just like holy"
10262720,10268640," fucking shit and all of that power was hidden from me all the time i couldn't see that all right"
10270480,10276880," so and like what kind of shit i can discover if i go down like below hardware level"
10276880,10280720," or something like that i don't fucking know so it's insane"
10280720,10288880," anyway it's kind of it's kind of interesting and that's why i spent three days making sense of the issue"
10293360,10302720," right so anyway so um what i was doing uh we are iterating uh all the current layers right we're"
10302720,10307920," iterating all the current layers and for each current all the current activations and for each current"
10307920,10316480," activation we're iterating each previous activation uh right so here's the thing so let's imagine that we"
10316480,10327920," have the current activation and the thing is my framework basically passes rows through the network"
10327920,10336880," right so if you have something like this right so this is the row and then you have a bunch of vectors"
10336880,10344320," right so here you would for us to say uh right so here you have three activations and let's pass them"
10344320,10349840," through uh two neurons right and it's going to be something like this uh right"
10349840,10360960," yes so this is something like this and that basically creates two new activations in here"
10360960,10367920," so the reason why i do it like that is because it's kind of easy for me to think about matrix"
10367920,10374160," multiplication as just basically this uh intersection right so this activation"
10374160,10381200," is computed on intersection here in here right and then i pass it to the next"
10381200,10389360," activation in here right so um and so on and so forth basically right you see what i mean"
10389360,10398800," so here is going to be another like this and we'll basically expand it to three uh but maybe it has to be"
10401440,10407520," yeah so it's going to be like that and so on and so forth as far as now this is not particularly"
10407520,10418400," conventional in neural networks right usually you pass activations as a column right uh you pass activation"
10418400,10425200," as a column uh and you multiply from a different side right so essentially i have one by three and"
10425200,10430800," because of that i have a three by three or like three by two in here so this is uh"
10430800,10439600," three by one right so that means the way it goes to the left right so i suppose the actual convention is"
10439600,10447680," something like this right and maybe because it just looks nicer because you write it like that i don't know"
10447680,10456400," the actual reason right maybe it just looks nicer but uh for i in my framework i did it like that"
10456400,10464480," because i don't know any better right maybe in the future maybe in the future uh we'll have to uh"
10464480,10468640," change it to a different convention but for now i kind of stuck with this sort of convention and i'm"
10468640,10475440," going to just go with it if you know what i mean uh anyway so essentially this is the previous"
10475440,10480560," activations right we can think about that stuff as previous activations and this is the current"
10480560,10486720," activation right so this is the current activation so that means the current activation uh right"
10488880,10493040," wait a second one okay so current activation"
10493040,10510880," is the uh weight matrix column right so if this thing is the current activation that we supposed to"
10510880,10518160," compute uh and the current activation is j that means the j is the column so that means k is the"
10518160,10528160," uh is the uh is the row rate matrix row it's kind of important right because you can actually quite"
10528160,10533600," easily get lost while trying to understand and compute all of that it's kind of like yeah"
10533600,10546320," all right so what do we do in here so we need to do g ws l minus one and uh the row is k right the row is k"
10546320,10554960," but the uh the column is j right something like this and what we add in here is 2 multiplied by d a multiplied"
10554960,10563680," by a 1 minus a as usual and then we need to take x i which is activation well it's supposed to be"
10563680,10570640," activation of the current layer but i just left it as x i so which is another mistake in here right"
10572240,10577600," is it activation of x i i don't think so so let me see"
10577600,10584000," uh so this is going to be the weight yeah it's activation from the previous layer"
10585120,10602320," so we can probably do uh pa which is activation from the previous layer um and"
10605440,10615760," so this is going to be the matrix at uh row column uh multiplied by p a hopefully i didn't make any"
10615760,10625040," mistakes in here right so we already managed compute to like a partial derivative for the uh for the biases"
10625040,10634080," and uh for the weights and uh this is two thirds of the way so now we need to compute the um activations"
10634080,10638480," um you know different differences for the activations and they're going to be computed into"
10638480,10645600," a s but uh i suppose l minus one yeah that makes sense"
10647120,10656480," so we can do two this one also has to be a difference uh okay i keep finding errors in this entire thing"
10656480,10662080," as i implement this entire stuff which is kind of which is kind of funny all right so"
10662080,10665040," yeah"
10668880,10686240," yeah so that that makes sense uh so d a a msa and we have to take the the weight right and i suppose"
10686240,10695680," the weight is going to be one of those things right so flow to weight but we take it from nn so i'm not"
10695680,10701360," sure if all of that is going to work at all and i'm really curious but yeah we're about to find out"
10701360,10707680," so yeah there we go so basically computed all the intermediate uh derivative and stuff like that but"
10707680,10714640," they are not divided by n so after we computed all of that for the entire thing we need to iterate"
10714640,10722800," through all of them and divide by n uh all right so let's iterate through well i mean we have separately"
10723760,10733120," weights and biases so we have to do something like size i write gws uh we're iterating the rows of this"
10733120,10740640," entire thing so this is going to be i uh well we are we need to iterate the"
10743040,10754000," the count yeah for each layer all right so we'll give it an uh g count so this is just the layer then size j0 um"
10754000,10756640," g w s i"
10756640,10761200," rows"
10764160,10770480," and then columns and then columns then we take what we have in here"
10770480,10774160," it has to be k"
10774160,10786400," jk divide by n right so and then we have to iterate through all of the biases in here"
10788080,10790320," all right so it could be s yes"
10790320,10799840," and that's basically back propagation"
10799840,10812560," so matt at ws yeah yeah i see what you mean just a second i will zoom in on your comments uh wl but"
10812560,10817520," informally you have l this one is a little bit complicated right this one is a little bit"
10817520,10824080," complicated because this is the specifics of our indecine in our framework right so we have"
10824080,10830640," count layers right so if we take a look at them at the definition this is a very good question by the"
10830640,10836640," way uh right so because it could be a source of confusion like that so here's the count right here's"
10836640,10842800," the count and this is the amount of layers of neural network uh right and the uh we have"
10842800,10849680," three arrays of matrices the array of weights for the current layer the array of biases for the"
10849680,10854400," current layer and the array of activations and the amount of activations is plus one"
10854400,10862720," the reason is right so you have layer uh first layer right so then uh the second layer"
10862720,10870240," maybe we can do vb1 so v w uh b stands for uh weights and biases uh right and so on and so forth"
10870240,10880080," um so maybe we can even do that from zero right and since we're programming in c right since we're"
10880080,10888400," programming in c uh we pre-allocate all of the necessary memory at once right so and essentially uh input"
10888400,10897920," is a zero right so a zero uh and by combining a zero and vb zero we get a one by combining a one and vb one"
10897920,10909600," we get a two until we get uh right so here we have uh wn but here we have a uh like and like this one is"
10909600,10916480," minus one right this minus one so there is more activations than one so that means the current and we"
10916480,10925360," we are actually iterating not uh from count minus one we iterating from like plus one we currently"
10925360,10928480," l is currently pointing on the layer after the current one"
10928480,10937840," so it's you see it's it's kind of like yeah so the framework itself adds a little bit more complexity"
10938640,10950000," into the index chasing right uh index index chasing all right so and uh yeah that's basically the reason"
10950000,10958240," so if we had like a more higher level language maybe it would not be a problem right but uh yeah you get"
10958240,10967920," okay i think i think i'm very glad that i managed to explain that uh okay so let me let me go through"
10967920,10974720," everything just to make sure that i didn't do any fucky walkie and potentially uh any oopsie doopsie"
10974720,10980800," so our convention with four loops in here is that uh i is the current sample we're iterating through"
10980800,10987120," current samples then l is the current layer j is the current activation right the current activation and k is the"
10987120,10993280," previous activation iterating through all the samples so copying the sample into the input of the neural"
10993280,11001360," network and forwarding it okay so that means in the output of the of this thing we got uh some stuff"
11001360,11008000," we iterating through all the columns of the output right so we take each individual value of that output"
11008000,11014000," and then then then the corresponding value from the expected output we take the difference and we put"
11014000,11021440," them into the output activation of j of the gradient because we use it as an intermediate thing right so"
11021440,11027600," all right we're starting to iterate the layers from the last one uh up until zero and we ignore the zero"
11027600,11033360," and the reason why we ignore the zero is because we sometimes get back we do l minus one and uh you"
11033360,11042720," cannot look past zero so that's why we do it like that so for the uh for the current layer we look"
11042720,11048160," through current activations and the current activations it's basically l uh their columns we look through"
11048160,11055200," the columns okay that makes sense so we came to the conclusion right so that uh maybe i actually have to"
11055200,11060320," put it in here because this information is kind of kind of irrelevant until you get to this particular"
11060320,11066480," part okay so i take the activation uh the current activation we're taking it from the neural network"
11066480,11071520," itself right so that's what we're getting here it's a row so that's why it's zero and we take the the j"
11071520,11078080," column so and then we also take the um difference of the activation basically the derivative right so"
11078080,11083360," derivative of the activation and it's uh coming from the j for the very first layer it's going to be"
11083360,11089920," literally the difference that we computed from the from the sample stuff right so and then we computing"
11089920,11095040," the uh derivative for the biases for the current layer and for this for the weights mattress and for"
11095040,11100000," the bias mattresses the current layer is l minus one right because it's kind of shifted because of the"
11100000,11107280," indices and stuff like that so let me double check so for the b uh for the b uh essentially we just"
11107280,11113360," to take two d a a multiply here but i mean the compiler would have told me this specific error so"
11113360,11118960," that should be fine okay that's fine cool uh so this is the only thing that could be computed on that"
11118960,11124560," level on the on the on the third uh you know layer the rest of the stuff has to be computed um"
11124560,11131920," in the innermost uh loop because it involves the previous activation so that's kind of important"
11132560,11136000," okay we are iterating through the previous activations right so iterating through the"
11136000,11141520," previous activations and the previous activation is yeah basically one of those so we keep track of"
11141520,11147280," that so i'm still not sure about this statement i need to double check that specific statement that the j"
11147280,11155120," is the weight matrix column right because okay so this is how we multiply so the current activation"
11155120,11161680," the current activation uh is j right so that's the current activation so it means it's a column for this thing"
11161680,11172480," uh and uh right so where is this thing current activation is j and it's a call so that means the j is the column"
11172480,11180000," right and uh row is one of the previous activations one of the previous activations so that means"
11180720,11187600," uh it's a row so that should be fine so for the previous activations scale for the current uh matrix is kj"
11187600,11196080," right because it's a row in column right that's how we index them uh right and this is from an n so for"
11196080,11203440," the gradient of that current thing we say kj is equal to that and this specific previous activation"
11206640,11218880," this specific previous activation okay and for this thing we'll just use the matrix looks good to me"
11218880,11225920," which is good looks good to me"
11225920,11233520," so that should work i think i think that should work uh but i'm not 100 sure"
11234240,11245600," not 100 percent a little bit scared uh a little bit scared uh but let's us see a letter c uh all right"
11245600,11249760," so let's try to recompile zor right let's do zor"
11249760,11256800," i'm calling all zor zor.clm and let's go to the compilation errors right so"
11258960,11265360," so this is multiplication so this compilation errors are really easy to to catch okay so this one is very"
11265360,11270720," important so we're iterating through the previous things so this is a row so that means we have to do"
11270720,11284240," something like uh k in here uh right and this one is very interesting so yeah because how neural network"
11284240,11292800," works yeah this one is really interesting um so imagine that we're iterating through the current"
11292800,11297840," layers through the current layers and then iterating through each in individual previous layer"
11297840,11309440," so change in this in the neuron of activation of this one affects this neuron but it also affects this one"
11312080,11315520," right so essentially you kind of have two separate derivatives"
11315520,11324640," change over this over this and change of this over this which one do you pick which derivative"
11324640,11330160," do you pick for that specific error which one do you pick for that specific error"
11330160,11337440," this is a good question and i watched the uh 3b1 brown explanation the second video on backpropagation"
11338000,11344000," and the only thing he said about this specific situation he literally like said one or two"
11344000,11351040," sentences of this specific situation he said just sum them up like literally he like this is a very"
11351040,11356160," like interesting situation like it's like what the do you do with that and he's just like yeah i just"
11356160,11361120," summed them up uh right and didn't really go much into the detail and it's just like do you really sum"
11361120,11367040," them up do you have to take the average of their sum why not average of the sum maybe average of their"
11367040,11374400," sum would make sense why just sum them up i don't freaking know but yeah so maybe there is like a reason"
11374400,11379920," if i went deeper right one of the things i could probably try to verify is just like try to compute"
11381040,11387520," the derivative of the entire neural network uh of the entire neural network without backpropagation"
11387520,11393280," just like basically tank the complexity right of course when you start doing that and apply chain"
11393280,11399280," rule is just going to explode with complexity uh right but just like tank this complexity"
11399280,11403360," and see if you ended up with the same solution as with backpropagation"
11403920,11409440," uh right and that's how you can confirm that but i don't want to do that i definitely don't want to"
11409440,11416160," tank uh computing the derivative of the cost function without backpropagation like i don't"
11416160,11421200," want to do that because it's just like it will just basically explode the more nested layers you have"
11421200,11424720," the more it's going to explode basically uh right"
11428400,11434560," yes you sum them up it's like a partial derivation of transformation from x y to r theta"
11434560,11438320," doesn't really make much sense to me because i'm not a mathematician but i will just trust you since"
11438320,11442960," you're a mathematician right and three blue one brown also mathematician and it's just like two"
11442960,11449200," mathematicians told me that i have to sum them up so that means it's probably true but i'm an engineer"
11449200,11453840," so i don't check things i check things by just running them and if they don't catch fire it's good"
11453840,11462000," good enough for me right i know it sounds weird after three hours of this but i'm just an engineer"
11462000,11465760," i'm not a mathematician anyway uh so"
11465760,11476560," anyways so everything seems to be working so it's actually uh yeah let's actually uh go ahead and see"
11476560,11482800," if it's going to work the uh the fun will start if this won't work the first try"
11484560,11492880," ah it's going to be unbearable anyway so uh let us see let us see so what i want to do"
11492880,11499760," i probably want to just uh comment this stuff out right and i'm gonna just do that"
11499760,11509600," uh okay if we just run zor uh what happened in here so we got the actual neural network uh maybe i'm not"
11509600,11516240," gonna print the actual neural network i want to print the uh gradient that we computed using finite"
11516240,11524800," difference right so so let's actually fixate the random number generator random number generator"
11524800,11532240," yes well i probably have to recompile this choice i'm sorry for saying that word again"
11533440,11538560," this is finite difference right so this is finite difference and now i'm gonna quickly switch this"
11538560,11543120," finite difference to a back prop right so finite difference uses epsilon so let's actually put this"
11543120,11559840," epsilon in here so else uh right else back drop back prop um so and if okay the moment of truth so it"
11559840,11561520," it doesn't really accept epsilon right"
11561520,11572960," okay not bad at least we don't see nans in infs that's already good right so that's already freaking"
11572960,11579040," good so if i comment it out this entire thing is supposed to be okay so this one is a little bit"
11579040,11590160," dangerous right so uh i forgot a very important thing i think i forgot a very important thing we need to"
11590160,11599280," clean up the entire gradient in here so right i would like to do something like uh and then zero and i want"
11599280,11605120," to fill up the entire gradient with zero before doing all of these accumulations and stuff like that if that"
11605120,11614160," makes any sense uh right uh so and i don't remember so does it we don't have such function yet okay so"
11614160,11622320," we have some kind of like um uh copy right we don't really have copy no i don't i don't have copy okay"
11622320,11628960," so let's do and then zero and here we're going to accept the neural network and we're gonna just like"
11628960,11638560," fill it with zero so let's put it in here so we're gonna iterate through all of the layers"
11638560,11644880," all right uh less than and then count actually count please"
11644880,11653920," so this is the layers and for each of them do we have matte zero we don't have a matte zero"
11654800,11660720," ah i think we have a matte field i should have actually called it an n field then whatever so"
11660720,11669920," let's do the matte field w s i you just fill it with zero b s and a s as well but in case of a s you"
11669920,11679120," also have uh an n count right so there's an extra layer to fill uh with zero so back prop uh back propagation"
11680400,11686640," right so when i think um there is another interesting thing in here right so we filled all of that stuff"
11686640,11695600," with zero we fill all that stuff with zero and in case of the gradient"
11695600,11703920," we compute intermediate like da's we compute intermediate da's um"
11705920,11714080," compute intermediate da's for each sample so i suppose uh for each kind of layer"
11714080,11724400," what we want to do we want to clean up this ace right so because here for instance we just set g"
11724400,11732560," but we can have uh the values from here so i suppose we want to just like set"
11733520,11739520," um zero um zero on each individual layer in here i guess that's what we want"
11739520,11749280," so let's quickly do that so i want to do uh j from zero to equal actually and then count right"
11751520,11760880," and j a s j mat fuel with zero right so this activation i think it's kind of important because"
11760880,11768800," uh they carry a lot of intermediate stuff that is not important uh between the samples so this is quite"
11768800,11776000," important to consider i think right so okay here i actually disable this thing so let me enable the back"
11776000,11783280," propagation back and yeah this is the reason why i wanted to do that so because in a finite difference"
11783280,11789920," if we switch to finite difference i remember that those weights the negative but then when i switch to back"
11789920,11798000," prop they became positive and this is actually very sus and if we don't clean this stuff up if we just"
11798000,11803760," literally remove the center i think they become positive and this is a completely different gradient"
11803760,11811120," and this could be a disaster right so yeah actually i'm really glad that we caught that really early"
11811120,11818560," all right so and this is one of the reasons why i want to be able to print uh those uh you know the"
11818560,11823920," gradients just to see see them right and they're kind of similar right they're kind of similar but they're"
11823920,11829840," different right so different i feel like the gradient is a little bit bigger right because it could be a"
11829840,11835760," little bit more precise on how sensitive the parameters are in uh to the cost function right and"
11835760,11843520," we can tell you precisely how to move in a certain direction uh okay so we can try to learn from a single"
11843520,11850400," sample right so we're currently at the back propagation and let's see how a single sample improves does that"
11850400,11855920," improve the cost function i think it does we had nine in here now it's eight so what i found with"
11855920,11862160," back propagation you can even have like one and it's gonna be fine it's gonna actually learn uh relatively"
11862160,11871360," fast all right so it was nine it became five so now we can try to maybe train for 100 the box and it"
11871360,11878240," kind of goes down right as you can see kind of goes down which is nice and it's yeah so with just"
11878240,11886000," thousands of iterations it's well it's not bad but this is because we we made a faster rate what if"
11886000,11891360," with this rate i'm gonna do finite difference all right also nice okay so this is because of the rate"
11891360,11900080," oh yeah all right so the we implemented backprop we literally just implemented backprop so what about"
11900080,11912080," five uh yep that's cool so this is a back propagation to be fair it would be kind of nice to see if back"
11912080,11919360," propagation actually saves time backprop first try yeah that's exactly so let me quickly commit the backprop"
11920400,11932480," so unfortunately my sort of like tour through the forest of the backpropagation uh partial derivatives"
11932480,11938240," is not a particularly great explanation on how it works and i really apologize for anyone who's a go"
11938240,11945360," who's going to watch this entire stuff right so better explanation could be like again uh as i already"
11945360,11951840," said is done by three blue and brown right but even his explanation is like requires you to"
11951840,11959120," stop and ponder if you know what i mean uh right so but here we have something i'm not sure if there are"
11959120,11965200," any mistakes in here right so because this is a very like complicated thing it's just like four nested"
11965200,11969920," loops and it's not like you can put some of the loops into separate function forget about them you need"
11969920,11975440," to sort of like mentally keep track of all that and on top of that we're using a convention like"
11975440,11984320," in conventional you know indexing of the matrices so yeah anyway so what i want to do is i want to do"
11984320,11992960," a committee committee right so i'm going to literally just make a commit and say implement and then backprop"
11992960,11997920," right implement and then backprop i'm going to push that uh right into the repo if you're interested where you"
11997920,12009440," can find the repo uh where is my browser uh the repo is located in here right so and the pdf is going to"
12009440,12018480," be available in here in ml notes in a separate a separate repo uh it was complex but i appreciated it"
12018480,12024960," the only annoyance is that there is no way for me to pay you uh you don't owe me anything you don't owe me"
12024960,12034400," anything don't worry about it so uh but again so for just learning a little bit about neural networks"
12034400,12040320," right it just in even training simple neural networks you don't need a back propagation you can just use"
12040320,12045680," finite difference right and finite difference is just like basically wiggle one of the parameters of the"
12045680,12052560," model uh and compute its cost subtract the original cost and divide by that small wiggle and you kind of get"
12052560,12057920," the direction in which the function wants to go and you subtract that direction go in opposite direction"
12057920,12065200," right so you can approximate the gradient gradient is being basically very precise about like how you"
12065200,12071120," move and stuff like that uh but you don't really have to be this precise but i wanted to be this precise"
12071120,12076560," just to learn how it's done and stuff like that right so this is actually kind of cool"
12078080,12085280," okay uh so let's try to do a random stuff so i want to migrate completely to back propagation here so we"
12085280,12092240," don't really need epsilon anymore right we don't really need epsilon um so and in here then we just"
12092240,12101600," print the final thing so in here we have like only 5000 things and it's enough to just train this into i think"
12101600,12118240," so as far as i understand gradient is more effective when you have huge networks in such small and then"
12118240,12124080," it's not much better than fd yeah maybe we need to build a bigger network let's actually give it a try"
12124640,12134800," so we have a zor right but what if we try to build uh a sum a summer or adder yeah let's call it an adder"
12134800,12141360," uh so let me let me try to do that adder dot c uh right so we're gonna have a full adder right and"
12141360,12151040," a full adder uh let's create some like training data for the full adder so it's gonna be float uh training data"
12151040,12159600," so this is the training data so let's say that uh the full adder actually has three inputs like the"
12159600,12166080," first bit uh the second bit well i mean it's not necessarily something like that"
12168880,12180320," um so let me let me think let me think so i want to implement the following adder uh all right so i"
12180320,12188240," have want to have like two bits in here right then two bits in here some sort of like a black box in here"
12188240,12195440," i don't know how many layers we'll have in there and then two bits in here and another one in here right"
12195440,12201440," so essentially we have something like this right it's fully connected of course uh and here we have"
12201440,12208560," something like that and then this is the last bit so essentially you pass the your first number a"
12208560,12217120," into these two bits in some representation okay and the second number in here and you get"
12217920,12225280," uh the third number which is the sum of these two numbers right and this one is the carry bit right"
12225280,12232160," so this is the carry so if or maybe we can call it overflow right so it's an overflow what's called"
12232160,12233680," all f"
12233680,12244560," so uh yeah it's an overflow bit if basically the sum overflows uh these things are going to be zero"
12244560,12250320," right and this one is going to be set to one right something like that uh so and here's an interesting"
12250320,12257200," thing i want this thing to be uh a variable in the sense that i want to have a global parameter n"
12257200,12264400," which for this specific neural network is is equal to two right if i set it to three i want it to"
12264400,12272400," automatically scale to three in here and three in here right and i want to just like keep increasing"
12273040,12279520," the amount of bits that you can calculate right and see at which point the difference in performance"
12279520,12287040," between finite differences and uh back propagation starts to matter to that next instance sounds good"
12287040,12294240," sounds good sounds too much i think i think it does all right so uh let's go let's fucking go"
12299920,12304640," in this case you could generate the training set also works with yeah yeah i mean of course like"
12304640,12309440," yeah i'm specifically picking the examples where i can generate the training set so i don't have to"
12309440,12315280," worry about this kind of stuff i have actually a lot of different interesting ideas where the training"
12315280,12320640," set could be just generated right so this is ideas for the future streams all right this is ideas for"
12320640,12326720," the future streams okay so let's introduce the thing called beats right and initially we're going to have"
12326720,12335200," like uh two right so this is two so in here i'm going to just include nn.h and uh we have to define and"
12335200,12343280," then implementation all right so uh what we need to do first we need to generate the training data so"
12343280,12351280," this one is going to be uh inputs right so let's allocate the metrics so how many uh samples are you going to have"
12351280,12363440," right so how many two-bit numbers do you have you have this amount of numbers so and how many uh additions"
12363440,12370960," between them so how many combinations of them you can have basically multiply it by that effectively and"
12370960,12378720," that's how many rows we can have right maybe we can do something like rows and this is the rows and uh so"
12378720,12385120," this is the rows this is amount of things but in terms of like input how many inputs can we have we"
12385120,12393600," can have uh two bits right because it's a bits plus bits right it's a bit plus bits it's a two bits"
12395440,12403120," okay so that's fine uh we can even try to generate this entire stuff so if we iterate through the rows"
12403120,12410720," rows plus plus five so then we take a row uh right we have to iterate the columns"
12413440,12417040," so maybe we can so maybe we can even do something like ti rows and then we're going to iterate the"
12417040,12431760," columns size t j zero j ti calls plus plus j though this is not how we want to generate that for sure"
12431760,12438240," this is not how we want to generate that um essentially we want to actually generate it something"
12438240,12445440," of this right so we're going to have x which is less than one of these things which we may want to"
12445440,12452960," extract to something like n right something like n so it's a little bit easier for us to work with"
12452960,12461520," right so then size t y zero uh y n plus plus y that's something like this"
12464000,12470400," and that doesn't tell us the row the the actual row in here so that means we have to compute that row"
12470400,12481120," uh we can say that um that row is sort of like index one of them uh it's x multiplied by that specific"
12481120,12487120," n plus y right so we can consider it like a square right so it's sort of like a square um"
12488800,12494560," sort of like a square or maybe what if i invert this entire step"
12494560,12502480," yeah what if i invert this entire step essentially what if i iterate"
12502480,12507040," all of the rows to i rows"
12507040,12513600," and then from them i'm computing x and y essentially x"
12514720,12522560," is going to be equal to i divided by n so this is x and y is a mod n"
12522560,12531680," and then i'm going to be iterating j which is the columns which is not necessarily columns but it's"
12531680,12535760," actually bits all right so this is bits"
12540320,12547440," and then what i can do it's a mat at ti the current row is i"
12547440,12555680," and uh then i just iterate the current bit and the current bit is equal to just basically that"
12555680,12564080," like this but then for this thing it's multiplied by bits"
12566240,12573680," good enough furthermore uh i can do a lot of interesting things in here so i can"
12573680,12576240," yeah let's actually keep it like that for now"
12576240,12582000," let's keep it like that that should be enough so that should generate like all of the possible"
12582000,12584720," combinations and stuff like that this one has to be y by the way"
12587360,12591200," so and then i'm going to do matprint ti"
12591200,12595200," okay so let's do adder"
12595200,12601520," does it work i think it worked"
12601520,12609200," yeah so it did that in a different order in here"
12614960,12616640," that's a really strange order isn't"
12616640,12621200," yeah"
12621200,12624080," now that's better okay"
12624080,12629600," yeah so that makes sense that makes a lot of sense"
12629600,12632000," yo yeah yeah yeah"
12632000,12637360," so i already a little bit tired so i really apologize for very stupid fucking mistakes man"
12637360,12642480," right stupid fucking mistakes man"
12643120,12649440," okay okay so let's actually create outputs right so it's an unlock rows but output is going to be the"
12649440,12658240," amount of bits plus the carry bits so one uh here what we can do we can do z which is uh essentially x plus y"
12658240,12660160," right it's essentially x plus one"
12663200,12668800," and essentially it's it will very much depend on what exactly we're doing"
12668800,12679440," uh right so if z is less than the maximum value that means we just do that uh and we can quite"
12679440,12683680," quite easily even do something like uh to i uh to o"
12683680,12689040," roll i j uh equal to"
12689040,12694000," to z j"
12694000,12696320," one all right"
12696320,12698240," but otherwise"
12698240,12700720," both of these things"
12701680,12704400," uh like all of these things has to be set to zero"
12704400,12710800," and i will hope that the compiler will just invert"
12710800,12714720," the loops it will just generate two separate loops in here so it doesn't really do this kind of stuff"
12714720,12723520," right so the compilers can actually like put for's and ifs inside out so you don't have to do this"
12723520,12727120," comparison on each iteration so you can have two separate loops so that's why i don't really care about"
12727120,12732800," that right and here we can do one of the interesting things uh essentially"
12732800,12734720," mat at"
12734720,12737200," to"
12737200,12738480," right mat at two"
12738480,12742160," i"
12742160,12745040," bit"
12745040,12749040," right because we have bits plus one in here right we have bits plus one"
12749040,12751200," and we can literally set"
12751200,12754000," this thing like that"
12754000,12756480," if it's less if it's not"
12757120,12762960," overflow otherwise blah blah blah and then we can kind of reuse this entire logic like here"
12762960,12766160," maybe"
12766160,12767920," we can say"
12767920,12769200," all right"
12769200,12773040," overflow"
12773040,12777200," so to increase the"
12785680,12786400," readability"
12786400,12786480," readability"
12786480,12787120," if you know what i mean"
12787120,12794080," so this is overflow if overflow we just set everything to zero otherwise we just"
12794080,12796400," kind of expand them and this one is overflow"
12796400,12798480," doesn't that make sense i think it makes sense"
12798480,12800480," so let's just print one right"
12800480,12804800," okay so there's different sums"
12813360,12814640," this doesn't make sense"
12814640,12820960," i feel like i've fucked up something badly"
12820960,12826800," why input zero if it's overflow i think it makes sense"
12826800,12829520," oh yeah i'm gonna do it i'm sorry"
12829520,12831120," you're right"
12831760,12837360," um so again i'm super tired right now so"
12837360,12846160," uh super tired extremely tired yeah okay okay so that makes sense now uh thank you thank you thank"
12846160,12851440," you thank you all right okay so this is the basically the training data cool"
12856880,12862800," so let's create a neural network that will accommodate all that so we can try uh basically the following"
12862800,12866000," architecture all right so we're going to accept um"
12867600,12872000," two bits right two bits and then"
12872000,12875600," so this has to be like this"
12875600,12880640," and output is going to be bits plus one so we're not going to have any hidden layers for now"
12881440,12885120," and let's allocate uh this entire stuff"
12885120,12890480," and let's see if it's going to work"
12890480,12897040," so i also want to take a look at this entire network"
12898080,12907440," uh right so what do we have um yeah we have to swap this stuff around"
12907440,12911680," i think i don't remember the order of the arguments"
12911680,12917760," from zero to one there we go so that's the neural network we have"
12918800,12927360," um right so this is the four inputs then three outputs yeah it's actually yeah so the rows are the"
12927360,12933440," inputs and this is the output so that's the entire thing uh we can compute the cost of this neural"
12933440,12941440," network with respect to ti to uh just to see how it goes all right so we can say this is the cost"
12941440,12948240," because we want to be able to see that i think it's kind of important uh that's the that's the entire"
12948240,12957440," cost cool so let's compute the gradient backprop backprop and then gti to let's take a look at the"
12957440,12964080," backprop first of all so does it make sense it kind of does make sense it actually grows"
12964080,12974080," okay so and then learn and then g rate i think that's it we don't really have to provide the idea"
12975040,12982080," i'm pretty sure um float rate let's set it to one for now uh and let's take a look at if that"
12982080,12990080," improved the gradient it freaking did let's let's fucking go let's fucking go"
12994960,13007840," okay so it's going down so what about thousands of those things"
13007840,13014000," look at that it's stuck at 46"
13016080,13023280," that is interesting okay uh let's go to an end of each um"
13023280,13028080," overflow is wrongly computed"
13030480,13039280," no"
13039280,13057360," so uh that means we don't have enough hidden layers so let's actually try to maybe add"
13057360,13069440," uh as far as you can see why if n this is the maximum value right so this is the maximum value"
13069440,13077520," that we can have which is two right so which is like four right so if the sum of these things"
13078800,13082960," greater or equal is greater or equal than four that means it's an overflow for that neural network"
13082960,13096640," uh okay so uh the amount of this thing so this is n this is how many numbers you can have on one side"
13096640,13098320," all right so"
13102560,13103520," so it's actually"
13103520,13109520," so it's actually maximum that you can oh i'm really easy to make a small break like okay we're making a break"
13109520,13115040," okay i was gaslighted so everything is fine actually so let's not listen to the chat anymore"
13115040,13117120," uh okay so"
13117120,13123520," yeah so essentially what i want to do i want to add a little bit of the bits"
13124320,13131680," to the um to the inner layer right so how many like uh like inner things can we add in there"
13131680,13137120," we can add one bit in there and see if it's going to improve okay so it's actually went"
13137120,13144160," lower 46 which is nice can we go even lower by like increasing the amount of iterations i don't think so"
13144720,13150640," nah well i mean it's kind of going somewhere so i would presume that you kind of maybe need"
13150640,13158800," the bits amount of additional things to go anywhere and okay it's going somewhere all right"
13158800,13165840," so what if we just add another one just in case and that was actually very effective i think"
13165840,13173680," it's actually kind of cool okay so let's actually switch to to here right so because it's a little bit"
13173680,13183440," faster uh all right was it all right uh and uh i'm gonna increase the amount of iterations in here"
13183440,13190960," right so i'm gonna increase the amount of iterations like twice as many uh don't don't run please don't"
13190960,13199280," run it just recompile because i'm gonna be running down here and doesn't really go much lower than that"
13200400,13203600," uh what about 100 so maybe"
13203600,13217520," uh-huh yeah so it kind of stops at there uh maybe we want to add as many bits in here as like here so"
13217520,13226720," maybe that will be helpful uh all right so so i'm just like literally guessing so at least it goes below"
13227280,13229200," will it go even further probably not"
13229200,13241920," maybe there is some problem with the uh with the overflow"
13241920,13248640," maybe there is that's a it's an interesting question"
13252640,13260400," but what if we preserve sort of the original bits what if we preserve the original bits and we just like"
13260400,13267040," say okay if the final thing is just like bigger then on top of that it's also overflows"
13267040,13272400," right is that a good idea it could be so"
13274800,13280240," all right that was a good idea so maybe it couldn't find like a circuitry for that specific behavior"
13280240,13286160," that could be the thing it's just like a very specific difficult behavior to achieve it actually"
13286160,13288640," then dropped really fast at some point"
13288640,13296400," i should probably not overfit it this much though it's a model that is meant to be overfit anyway"
13297520,13302240," right so it's meant to be overfit so this is actually a pretty good uh pretty good thing so"
13302240,13308480," it's kind of interesting that sometimes that when you find the correct like architecture for the neural"
13308480,13313920," network it sort of like clicks in place and the cost goes to zero very very quickly but sometimes if the"
13313920,13318080," architecture cannot accommodate that it gets stuck somewhere it's just like it cannot find the"
13318080,13324480," combinations like cannot brute force the circuitry but if you get the amount of neurons and layers right it's"
13324480,13329920," just like clicks in place and it's just like solves it's so cool it's it's kind of magical in a sense"
13329920,13334400," right so it's just like it's you didn't solve the puzzle yet and then it solves the puzzle and it's"
13334400,13344160," just like it works it's so cool um uh so what if we can reduce the amount of bits what if we just like"
13344160,13351600," use this one is it going to to work was it because of the circuitry maybe nah not enough"
13353440,13357920," the amount of bits in here it doesn't seem to be enough right because it just slows down"
13357920,13363600," but maybe it's because it just started now but if we add one additional bit maybe that's what"
13363600,13371360," does the magic okay so probably twice as many bits in here right"
13371360,13377520," i'm literally guessing like this is how we're supposed to do machine learning i suppose"
13377520,13383360," yeah like twice as many bits"
13383360,13393520," and plus one just in case okay so let's actually set uh 50 000 right and um"
13399360,13406160," uh now time to have a pool of random architectures and then uh augment them based on performance with"
13406160,13412480," random mutations also yeah so we can basically raise the level of distractions uh right okay"
13414800,13428160," so let's try to do all of that right so let's iterate um size x zero uh less than n right so this is going to be x"
13430240,13432480," so we're going to be very fine in this intestine"
13432480,13437840," and y and let's just print"
13437840,13450240," let's just print z u plus z u so x and y so let's do maybe 5000"
13451200,13456160," so it doesn't really go for too long uh right so this is the different combinations"
13456160,13463440," and this is the different combinations and uh i want to basically put all of that stuff into the"
13463440,13471120," inputs of the neural network first all right so uh but here instead of being so that stuff we're going"
13471120,13478400," to be doing and then input of nn all right so we're just like feeding that stuff into the neural network"
13478400,13486400," and then we do forward of an n and this one is going to be super interesting right so we're going to be"
13486400,13496640," iterating through the bits of this stuff we can do size t z initially zero all right and and then output"
13496640,13499680," all right and then output uh"
13502240,13513600," mat at row is zero we're iterating j and we can say okay uh if it's greater than half that's what we're"
13513600,13518400," going to consider like bit right so we're going to call it beat"
13518400,13529120," greater than half and uh that bit right since we're compressing that bit in there so that means we'll have to"
13529840,13539440," move that bit by j right and do something like this i think that could be a good case"
13539440,13547200," all right so this is how we're going to be very fine listen to our stuff uh first we're going to be"
13547200,13550400," verifying all of that visually and declare it i"
13554960,13558400," so this one is going to be zero because it's an input"
13558400,13566720," okay is it correct no it is not correct even because it didn't train"
13566720,13576160," fully i think well it came like here it worked here it worked here it didn't work for sure didn't work for"
13576160,13582640," sure uh right so let's actually keep um training this entire time"
13582640,13588640," excuse me um"
13588640,13594160," right so we need to increase the amount of iterations let's do"
13594160,13596560," uh 100"
13599760,13606320," um so three oh three plus one this has to be yeah so we have to be super careful with that thank you"
13606320,13615520," thank you so much um essentially what we have to do let's move this stuff maybe somewhere here"
13615520,13620960," all right so this is going to be just this and then"
13623760,13630560," uh if and then output and then mat at"
13630560,13641520," zero beats greater than half that means it's overflow so what we want to print instead"
13641520,13647680," overflow right otherwise"
13651360,13652400," we want to print"
13652400,13666880," zero zero zero there we go so that's a good one"
13666880,13675120," well i mean yeah so i suppose this is a full set of training on 100"
13675120,13677680," so this is going to be very very good"
13681360,13684080," all good to me"
13684080,13690320," so yeah this neural network behaves like an adder"
13690320,13698720," it does behave like an adder we can now add um essentially more"
13698720,13704080," bits in here we can add three bits right"
13704080,13709040," and the back propagation seem to be finding a solution for all of that"
13710080,13715680," okay so i think maybe 10k should be enough right so because i don't really want to wait all day"
13715680,13720960," all right let's do 10k and if it doesn't do in 10k well it's not worth it"
13720960,13729600," uh okay so one of the things we could have probably tried is um essentially"
13731840,13739840," yeah that works uh we don't really want to output all of that in here uh we want to just basically say"
13739840,13744240," okay uh all right and if you find any of the examples that"
13744240,13750240," that basically don't fit we want to print them right"
13755280,13760800," why not have loop until cost is close to zero or less than zero because i don't really know for"
13760800,13767440," how long it's going to go right so i want to have like a control over how much time we'll spend on"
13767440,13777840," that so essentially what i want to do um all right so if it's overflow uh we want to do the following thing"
13780000,13791360," so this is z um like maybe we can call it a for an activation but for an activation and then we can"
13791360,13796320," have an actual z which is uh x plus y right x plus y"
13800560,13810320," so if we overflow right if we overflow how do we compare overflow i just want to double check z equal"
13810320,13812640," or greater or equal than n"
13812640,13825600," if z equal greater than n actually if z is less than n uh we have to print something right print"
13830560,13847760," so we can say maybe expected overflow right but got uh something else we can print that z"
13847760,13857200," maybe we can even count the fails fails 0"
13860560,13886240," so if we don't know if z not equal not equal a we move on to something like this expected this but got a"
13890560,13896000," maybe we can do something like this difference"
13896000,13903760," if fails equals 0"
13903760,13909040," i think that's a good solution"
13909040,13912160," uh this is not good"
13912160,13931120," so if we uh actually train it for less this is actually very good uh you know verification uh what"
13931120,13939040," if we train it like for 10 right and it basically tells you that it doesn't freaking know uh it doesn't"
13939040,13946560," really work at all so it's completely failed uh we can even yeah let's actually put it this that's"
13946560,13955360," fine so we need more uh i wonder how many like is thousand no no a thousand not enough but it's actually"
13955360,13959520," less right so it's actually less uh five thousand is five thousand"
13959520,13964160," maybe five thousand is more than enough to actually"
13964960,13972320," yeah only one okay uh what about then 10 so i think 10 is like a magical number in here so with 10"
13972320,13974640," you're gonna be fine"
13974640,13986160," okay it's okay cool so let's actually see uh like what if we used finite differences instead all right if zero"
13986880,13995680," um um so would it perform similarly find that differences at epsilon is going to be let's say"
13995680,13998400," one minus this and then"
13998400,14009040," that's slower that's already slower can you feel that"
14011520,14021920," man so that's why people use gradient descent it's way slower we can actually try to increase the amount"
14021920,14024800," of bits what if we increase the amount of bits like four"
14024800,14037360," this is the finite differences though at some point it actually starts to go a bit faster but"
14039360,14048880," man that sucks that freaking sucks this is a finite difference so to see the actual benefit we had to"
14048880,14058000," like compare them like on bigger neural networks and now let's switch to uh back propagation so"
14058000,14065360," back propagation also has its limits but it's way faster look at that holy so as far as i know back"
14065360,14072160," back propagation like at the time when people like finally figured it out it was a huge boost to like"
14072160,14077600," uh to training neural networks right because you finally could do it way faster than people used to do"
14077600,14085360," it before uh right so and there are other techniques that that we can apply look look at that it's actually"
14085360,14092560," struggling it's actually not enough i wonder if it's just like started from a wrong place i wonder from which"
14092560,14099120," place it started do we randomize anything we don't even randomize anything let's do srand time maybe"
14099120,14103040," we just have to start from a different place yeah"
14103040,14112320," okay so this is a different place or maybe just circuitry is not enough for this thing"
14113200,14129760," to work properly is this open source of course uh use the today command and it will give you the link too"
14135360,14142720," okay i don't think it will get it quite there though one of the things we can try to do we can stack more"
14142720,14150720," layers right to give to make it a little bit more flexible and at some point yeah so it still couldn't"
14150720,14157600," train on these examples expected 14 but actually 10. but it's like it's close it's close because this is"
14157600,14162640," like way more examples than than usual so it couldn't quite get the circuitry right"
14162640,14170640," um so what if we do a little bit deeper what if we just like make it like this is"
14170640,14173520," it going to be better so it's probably going to be also slower"
14177920,14187520," now it's just like really struggling uh by the way for this neural networks for deeper i think you have"
14187520,14194880," to be a little bit slower when you when you do that right i remember having some sort of problems in that"
14194880,14204320," part but it's yeah"
14204320,14211680," so maybe the depth is not gonna solve anything in here uh one of the things we can try we can try to"
14211680,14215920," do but you don't really want to make it too big because essentially what it will do it will memorize"
14215920,14219280," but in case of like overfitting model maybe that's precisely what you want anyway"
14221360,14227760," let's put the rate back to one there is another interesting technique to speed this entire stuff"
14227760,14237040," up there is a also thing called stochastic gradient descent uh but i wanna do this thing in a different"
14237040,14237520," stream"
14237520,14250960," um maybe it can print uh maybe but i mean it was doing fine with uh less amount of bits"
14250960,14252960," here it goes well kind of well"
14252960,14261520," so if i just use uh four"
14261520,14269920," of these bits it basically yeah it basically memorizes everything and i presume right"
14269920,14278000," we're almost there by the way"
14280560,14283600," uh bacheloring yeah so bacheloring can actually help"
14283600,14287520," so what was there"
14287520,14295280," 10k and i think it's gonna be okay so because at that precision i think it's already done"
14295280,14300560," yeah it's okay so we can yeah we can try the following thing"
14304800,14311840," yeah yeah if the the cost gets like below one hundredth right so we can probably stop all of"
14311840,14319760," that but that's besides the point so uh there there is a very interesting technique that can speed up"
14319760,14332320," um the learning quite a bit so essentially uh you have some uh data right so you have the training data"
14332960,14339440," and the cost function as you can see when we're computing cost function"
14339440,14341120," computing cost function"
14341120,14349920," to print this entire thing we provide the entire data in here so the whole data training data is put in"
14349920,14357040," into the cost function to compute the full cost function so and which could be actually like computationally"
14357840,14366320," heavy so the idea is uh essentially split the entire uh training data into like these small batches"
14366320,14371840," and also to make the stochastic part of gradient descent just like shuffle them around right just"
14371840,14377280," shuffle them around and essentially uh on each step uh instead of using the full"
14378000,14388400," data use that single batch then the next batch then the next batch then the next batch and just circle"
14388400,14396480," through them just circle through them the question is why the that would work right so imagine that you have"
14396480,14407200," like um multi-variable uh cost function where is that okay so uh essentially you have two parameters"
14407200,14412640," right so obviously you're gonna have more parameters but yeah so this is gonna be let's say weight and this"
14412640,14417200," is the bias or something and let's say that the local minimum is somewhere here and we started somewhere"
14417200,14424080," here and there is like a uh sort of like a contour lines right so the contour lines are going like that"
14424080,14430960," something something the gradient uh actually since it's a minimum the gradient goes outwards right so"
14430960,14437760," the gradient goes outwards in here so this is the vector field uh right and essentially a gradient descent"
14437760,14445200," is going to be going slowly in here right so if you use the full data the gradient descent is going"
14445200,14451040," to slowly descend in here in an opposite direction of the as the gradient right because we subtracted the gradient"
14451040,14454880," so let's actually say that this is the thing uh for a single batch"
14454880,14461120," for a single batch uh a single step is not going to be in that direction so let's actually make it even"
14461120,14468000," even more visible something like this right so if you use full data this is the single step but if you use"
14468000,14473360," one batch a single step is going to be here it's going to go in a completely different direction"
14474320,14481040," but here's an interesting thing if you do scroll basically scroll through all of the batches"
14481040,14484560," your path of the gradient descent is going to look like this"
14484560,14497840," on average it's going to converge towards the local minimum but since each step requires less data it's"
14497840,14506080," computationally um it's computationally uh less expensive so this is called stochastic gradient"
14506080,14511840," descent or like uh batch training or something like that somebody called it batch training because you"
14511840,14517520," split the data into these batches and stuff like that so this is one of the ideas people use to speed"
14517520,14522640," up the training as well so they use back propagation and along with back propagation is batch training or"
14523280,14528080," stochastic gradient descent and it's called stochastic gradient descent because the path ends up like very"
14528080,14533040," stochastic and stochasticness is achieved by shuffling these batches around does that make sense"
14533040,14537600," right so i'm planning to make a separate stream where we can try to apply that"
14537600,14546000," um you can also optimize one of the oh yeah we didn't even apply any headway optimizations like anywhere"
14546000,14553280," like what we're doing so far uh right what we're doing so far is when it comes to metric multiplication"
14553280,14560720," so that's literally how we're doing that like that's the whole matrix multiplication on cpu because we're"
14560720,14568880," learning things uh right and uh there is a lot of like low hanging fruits to fruits to actually speed up"
14568880,14574720," this entire thing right but they intentionally implement uh everything in a very dumb way to just"
14574720,14579680," learn right so because i think um you know implementing everything from scratch and just"
14579680,14586400," like making it work at least slow is actually very educational because again there is no point in moving"
14586400,14593600," fast if you don't know the direction right so again it's just like why don't i use gpu acceleration why"
14593600,14599360," don't i use fancy algorithms why don't i go fast zoisin you have to go fast zoisin why are you so slow"
14599360,14607680," what's the point of going fast if you don't know the direction i'm learning trying to implement uh the"
14607680,14614800," most optimal the most performing algorithms when you learn the concept is actually very dangerous for"
14614800,14620880," educational value so it doesn't have an educational value figure out the direction and once you figure it"
14620880,14635760," out on the event like call a taxi or something i don't know just to go fast so that's basically what it is"
14637520,14640800," is that by the way and there is another low hanging fruit"
14640800,14649520," and you can try that"
14649520,14656800," so it's gonna take some time to to build something"
14658800,14661520," is it gonna be faster i feel like it's a bit faster"
14661520,14667360," yeah it's already all almost done"
14667360,14671840," yeah it's okay"
14671840,14684240," cool uh so i guess that's it for today uh i hope i didn't miss any uh subs okay so thank you to"
14684240,14690240," uh log can for uh for twitch prime in neutral x dev thank you so much for uh twitch prime with the"
14690240,14698720," message chat is cute chat is cute indeed uh so all right so that's it for today thanks everyone who's"
14698720,14704560," watching me right now i really appreciate it i hope uh this stream was interesting right i don't know how"
14704560,14711200," educational that was because backpropagation is really difficult to explain in layman terms if you"
14711200,14718480," know what i mean right because of sheer complexity and i want to stress that out it's not difficult"
14718480,14723760," right it doesn't really do anything that you can't comprehend it's just like complex right it's a"
14723760,14729680," four-dimensional loop that you have to keep in mind yeah thanks everyone for watching uh see you all on the"
14729680,14744000," next as soon as this session love you you"
